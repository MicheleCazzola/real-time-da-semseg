{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPliIXlMh2bf"
      },
      "source": [
        "# Seed initialization (to make results reproducible)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCA5kpzUh9aG",
        "outputId": "1a8387a0-71b4-408d-cb32-6bd5ab934dd6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "torch.manual_seed(0)\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.use_deterministic_algorithms(True, warn_only=True)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "g = torch.Generator()\n",
        "g.manual_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUccqUJxsmMl"
      },
      "source": [
        "# Environment setup (execute for any Step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-sBVNVCbUsy"
      },
      "source": [
        "## Package install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FuKcgT_Xo_G",
        "outputId": "975c8696-c043-4694-d615-8729c02a2b08"
      },
      "outputs": [],
      "source": [
        "!pip install wget\n",
        "!pip install requests gdown\n",
        "!pip install fvcore\n",
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnTYhfbCbbUN"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeSEfBMAslUP",
        "outputId": "8cc0bdcb-f00b-44b9-c4d7-2c8c3286bfbd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.patches as mpatches\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import wget\n",
        "import gdown\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import VisionDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from enum import Enum\n",
        "from google.colab import drive\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "from fvcore.nn import FlopCountAnalysis, flop_count_table\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import cv2\n",
        "from torch.nn.utils import clip_grad\n",
        "import albumentations as A\n",
        "import shutil\n",
        "\n",
        "from models.deeplab_v2 import get_deeplab_v2\n",
        "from models.pidnet import PIDNet\n",
        "from losses.bondary import BondaryLoss\n",
        "from losses.cross_entropy import CrossEntropy\n",
        "from losses.focal import FocalLoss\n",
        "from losses.ohem import OhemCrossEntropy\n",
        "from models.discriminator import FCDiscriminator\n",
        "from models.bisenet import BiSeNet\n",
        "from models.stdc import STDC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vnEBr7iwiI8"
      },
      "source": [
        "## Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABxsgPkxwlmT"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = 'loveDA_dataset'\n",
        "TRAIN_ZIP = f'{DATA_DIR}/train.zip'\n",
        "VAL_ZIP = f'{DATA_DIR}/validation.zip'\n",
        "TEST_ZIP = f'{DATA_DIR}/test.zip'\n",
        "TRAIN_DIR = f'{DATA_DIR}/train'\n",
        "VAL_DIR = f'{DATA_DIR}/validation'\n",
        "TEST_DIR = f'{DATA_DIR}/test'\n",
        "RURAL_PATH = \"Rural\"\n",
        "URBAN_PATH = \"Urban\"\n",
        "IMG_PATH = \"images_png\"\n",
        "MASK_PATH = \"masks_png\"\n",
        "PRETRAINED_WEIGHTS_DIR = 'pretrained_weights'\n",
        "DEEPLAB_V2_WEIGHTS = f'{PRETRAINED_WEIGHTS_DIR}/DeepLab_resnet_pretrained_imagenet.pth'\n",
        "STDC1_WEIGHTS = f\"{PRETRAINED_WEIGHTS_DIR}/STDC1_pretrained_weights.pth\"\n",
        "\n",
        "IGNORE_INDEX=-1\n",
        "\n",
        "RGB = 'RGB'\n",
        "grayscale = 'L'\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Domain(Enum):\n",
        "    RURAL = 0\n",
        "    URBAN = 1\n",
        "\n",
        "class ModelType(Enum):\n",
        "    DEEPLAB = 0\n",
        "    PIDNET = 1\n",
        "    BISENET = 2\n",
        "    STDC = 3\n",
        "\n",
        "categories = {\n",
        "    'BARREN': (0.003921568859368563, (159, 129, 183)),       # Lilla\n",
        "    'AGRICULTURE': (0.027450980618596077, (255, 195, 128)),  # Arancione\n",
        "    'BUILDING': (0.007843137718737125, (255, 0, 0)),         # Rosso\n",
        "    'WATER': (0.01568627543747425, (0, 0, 255)),             # Blu\n",
        "    'ROAD': (0.0117647061124444, (255, 255, 0)),             # Giallo\n",
        "    'BG': (0.019607843831181526, (255, 255, 255)),           # Bianco\n",
        "    'FOREST': (0.0235294122248888, (0, 255, 0))              # Verde\n",
        "}\n",
        "\n",
        "categories = dict(sorted(categories.items(), key=lambda item: item[1][0]))\n",
        "\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "num_classes = len(categories.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl_nALVQtCuk"
      },
      "source": [
        "## Dataset: LoveDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpFwzlM2UoL_"
      },
      "source": [
        "### Download dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs1L8KHZ9GAq"
      },
      "source": [
        "#### Without Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyWrp2BomFDS",
        "outputId": "d8401b12-d04f-4805-a646-c2a725d0d594"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "download_directory = Path(DATA_DIR)\n",
        "if not download_directory.exists():\n",
        "    download_directory.mkdir(exist_ok=True)\n",
        "\n",
        "# Zip download\n",
        "\n",
        "train_zip = Path(TRAIN_ZIP)\n",
        "if not train_zip.exists():\n",
        "    !wget -O {TRAIN_ZIP} 'https://zenodo.org/record/5706578/files/Train.zip?download=1'\n",
        "\n",
        "val_zip = Path(VAL_ZIP)\n",
        "if not val_zip.exists():\n",
        "    !wget -O {VAL_ZIP} 'https://zenodo.org/records/5706578/files/Val.zip?download=1'\n",
        "\n",
        "test_zip = Path(TEST_ZIP)\n",
        "if not test_zip.exists():\n",
        "    !wget -O {TEST_ZIP} 'https://zenodo.org/records/5706578/files/Test.zip?download=1'\n",
        "\n",
        "# Zip extraction\n",
        "\n",
        "## I suppose to not cancel the original zip since who knows\n",
        "\n",
        "train_dir = Path(TRAIN_DIR)\n",
        "if not train_dir.exists():\n",
        "    !unzip -q {TRAIN_ZIP} -d {DATA_DIR}\n",
        "    !mv {DATA_DIR}/Train {TRAIN_DIR}\n",
        "\n",
        "val_dir = Path(VAL_DIR)\n",
        "if not val_dir.exists():\n",
        "    !unzip -q {VAL_ZIP} -d {DATA_DIR}\n",
        "    !mv {DATA_DIR}/Val {VAL_DIR}\n",
        "\n",
        "test_dir = Path(TEST_DIR)\n",
        "if not test_dir.exists():\n",
        "    !unzip -q {TEST_ZIP} -d {DATA_DIR}\n",
        "    !mv {DATA_DIR}/Test {TEST_DIR}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsP0RjvL9NlC"
      },
      "source": [
        "#### With Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7t0sVWE9AMl"
      },
      "outputs": [],
      "source": [
        "def download_to_gdrive():\n",
        "    drive_path_dir = '/content/drive'\n",
        "    mydrive_path_dir = f'{drive_path_dir}/MyDrive'\n",
        "    data_path_dir = f'{mydrive_path_dir}/{DATA_DIR}'\n",
        "    train_path_zip = f'{mydrive_path_dir}/{TRAIN_ZIP}'\n",
        "    val_path_zip = f'{mydrive_path_dir}/{VAL_ZIP}'\n",
        "    test_path_zip = f'{mydrive_path_dir}/{TEST_ZIP}'\n",
        "    train_path_dir = f'{mydrive_path_dir}/{TRAIN_DIR}'\n",
        "    val_path_dir = f'{mydrive_path_dir}/{VAL_DIR}'\n",
        "    test_path_dir = f'{mydrive_path_dir}/{TEST_DIR}'\n",
        "\n",
        "    drive.mount(drive_path_dir)\n",
        "\n",
        "    download_directory = Path(data_path_dir)\n",
        "    if not download_directory.exists():\n",
        "        download_directory.mkdir(exist_ok=True)\n",
        "\n",
        "    train_zip = Path(train_path_zip)\n",
        "    if not train_zip.exists():\n",
        "        !wget -O {train_path_zip} 'https://zenodo.org/record/5706578/files/Train.zip?download=1'\n",
        "\n",
        "    val_zip = Path(val_path_zip)\n",
        "    if not val_zip.exists():\n",
        "        !wget -O {val_path_zip} 'https://zenodo.org/records/5706578/files/Val.zip?download=1'\n",
        "\n",
        "    test_zip = Path(test_path_zip)\n",
        "    if not test_zip.exists():\n",
        "        !wget -O {test_path_zip} 'https://zenodo.org/records/5706578/files/Test.zip?download=1'\n",
        "\n",
        "def extract_from_gdrive():\n",
        "\n",
        "    drive_path_dir = '/content/drive'\n",
        "    mydrive_path_dir = f'{drive_path_dir}/MyDrive'\n",
        "    data_path_dir = f'{mydrive_path_dir}/{DATA_DIR}'\n",
        "    train_path_zip = f'{mydrive_path_dir}/{TRAIN_ZIP}'\n",
        "    val_path_zip = f'{mydrive_path_dir}/{VAL_ZIP}'\n",
        "    test_path_zip = f'{mydrive_path_dir}/{TEST_ZIP}'\n",
        "    train_path_dir = f'{mydrive_path_dir}/{TRAIN_DIR}'\n",
        "    val_path_dir = f'{mydrive_path_dir}/{VAL_DIR}'\n",
        "    test_path_dir = f'{mydrive_path_dir}/{TEST_DIR}'\n",
        "\n",
        "    drive.mount(drive_path_dir)\n",
        "\n",
        "    train_dir = Path(TRAIN_DIR)\n",
        "    if not train_dir.exists():\n",
        "        !unzip -q {train_path_zip} -d {DATA_DIR}\n",
        "        !mv {DATA_DIR}/Train {TRAIN_DIR}\n",
        "\n",
        "    val_dir = Path(VAL_DIR)\n",
        "    if not val_dir.exists():\n",
        "        !unzip -q {val_path_zip} -d {DATA_DIR}\n",
        "        !mv {DATA_DIR}/Val {VAL_DIR}\n",
        "\n",
        "    #test_dir = Path(TEST_DIR)\n",
        "    #if not test_dir.exists():\n",
        "    #    !unzip -q {test_path_zip} -d {DATA_DIR}\n",
        "    #    !mv {DATA_DIR}/Test {TEST_DIR}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZF7aCc_87V2"
      },
      "outputs": [],
      "source": [
        "def copy_to_gdrive():\n",
        "    drive_path_dir = '/content/drive'\n",
        "    mydrive_path_dir = f'{drive_path_dir}/MyDrive'\n",
        "    data_path_dir = f'{mydrive_path_dir}/{DATA_DIR}'\n",
        "    train_path_zip = f'{mydrive_path_dir}/{TRAIN_ZIP}'\n",
        "    val_path_zip = f'{mydrive_path_dir}/{VAL_ZIP}'\n",
        "    test_path_zip = f'{mydrive_path_dir}/{TEST_ZIP}'\n",
        "\n",
        "    drive.mount(drive_path_dir)\n",
        "\n",
        "    # Create the directory if it doesn't exist\n",
        "    os.makedirs(data_path_dir, exist_ok=True)\n",
        "\n",
        "    # Copy the zip files using shutil.copy\n",
        "    if not os.path.exists(train_path_zip):\n",
        "        shutil.copy(TRAIN_ZIP, train_path_zip)\n",
        "        print(f\"Copied {TRAIN_ZIP} to {train_path_zip}\")\n",
        "\n",
        "    if not os.path.exists(val_path_zip):\n",
        "        shutil.copy(VAL_ZIP, val_path_zip)\n",
        "        print(f\"Copied {VAL_ZIP} to {val_path_zip}\")\n",
        "\n",
        "    if not os.path.exists(test_path_zip):\n",
        "        shutil.copy(TEST_ZIP, test_path_zip)\n",
        "        print(f\"Copied {TEST_ZIP} to {test_path_zip}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnzRinjI_HKi",
        "outputId": "b2e08164-1bd0-4f4f-a1ed-15ee9b35ab8e"
      },
      "outputs": [],
      "source": [
        "#download_to_gdrive()\n",
        "#copy_to_gdrive()\n",
        "extract_from_gdrive()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eORGOwqKY94j"
      },
      "source": [
        "### Dataset construction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1JhaLdfZAnt"
      },
      "outputs": [],
      "source": [
        "def pil_loader(path, codify):\n",
        "    with open(path, 'rb') as f:\n",
        "        img = Image.open(f)\n",
        "        return img.convert(codify)\n",
        "\n",
        "def load_images(root_path, directory, img, mask):\n",
        "    directory_path = root_path / directory\n",
        "    img_path = directory_path / img\n",
        "    mask_path = directory_path / mask\n",
        "    if not img_path.is_dir() or not mask_path.is_dir():\n",
        "        raise RuntimeError(\"folder structure different from expected\")\n",
        "\n",
        "    images = [item.name for item in img_path.iterdir()]\n",
        "    masks = [item.name for item in mask_path.iterdir()]\n",
        "\n",
        "    if set(images) != set(masks):\n",
        "        raise RuntimeError(\"images and masks do not match\")\n",
        "\n",
        "    return images\n",
        "\n",
        "def generate_bd(mask, edge_pad=False, is_flip=False, edge_size=2):\n",
        "\n",
        "    y_k_size = 6\n",
        "    x_k_size = 6\n",
        "\n",
        "    edge = cv2.Canny(mask, 0, 8)\n",
        "    kernel = np.ones((edge_size, edge_size), np.uint8)\n",
        "\n",
        "    if edge_pad:\n",
        "        edge = edge[y_k_size:-y_k_size, x_k_size:-x_k_size]\n",
        "        edge = np.pad(edge, ((y_k_size,y_k_size),(x_k_size,x_k_size)), mode='constant')\n",
        "    edge = (cv2.dilate(edge, kernel, iterations=1)>50)*1.0\n",
        "\n",
        "    return edge\n",
        "\n",
        "class LoveDA(VisionDataset):\n",
        "    def __init__(self, root, img, mask, directories=None, transforms=None, bd=False):\n",
        "        super(LoveDA, self).__init__(root)\n",
        "\n",
        "        root_path = Path(root)\n",
        "\n",
        "        if not root_path.is_dir():\n",
        "            raise RuntimeError(\"root should be a directory\")\n",
        "\n",
        "        self.root = root\n",
        "        self.img_path = img\n",
        "        self.mask_path = mask\n",
        "        self.transforms = transforms\n",
        "\n",
        "        self.image_names = []\n",
        "\n",
        "        self.bd = bd\n",
        "\n",
        "        if directories is None:\n",
        "            raise RuntimeError(\"at least one directory must be passed\")\n",
        "\n",
        "        directories = [directories] if isinstance(directories, str) else directories\n",
        "\n",
        "        for d in directories:\n",
        "          image_names = load_images(root_path, d, img, mask)\n",
        "          self.image_names.extend([(d, image_name) for image_name in image_names])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        dir, image_name = self.image_names[index]\n",
        "        image_path = f'{self.root}/{dir}/{self.img_path}/{image_name}'\n",
        "        mask_path = f'{self.root}/{dir}/{self.mask_path}/{image_name}'\n",
        "\n",
        "        image = pil_loader(image_path, RGB)\n",
        "        mask = pil_loader(mask_path, grayscale)\n",
        "\n",
        "        image = np.array(image)\n",
        "        mask = np.array(mask)\n",
        "\n",
        "        if self.transforms is not None:\n",
        "          data = self.transforms(image=image, mask=mask)\n",
        "          image = data['image']\n",
        "          mask = data['mask']\n",
        "\n",
        "        image = transforms.ToTensor()(image)\n",
        "        mask = transforms.ToTensor()(mask).squeeze(0)\n",
        "        mask = transforms.ToPILImage()(mask)\n",
        "        mask = transforms.PILToTensor()(mask).squeeze(0).long()\n",
        "\n",
        "        mask = mask - 1\n",
        "\n",
        "        if self.bd:\n",
        "            bd = generate_bd(mask.numpy().astype(np.uint8))\n",
        "\n",
        "            return image, mask, bd\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "    def __len__(self):\n",
        "        length = len(self.image_names)\n",
        "        return length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zim4nyhUgTEE"
      },
      "source": [
        "### Statistics and metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUlnYE8RSUYX"
      },
      "source": [
        "#### Average, Standard deviation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZX5a7cLVrqn"
      },
      "outputs": [],
      "source": [
        "def compute_avg_std(dataset, dataloader, device):\n",
        "    with torch.no_grad():\n",
        "        avg = torch.zeros((1,3)).to(device)\n",
        "        std = torch.zeros((1,3)).to(device)\n",
        "        data_len = 0\n",
        "        tot_pixels = 0\n",
        "\n",
        "        assert len(dataloader) > 0, \"Dataloader must contain some data\"\n",
        "\n",
        "        tot_batches = len(dataloader)\n",
        "\n",
        "        for (step, (inputs, labels)) in enumerate(dataloader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            b, _, h, w = inputs.shape\n",
        "\n",
        "            data_len += b\n",
        "            tot_pixels += b * h * w\n",
        "            avg += torch.sum(inputs, dim=(0,2,3))\n",
        "            std += torch.sum(inputs * inputs, dim=(0,2,3))\n",
        "\n",
        "        avg /= tot_pixels\n",
        "        std = torch.sqrt(std / tot_pixels - avg * avg)\n",
        "\n",
        "        return data_len, avg.flatten().tolist(), std.flatten().tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Us9dt9zBSYSq"
      },
      "source": [
        "#### IoU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UBrHS8zpuCS"
      },
      "outputs": [],
      "source": [
        "def calculate_iou(outputs, masks, num_classes):\n",
        "\n",
        "    # Get predictions from the model output probabilities\n",
        "    _, preds = torch.max(outputs, dim=1) # B x H x W\n",
        "\n",
        "    # IoU for each class\n",
        "    iou_per_class = torch.zeros(num_classes, dtype=torch.float32, device=outputs.device)\n",
        "\n",
        "    for i in range(num_classes):  # Iterate over all classes\n",
        "        pred_mask = preds == i\n",
        "        label_mask = masks == i\n",
        "\n",
        "        intersection = torch.logical_and(pred_mask, label_mask).sum().float()\n",
        "        union = torch.logical_or(pred_mask, label_mask).sum().float()\n",
        "\n",
        "        if union > 0:\n",
        "            iou_per_class[i] = intersection / union\n",
        "\n",
        "    # Calculate mIoU for classes with a non-zero IoU\n",
        "    valid_ious = iou_per_class\n",
        "    miou = valid_ious.mean() if len(valid_ious) > 0 else torch.tensor(0.0, device=outputs.device)\n",
        "\n",
        "    return miou, iou_per_class\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIxonvMiSZsv"
      },
      "source": [
        "#### Latency, FPS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODklmynO9hah"
      },
      "outputs": [],
      "source": [
        "def calculate_latency_fps(model, device, height, width, iterations, model_type: ModelType):\n",
        "    image = torch.randn(1, 3, height, width).to(device)\n",
        "    mask = None\n",
        "    boundary = None\n",
        "\n",
        "    if model_type == ModelType.PIDNET:\n",
        "        mask = torch.randint(0, num_classes, (1, height, width), dtype=torch.int64).to(device)\n",
        "        boundary = torch.randint(0, 2, (1, height, width), dtype=torch.float64).to(device)\n",
        "\n",
        "    latency = []\n",
        "    FPS = []\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        start = time.time()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if model_type == ModelType.DEEPLAB:\n",
        "                _ = model(image)\n",
        "            else:\n",
        "                _ = model(image, mask, boundary)\n",
        "\n",
        "        end = time.time()\n",
        "\n",
        "        latency_i = end - start\n",
        "        latency.append(latency_i)\n",
        "\n",
        "        FPS_i = 1 / latency_i\n",
        "        FPS.append(FPS_i)\n",
        "\n",
        "    meanLatency = np.mean(latency) * 1000 # millis\n",
        "    stdLatency = np.std(latency) * 1000\n",
        "    meanFPS = np.mean(FPS)\n",
        "    stdFPS = np.std(FPS)\n",
        "\n",
        "    return meanLatency, stdLatency, meanFPS, stdFPS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gG_0OQ72SdhY"
      },
      "source": [
        "#### FLOPS, Params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qc4UaQlG_lux"
      },
      "outputs": [],
      "source": [
        "def calculate_flops_params(model, device, height, width, model_type: ModelType):\n",
        "    image = torch.zeros(1, 3, height, width).to(device)\n",
        "    model = model.to(device)\n",
        "    flops = None\n",
        "    if model_type == ModelType.PIDNET:\n",
        "        mask = torch.zeros(1, height, width, dtype=torch.int64).to(device)\n",
        "        boundary = torch.zeros(1, height, width, dtype=torch.float64).to(device)\n",
        "        flops = FlopCountAnalysis(model, (image, mask, boundary))\n",
        "    else:\n",
        "        flops = FlopCountAnalysis(model, image)\n",
        "    print(flop_count_table(flops))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8wad1WFHByx"
      },
      "source": [
        "### Plot losses and mious"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tANlpOZDHGoX"
      },
      "outputs": [],
      "source": [
        "def plot_losses_mious(train_losses, eval_losses, miou_scores, num_epochs):\n",
        "    # Crea una figura con due assi, disposti uno accanto all'altro\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 5))\n",
        "\n",
        "    # Disegna il grafico delle perdite di training e validation sul primo asse\n",
        "    ax1.plot(train_losses, label='Training Loss')\n",
        "    ax1.plot(eval_losses, label='Validation Loss')\n",
        "    ax1.set_xlabel('Epochs')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.set_xticks(range(0, num_epochs), range(1, num_epochs + 1))\n",
        "    ax1.set_title('Training and Validation Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid()\n",
        "\n",
        "    # Disegna il grafico di mIoU sul secondo asse\n",
        "    ax2.plot(miou_scores, label='mIoU')\n",
        "    ax2.set_xlabel('Epochs')\n",
        "    ax2.set_ylabel('mIoU')\n",
        "    ax2.set_xticks(range(0, num_epochs), range(1, num_epochs + 1))\n",
        "    ax2.set_title('mIoU')\n",
        "    ax2.legend()\n",
        "    ax2.grid()\n",
        "\n",
        "    # Mostra la figura\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwOd5iQLitoS"
      },
      "outputs": [],
      "source": [
        "def plot_mious_per_category(miou_scores, num_epochs):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for class_name, miou_values in miou_scores.items():\n",
        "        plt.plot(range(num_epochs), miou_values, label=class_name)\n",
        "\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('mIoU (%)')\n",
        "    plt.xticks(range(0, num_epochs), range(1, num_epochs + 1))\n",
        "    plt.title('mIoU per Class over Epochs')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbMkS3H2iS1v"
      },
      "source": [
        "### Checkpoint resume"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yd2ITZT4iWbZ"
      },
      "outputs": [],
      "source": [
        "def resume_checkpoint(resume_path, model, optimizer=None, scheduler=None):\n",
        "    checkpoint = torch.load(resume_path)\n",
        "    iteration = checkpoint['iteration'] + 1\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "    if optimizer is not None:\n",
        "      optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    if scheduler is not None:\n",
        "      scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "    return iteration, model, optimizer, scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEC_H4S0igkg"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(path, iteration, model, optimizer, scheduler):\n",
        "    checkpoint = {\n",
        "        'iteration': iteration,\n",
        "        'model': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'scheduler': scheduler.state_dict()\n",
        "    }\n",
        "    torch.save(checkpoint, path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "II5pa_fmLVv4"
      },
      "source": [
        "### Image visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Teb1PthuoPR8"
      },
      "outputs": [],
      "source": [
        "def plot_tensor_mask(mask_tensor, categories):\n",
        "\n",
        "    categories = dict(sorted(categories.items(), key=lambda item: item[1][0]))\n",
        "\n",
        "    # Convert mask tensor to numpy array\n",
        "    mask_array = mask_tensor.squeeze().numpy()\n",
        "\n",
        "    # Create a colored mask image\n",
        "    colored_mask = np.zeros((mask_array.shape[0], mask_array.shape[1], 3), dtype=np.uint8)\n",
        "    for i, (label, (value, color)) in enumerate(categories.items()):\n",
        "        mask = mask_array == i\n",
        "        colored_mask[mask] = color\n",
        "\n",
        "    # Display the colored mask\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.imshow(colored_mask)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    # Create a legend\n",
        "    legend_patches = [mpatches.Patch(color=np.array(color)/255, label=label) for label, (_, color) in categories.items()]\n",
        "    plt.legend(handles=legend_patches, bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8jpvfnedTbG"
      },
      "source": [
        "# Step 2a: Testing classic semantic segmentation network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk_O5SS6BqXV"
      },
      "source": [
        "### Download pre-trained weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoZlf8G-nabe",
        "outputId": "60fa8f65-483e-4bde-cca7-acf6c70de8df"
      },
      "outputs": [],
      "source": [
        "weights_dir = Path(PRETRAINED_WEIGHTS_DIR)\n",
        "if not weights_dir.exists():\n",
        "    weights_dir.mkdir(exist_ok=True)\n",
        "\n",
        "deeplab_v2_weights = Path(DEEPLAB_V2_WEIGHTS)\n",
        "if not deeplab_v2_weights.exists():\n",
        "    # Replace with the correct Google Drive file ID\n",
        "    file_id = '1ZX0UCXvJwqd2uBGCX7LI2n-DfMg3t74v'\n",
        "    gdown.download(id=file_id, output=str(deeplab_v2_weights), quiet=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glRuMNQhsq1p"
      },
      "source": [
        "## Model: DeepLabv2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDZFT-myZ1Xd"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTdVkPSJoR4M"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XajKG5FoRhQ"
      },
      "outputs": [],
      "source": [
        "# Change in case of resume training\n",
        "RESUME_TRAINING = False\n",
        "epoch = None\n",
        "\n",
        "num_epochs = 20\n",
        "BATCH_SIZE = 6\n",
        "learning_rate = 1e-3\n",
        "step_size = 10\n",
        "gamma = 0.1\n",
        "resize = 512\n",
        "w_decay = 1e-3\n",
        "\n",
        "RESUME_PATH = f\"/content/drive/MyDrive/loveDA_dataset/Model training/DeepLab/DeepLabV2_{num_epochs}_{learning_rate}_{step_size}_{gamma}_{resize}_{w_decay}_epoch{epoch}.pth.tar\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y7JhEDjpOk1"
      },
      "source": [
        "### Dataset preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PhgKOtw9p7X"
      },
      "source": [
        "#### Normalization metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10-2EUtg9uMk"
      },
      "outputs": [],
      "source": [
        "#preprocessing_dataset = LoveDA(TRAIN_DIR, IMG_PATH, MASK_PATH, directories=URBAN_PATH)\n",
        "# No shuffle (waste of time), no drop last (we lose some data)\n",
        "\n",
        "num_workers = 2 if device.type == 'cuda' else 0\n",
        "\n",
        "#preprocessing_dataloader = DataLoader(preprocessing_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=num_workers)\n",
        "#_, avg, std = compute_avg_std(preprocessing_dataset, preprocessing_dataloader, device)\n",
        "\n",
        "# Poiché il modello è pretrainato su ImageNet, si usano media e varianza di ImageNet\n",
        "avg = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ-c2UvI9zLl"
      },
      "source": [
        "#### Training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8a0vk2FFpQ6D"
      },
      "outputs": [],
      "source": [
        "train_transform = A.Compose([\n",
        "    A.Normalize(mean=avg, std=std, p=1, always_apply=True, max_pixel_value=255),\n",
        "    A.Resize(resize, resize, p=1, always_apply=True)\n",
        "])\n",
        "train_dataset = LoveDA(TRAIN_DIR, IMG_PATH, MASK_PATH, directories=URBAN_PATH, transforms=train_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=num_workers, worker_init_fn=seed_worker, generator=g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fF9WvY_927b"
      },
      "source": [
        "#### Validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2zFQmoB76kv"
      },
      "outputs": [],
      "source": [
        "val_transform = A.Compose([\n",
        "    A.Normalize(mean=avg, std=std, p=1, always_apply=True, max_pixel_value=255)\n",
        "])\n",
        "val_dataset = LoveDA(VAL_DIR, IMG_PATH, MASK_PATH, directories=URBAN_PATH, transforms=val_transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=num_workers, worker_init_fn=seed_worker, generator=g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpUmP3uj-byv"
      },
      "source": [
        "### Training process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnRpfdeq_o0u"
      },
      "source": [
        "#### Model engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvUK7Dv_8Qol"
      },
      "outputs": [],
      "source": [
        "model = get_deeplab_v2(num_classes=num_classes, pretrain=True, pretrain_model_path=DEEPLAB_V2_WEIGHTS).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=w_decay)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4RC0eIiHoY2"
      },
      "source": [
        "#### Model handling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VBqKEft_0iB"
      },
      "source": [
        "#### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3g1ubaFc5AD"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "\n",
        "if RESUME_TRAINING:\n",
        "  start_epoch, model, optimizer, scheduler = resume_checkpoint(RESUME_PATH, model, optimizer, scheduler)\n",
        "else:\n",
        "  start_epoch = 0\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    print(\"### Training mode\")\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (images, masks) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs, _, _ = model(images)\n",
        "        loss = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if (i + 1) % 25 == 0:\n",
        "            print(f\"Processed {i + 1} batches, loss: {running_loss / (i+1)}\")\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(epoch_loss)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}\")\n",
        "    path=f\"/content/drive/MyDrive/loveDA_dataset/Model training/DeepLab/DeepLabV2_{num_epochs}_{learning_rate}_{step_size}_{gamma}_{resize}_{w_decay}_epoch{epoch}.pth.tar\"\n",
        "    save_checkpoint(path, epoch, model, optimizer, scheduler)\n",
        "\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPeSXzIwhqdD"
      },
      "source": [
        "#### Evaluation loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VLNQrTBhjDH"
      },
      "outputs": [],
      "source": [
        "# Requires saving the models for each epoch\n",
        "\n",
        "start_epoch_eval = 0\n",
        "\n",
        "eval_losses = []\n",
        "mious = []\n",
        "\n",
        "for epoch in range(start_epoch_eval, num_epochs):\n",
        "\n",
        "    model = get_deeplab_v2(num_classes=num_classes, pretrain=False)  # Assuming get_deeplab_v2 is defined\n",
        "\n",
        "    path = f\"/content/drive/MyDrive/loveDA_dataset/Model training/DeepLab/DeepLabV2_{num_epochs}_{learning_rate}_{step_size}_{gamma}_{resize}_{w_decay}_epoch{epoch}.pth.tar\"\n",
        "    _, model, _, _ = resume_checkpoint(path, model)\n",
        "\n",
        "    model.to(device)\n",
        "    print(\"### Evaluation mode\")\n",
        "    miou = 0.0 # Accumulator for mIoU\n",
        "    val_loss = 0.0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (images, masks) in enumerate(val_loader):\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            # loss\n",
        "            outputs= model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # mIoU\n",
        "            iou, _ = calculate_iou(outputs, masks, num_classes)\n",
        "            miou += iou\n",
        "\n",
        "            if (i + 1) % 25 == 0:\n",
        "                print(f\"Processed {i + 1} batches: loss {val_loss / (i+1)}, mIoU: {miou / (i+1)}\")\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "    eval_losses.append(val_loss)\n",
        "\n",
        "    miou /= len(val_loader)\n",
        "\n",
        "    mious.append(miou)\n",
        "\n",
        "    print(f\"Epoch: [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}, mIoU: {(miou * 100):.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D1oBEq_CwDn"
      },
      "source": [
        "### Metric calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qg_YfiVjCF1j"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "mean_latency, _, _, _ = calculate_latency_fps(model, device, 1024, 1024, num_epochs, ModelType.DEEPLAB)\n",
        "print(f\"Mean Latency: {mean_latency:.2f} ms\")\n",
        "\n",
        "calculate_flops_params(model, device, 1024, 1024, ModelType.DEEPLAB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ww8qQrf8bgye"
      },
      "source": [
        "# PIDNet Implementation (execute for any Step from 2b on)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ie5UdjLXxKEQ"
      },
      "source": [
        "## Model: PIDnet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHFohgLtsep5"
      },
      "source": [
        "#### FullModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zz72vdhb9cL2"
      },
      "outputs": [],
      "source": [
        "class FullModel(nn.Module):\n",
        "\n",
        "    def __init__(self, model, sem_loss, bd_loss):\n",
        "        super(FullModel, self).__init__()\n",
        "        self.model = model\n",
        "        self.sem_loss = sem_loss\n",
        "        self.bd_loss = bd_loss\n",
        "\n",
        "    def pixel_acc(self, pred, label):\n",
        "        _, preds = torch.max(pred, dim=1)\n",
        "        valid = (label != IGNORE_INDEX).long()\n",
        "        acc_sum = torch.sum(valid * (preds == label).long())\n",
        "        pixel_sum = torch.sum(valid)\n",
        "        acc = acc_sum.float() / (pixel_sum.float() + 1e-10)\n",
        "        return acc\n",
        "\n",
        "    def forward(self, inputs, labels, bd_gt, *args, **kwargs):\n",
        "        outputs = self.model(inputs, *args, **kwargs)\n",
        "\n",
        "        if labels is None:\n",
        "          h, w = inputs.size(2), inputs.size(3)\n",
        "        else:\n",
        "          h, w = labels.size(1), labels.size(2)\n",
        "\n",
        "        ph, pw = outputs[0].size(2), outputs[0].size(3)\n",
        "        if ph != h or pw != w:\n",
        "            for i in range(len(outputs)):\n",
        "                outputs[i] = F.interpolate(outputs[i], size=(\n",
        "                    h, w), mode='bilinear', align_corners=True)     #from original configs\n",
        "\n",
        "        if bd_gt is  None:\n",
        "            return None, outputs, None, None\n",
        "\n",
        "        acc  = self.pixel_acc(outputs[-2], labels)\n",
        "        loss_s = self.sem_loss(outputs[:-1], labels)\n",
        "\n",
        "        loss_b = self.bd_loss(outputs[-1], bd_gt)\n",
        "\n",
        "        filler = torch.ones_like(labels) * IGNORE_INDEX       #from original configs\n",
        "        bd_label = torch.where(F.sigmoid(outputs[-1][:,0,:,:])>0.8, labels, filler)\n",
        "\n",
        "        loss_sb = self.sem_loss([outputs[-2]], bd_label)\n",
        "\n",
        "        loss = loss_s + loss_b + loss_sb\n",
        "\n",
        "\n",
        "        return torch.unsqueeze(loss,0), outputs, acc, [loss_s, loss_b, loss_sb]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waOaWaJzszW_"
      },
      "source": [
        "#### Other functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKdWPZZHs4kU"
      },
      "outputs": [],
      "source": [
        "def get_seg_model(model_name, num_classes, pretrained_weights, imgnet_pretrained):\n",
        "\n",
        "    if 's' in model_name:\n",
        "        model = PIDNet(m=2, n=3, num_classes=num_classes, planes=32, ppm_planes=96, head_planes=128, augment=True)\n",
        "    elif 'm' in model_name:\n",
        "        model = PIDNet(m=2, n=3, num_classes=num_classes, planes=64, ppm_planes=96, head_planes=128, augment=True)\n",
        "    else:\n",
        "        model = PIDNet(m=3, n=4, num_classes=num_classes, planes=64, ppm_planes=112, head_planes=256, augment=True)\n",
        "\n",
        "    if imgnet_pretrained:\n",
        "        pretrained_state = torch.load(pretrained_weights, map_location='cpu')['state_dict']\n",
        "        model_dict = model.state_dict()\n",
        "        pretrained_state = {k: v for k, v in pretrained_state.items() if (k in model_dict and v.shape == model_dict[k].shape)}\n",
        "        model_dict.update(pretrained_state)\n",
        "        msg = 'Loaded {} parameters!'.format(len(pretrained_state))\n",
        "        print('Attention!!!')\n",
        "        print(msg)\n",
        "        print('Over!!!')\n",
        "        model.load_state_dict(model_dict, strict = False)\n",
        "    else:\n",
        "        pretrained_dict = torch.load(pretrained_weights, map_location='cpu')\n",
        "        if 'state_dict' in pretrained_dict:\n",
        "            pretrained_dict = pretrained_dict['state_dict']\n",
        "        model_dict = model.state_dict()\n",
        "        pretrained_dict = {k[6:]: v for k, v in pretrained_dict.items() if (k[6:] in model_dict and v.shape == model_dict[k[6:]].shape)}\n",
        "        msg = 'Loaded {} parameters!'.format(len(pretrained_dict))\n",
        "        print('Attention!!!')\n",
        "        print(msg)\n",
        "        print('Over!!!')\n",
        "        model_dict.update(pretrained_dict)\n",
        "        model.load_state_dict(model_dict, strict = False)\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_pred_model(name, num_classes):\n",
        "\n",
        "    if 's' in name:\n",
        "        model = PIDNet(m=2, n=3, num_classes=num_classes, planes=32, ppm_planes=96, head_planes=128, augment=False)\n",
        "    elif 'm' in name:\n",
        "        model = PIDNet(m=2, n=3, num_classes=num_classes, planes=64, ppm_planes=96, head_planes=128, augment=False)\n",
        "    else:\n",
        "        model = PIDNet(m=3, n=4, num_classes=num_classes, planes=64, ppm_planes=112, head_planes=256, augment=False)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p8kkdlmmd37"
      },
      "source": [
        "## Download pre-trained weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2kM2RnbF44b"
      },
      "outputs": [],
      "source": [
        "weights_dir = Path(PRETRAINED_WEIGHTS_DIR)\n",
        "if not weights_dir.exists():\n",
        "    weights_dir.mkdir(exist_ok=True)\n",
        "\n",
        "PIDNET_S_WEIGHTS = weights_dir / 'pidnet_s_imagenet_pretrained.pth'\n",
        "\n",
        "pidnet_s_weights = Path(PIDNET_S_WEIGHTS)\n",
        "if not pidnet_s_weights.exists():\n",
        "    # Replace with the correct Google Drive file ID\n",
        "\n",
        "    file_id = '1hIBp_8maRr60-B3PF0NVtaA6TYBvO4y-'\n",
        "    gdown.download(id=file_id, output=str(pidnet_s_weights), quiet=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZwedtEfnOOM"
      },
      "source": [
        "# Step 2b: Real-time semantic segmentation network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ofu29vcvCFb"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWfFzjriv59i"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaEZ_xJkv7e8"
      },
      "outputs": [],
      "source": [
        "resize = 512\n",
        "BATCH_SIZE = 6\n",
        "num_epochs = 20\n",
        "\n",
        "LR = 1e-3\n",
        "MOMEUNTUM = 0.9\n",
        "WEIGHT_DECAY = 1e-2\n",
        "STEP_SIZE = 10\n",
        "GAMMA = 0.1\n",
        "\n",
        "log_frequency = 25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCZ8MTPjup_T"
      },
      "source": [
        "### Dataset preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoxLxfHfuwZ9"
      },
      "source": [
        "#### Normalization metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vup_B8CouyYT"
      },
      "outputs": [],
      "source": [
        "num_workers = 2 if device.type == 'cuda' else 0\n",
        "\n",
        "# Poiché il modello è pretrainato su ImageNet, si usano media e varianza di ImageNet\n",
        "avg = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCzdv2TCu10X"
      },
      "source": [
        "#### Training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcRZO3dwucly"
      },
      "outputs": [],
      "source": [
        "train_transform = A.Compose([\n",
        "    A.Normalize(mean=avg, std=std, p=1, always_apply=True, max_pixel_value=255),\n",
        "    A.Resize(resize, resize, p=1, always_apply=True)\n",
        "])\n",
        "train_dataset = LoveDA(TRAIN_DIR, IMG_PATH, MASK_PATH, directories=URBAN_PATH, transforms=train_transform, bd=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=num_workers, worker_init_fn=seed_worker, generator=g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwFbFBRvu4LI"
      },
      "source": [
        "#### Validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_62pcuDKu59K"
      },
      "outputs": [],
      "source": [
        "# La resize è bene farla solo sul training set\n",
        "# La normalizzazione invece può essere applicata anche qui\n",
        "\n",
        "val_transform = A.Compose([\n",
        "    A.Normalize(mean=avg, std=std, p=1, always_apply=True, max_pixel_value=255)\n",
        "])\n",
        "val_dataset = LoveDA(VAL_DIR, IMG_PATH, MASK_PATH, directories=URBAN_PATH, transforms=val_transform, bd=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=num_workers, worker_init_fn=seed_worker, generator=g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpiF-7qPvzUy"
      },
      "source": [
        "### Training process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgQujuEIwUHO"
      },
      "source": [
        "#### Model engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExGpSNtXwaXe",
        "outputId": "c11270cd-688f-4314-9a4a-f961444ebb23"
      },
      "outputs": [],
      "source": [
        "pidnet = get_seg_model(\"pidnet_s\", num_classes, PIDNET_S_WEIGHTS,imgnet_pretrained=True)\n",
        "model = FullModel(pidnet, sem_loss=CrossEntropy(ignore_label=IGNORE_INDEX), bd_loss=BondaryLoss())\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = LR, momentum = MOMEUNTUM, weight_decay = WEIGHT_DECAY)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgdaTcMTxkQI"
      },
      "source": [
        "#### Evaluate function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JG1ocX7Fxmcd"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, dataloader, device, ) -> tuple:\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    data_len = 0\n",
        "    iou_scores = 0.0\n",
        "\n",
        "    for (inputs, masks, boundaries) in dataloader:\n",
        "\n",
        "        data_len += inputs.size(0)\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        masks = masks.to(device)\n",
        "        boundaries = boundaries.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        loss, outputs, acc, loss_list = model(inputs, masks, boundaries)\n",
        "        running_loss += loss.item()*inputs.size(0)\n",
        "\n",
        "        # Calculate mIoU\n",
        "        iou, _ = calculate_iou(outputs[1], masks, num_classes)\n",
        "        iou_scores += iou*inputs.size(0)\n",
        "\n",
        "    mIoU = iou_scores/data_len\n",
        "    loss = running_loss/data_len\n",
        "\n",
        "\n",
        "    return loss, mIoU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9jPEHbfw_uG"
      },
      "source": [
        "#### Training/evaluation loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4NPhxYwxEAD"
      },
      "outputs": [],
      "source": [
        "val_losses, val_accuracies = [], []\n",
        "train_losses, train_accuracies = [], []\n",
        "miou_scores = []\n",
        "best_mIoU = -1\n",
        "best_num_epochs = None\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    current_step = 0\n",
        "    train_loss = 0.0\n",
        "    model.train()\n",
        "    for (inputs, masks, boundaries) in train_loader:\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        boundaries = boundaries.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss, outputs, pixel_acc, [loss_s, loss_b, loss_sb] = model(inputs, masks, boundaries)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if current_step % log_frequency == 0:\n",
        "            print(f\"Epoch {epoch+1}, Iteration {current_step}, Loss: {loss.item():.3f} Loss_s: {loss_s.item():.3f} Loss_b: {loss_b.item():.3f} Loss_sb: {loss_sb.item():.3f}\")\n",
        "\n",
        "        current_step += 1\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    print(f\"End of Epoch {epoch+1}\")\n",
        "    print(f\"Training loss: {train_loss:.5f}\")\n",
        "\n",
        "\n",
        "    val_loss, val_mean_iou = evaluate(model, val_loader, device)\n",
        "    print(f\"Validation mIoU: {val_mean_iou*100:.3f}%, Validation loss: {val_loss:.5f}\")\n",
        "\n",
        "    val_losses.append(val_loss)\n",
        "    miou_scores.append(val_mean_iou)\n",
        "    val_accuracies.append(val_mean_iou.cpu().item())\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    print()\n",
        "    # Scheduler is None if learning rate is constant\n",
        "    if scheduler is not None:\n",
        "        scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj0QJgtO_IiW"
      },
      "source": [
        "### Metric calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cw5AGBi6_IiW"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "mean_latency, _, _, _ = calculate_latency_fps(model, device, 1024, 1024, num_epochs, ModelType.PIDNET)\n",
        "print(f\"Mean Latency: {mean_latency:.2f} ms\")\n",
        "calculate_flops_params(model, device, 1024, 1024, ModelType.PIDNET)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsZrRWKDYLjM"
      },
      "source": [
        "# Step 3a: Evaluating the domain shift problem in Semantic Segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtM5dWX-Yc5K"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LHK7BBwYc5L"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHYrmBZSYc5L"
      },
      "outputs": [],
      "source": [
        "resize = 512\n",
        "BATCH_SIZE = 6\n",
        "num_epochs = 20\n",
        "\n",
        "LR = 1e-3\n",
        "MOMEUNTUM = 0.9\n",
        "WEIGHT_DECAY = 1e-2\n",
        "STEP_SIZE = 10\n",
        "GAMMA = 0.1\n",
        "\n",
        "log_frequency = 25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3myb3foIYc5L"
      },
      "source": [
        "### Dataset preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxLdbQm_Yc5L"
      },
      "source": [
        "#### Normalization metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzyfFAqyYc5L"
      },
      "outputs": [],
      "source": [
        "num_workers = 2 if device.type == 'cuda' else 0\n",
        "\n",
        "# Poiché il modello è pretrainato su ImageNet, si usano media e varianza di ImageNet\n",
        "avg = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QMBfrdVYc5L"
      },
      "source": [
        "#### Training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWjByEsQYc5L"
      },
      "outputs": [],
      "source": [
        "train_transform = A.Compose([\n",
        "    A.Normalize(mean=avg, std=std, p=1, always_apply=True, max_pixel_value=255),\n",
        "    A.Resize(resize, resize, p=1, always_apply=True)\n",
        "])\n",
        "\n",
        "train_dataset = LoveDA(TRAIN_DIR, IMG_PATH, MASK_PATH, directories=URBAN_PATH, bd=True, transforms=train_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=num_workers, worker_init_fn=seed_worker, generator=g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqwfjp-YYc5M"
      },
      "source": [
        "#### Validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bsc_hSQ1Yc5M"
      },
      "outputs": [],
      "source": [
        "val_transform = A.Compose([\n",
        "    A.Normalize(mean=avg, std=std, p=1, always_apply=True, max_pixel_value=255)\n",
        "])\n",
        "\n",
        "val_dataset = LoveDA(VAL_DIR, IMG_PATH, MASK_PATH, directories=RURAL_PATH, bd=True, transforms=val_transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=num_workers, worker_init_fn=seed_worker, generator=g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks8YNvchYc5M"
      },
      "source": [
        "### Training process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htCmAmq0Yc5M"
      },
      "source": [
        "#### Model engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8Jj6eYiYc5M",
        "outputId": "e17b6364-d588-4c9b-9d93-6a2acf9d5ad5"
      },
      "outputs": [],
      "source": [
        "pidnet = get_seg_model(\"pidnet_s\", num_classes, PIDNET_S_WEIGHTS,imgnet_pretrained=True)\n",
        "model = FullModel(pidnet, sem_loss=CrossEntropy(ignore_label=IGNORE_INDEX), bd_loss=BondaryLoss())\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = LR, momentum = MOMEUNTUM, weight_decay = WEIGHT_DECAY)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = STEP_SIZE, gamma = GAMMA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBGrbjXaYc5M"
      },
      "source": [
        "#### Evaluate function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HILKj4bYc5M"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, dataloader, device, ) -> tuple:\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    data_len = 0\n",
        "    iou_scores = 0.0\n",
        "    ious_per_class = torch.zeros(num_classes)\n",
        "\n",
        "    for (inputs, masks, boundaries) in dataloader:\n",
        "\n",
        "        data_len += inputs.size(0)\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        masks = masks.to(device)\n",
        "        boundaries = boundaries.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        loss, outputs, acc, loss_list = model(inputs, masks, boundaries)\n",
        "        running_loss += loss.item()*inputs.size(0)\n",
        "\n",
        "        # Calculate mIoU\n",
        "        iou, iou_per_class = calculate_iou(outputs[1], masks, num_classes)\n",
        "        iou_scores += iou*inputs.size(0)\n",
        "        ious_per_class+=iou_per_class.cpu()*inputs.size(0)\n",
        "\n",
        "    mIoU = iou_scores/data_len\n",
        "    loss = running_loss/data_len\n",
        "    ious_per_class/=data_len\n",
        "\n",
        "\n",
        "    return loss, mIoU, ious_per_class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyZhv1kVYc5N"
      },
      "source": [
        "#### Training/evaluation loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziNXwVeiYc5N",
        "outputId": "dc1e6bda-2fdb-47ac-d2ae-4cc100431df4"
      },
      "outputs": [],
      "source": [
        "val_losses, val_accuracies = [], []\n",
        "train_losses, train_accuracies = [], []\n",
        "miou_scores = []\n",
        "best_mIoU = -1\n",
        "best_num_epochs = None\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    current_step = 0\n",
        "    train_loss = 0.0\n",
        "    model.train()\n",
        "    for (inputs, masks, boundaries) in train_loader:\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        boundaries = boundaries.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss, outputs, pixel_acc, [loss_s, loss_b, loss_sb] = model(inputs, masks, boundaries)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if current_step % log_frequency == 0:\n",
        "            print(f\"Epoch {epoch+1}, Iteration {current_step}, Loss: {loss.item():.3f} Loss_s: {loss_s.item():.3f} Loss_b: {loss_b.item():.3f} Loss_sb: {loss_sb.item():.3f}\")\n",
        "\n",
        "        current_step += 1\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    print(f\"End of Epoch {epoch+1}\")\n",
        "    print(f\"Training loss: {train_loss:.5f}\")\n",
        "\n",
        "\n",
        "    val_loss, val_mean_iou, ious_per_class = evaluate(model, val_loader, device)\n",
        "\n",
        "    print(f\"Validation mIoU: {val_mean_iou*100:.3f}%, Validation loss: {val_loss:.5f}\")\n",
        "\n",
        "    for i, cat in enumerate(categories.keys()):\n",
        "        print(f\"{cat} mIoU: {ious_per_class[i]*100:.3f}\")\n",
        "\n",
        "    val_losses.append(val_loss)\n",
        "    train_losses.append(train_loss)\n",
        "    miou_scores.append(val_mean_iou)\n",
        "\n",
        "    print()\n",
        "    # Scheduler is None if learning rate is constant\n",
        "    if scheduler is not None:\n",
        "        scheduler.step()\n",
        "\n",
        "miou_scores = list(map(lambda x: x.item(), miou_scores))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cx9j7hzvL_M0"
      },
      "source": [
        "### Evaluate using saved models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        },
        "id": "Ykk41_c7-xrp",
        "outputId": "e33346c0-fc4c-45d4-fb3c-252f6c15946a"
      },
      "outputs": [],
      "source": [
        "validation_losses = []\n",
        "miou_scores = []\n",
        "\n",
        "starting_epoch = 0\n",
        "\n",
        "for epoch in range(starting_epoch, num_epochs):\n",
        "    model_path = f\"/content/drive/MyDrive/loveDA_dataset/Model training/PIDNet_{num_epochs}_{LR}_{STEP_SIZE}_{GAMMA}_{resize}_{WEIGHT_DECAY}_{MOMEUNTUM}_{GAMMA}_epoch{epoch}.pth\"\n",
        "\n",
        "    pidnet = get_seg_model(\"pidnet_s\", num_classes, PIDNET_S_WEIGHTS,imgnet_pretrained=True)\n",
        "    model = FullModel(pidnet, sem_loss=CrossEntropy(ignore_label=IGNORE_INDEX), bd_loss=BondaryLoss())\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    val_loss, val_mean_iou, ious_per_class = evaluate(model, val_loader, device)\n",
        "\n",
        "    validation_losses.append(val_loss)\n",
        "    miou_scores.append(val_mean_iou)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}\")\n",
        "    print(f\"Validation mIoU: {val_mean_iou*100:.3f}%, Validation loss: {val_loss:.5f}\")\n",
        "\n",
        "    for i, cat in enumerate(categories.keys()):\n",
        "        print(f\"{cat} mIoU: {ious_per_class[i]*100:.3f}\")\n",
        "\n",
        "    print()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOz--WLUg2K4"
      },
      "source": [
        "# Step 3b: Data augmentations to reduce the domain shift"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UvUsl_sg2K4"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KdKOjqdg2K4"
      },
      "outputs": [],
      "source": [
        "resize = 512\n",
        "BATCH_SIZE = 6\n",
        "num_epochs = 20\n",
        "\n",
        "LR = 1e-3\n",
        "MOMEUNTUM = 0.9\n",
        "WEIGHT_DECAY = 1e-2\n",
        "STEP_SIZE = 10\n",
        "GAMMA = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS8KiIRmg2K4"
      },
      "source": [
        "### Dataset preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOF0Fu6vg2K4"
      },
      "source": [
        "#### Normalization metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKLXZraAg2K4"
      },
      "outputs": [],
      "source": [
        "num_workers = 2 if device.type == 'cuda' else 0\n",
        "\n",
        "# Poiché il modello è pretrainato su ImageNet, si usano media e varianza di ImageNet\n",
        "avg = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyigoPYag2K4"
      },
      "source": [
        "#### Augmentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5YCifjTg2K5"
      },
      "outputs": [],
      "source": [
        "aug_prob = 0.5\n",
        "\n",
        "augmentations = [\n",
        "    A.ShiftScaleRotate(p=1),\n",
        "    A.GridDistortion(p=1),\n",
        "    A.RandomCrop(height=resize, width=resize, p=1),\n",
        "    A.HorizontalFlip(p=1),\n",
        "    A.GaussianBlur(p=1),\n",
        "    A.GridDropout(p=1),\n",
        "    A.ColorJitter(p=1),\n",
        "    A.GaussNoise(var_limit=(0.2, 0.3), p=1),\n",
        "    A.ChannelDropout(p=1),\n",
        "    A.RandomSizedCrop(min_max_height=(resize//8, resize), height=resize, width=resize, p=1),\n",
        "]\n",
        "\n",
        "selected_indices = [2]\n",
        "\n",
        "selected_augmentations = A.Compose([augmentations[i] for i in selected_indices], p=aug_prob)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LY9ONa1Ig2K5"
      },
      "source": [
        "#### Training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARH3hXgpg2K5"
      },
      "outputs": [],
      "source": [
        "train_transform = A.Compose([\n",
        "    A.Normalize(mean=avg, std=std, p=1, always_apply=True, max_pixel_value=255),\n",
        "    selected_augmentations,\n",
        "    A.Resize(resize, resize, p=1, always_apply=True)\n",
        "])\n",
        "\n",
        "train_dataset = LoveDA(TRAIN_DIR, IMG_PATH, MASK_PATH, directories=URBAN_PATH, bd=True, transforms=train_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=num_workers, worker_init_fn=seed_worker, generator=g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXT2zPn2g2K5"
      },
      "source": [
        "#### Validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87g4SX0_g2K5"
      },
      "outputs": [],
      "source": [
        "val_transform = A.Compose([\n",
        "    A.Normalize(mean=avg, std=std, p=1, always_apply=True, max_pixel_value=255)\n",
        "])\n",
        "\n",
        "val_dataset = LoveDA(VAL_DIR, IMG_PATH, MASK_PATH, directories=RURAL_PATH, bd=True, transforms=val_transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=num_workers, worker_init_fn=seed_worker, generator=g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pYGh7_Wg2K5"
      },
      "source": [
        "### Training process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz14zhkQg2K5"
      },
      "source": [
        "#### Model engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZD5mOHBg2K5",
        "outputId": "a49a8553-e2f7-4de6-d02a-cec361836181"
      },
      "outputs": [],
      "source": [
        "pidnet = get_seg_model(\"pidnet_s\", num_classes, PIDNET_S_WEIGHTS,imgnet_pretrained=True)\n",
        "model = FullModel(pidnet, sem_loss=CrossEntropy(ignore_label=IGNORE_INDEX), bd_loss=BondaryLoss())\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = LR, momentum = MOMEUNTUM, weight_decay = WEIGHT_DECAY)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = STEP_SIZE, gamma = GAMMA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcaSQTY9g2K5"
      },
      "source": [
        "#### Evaluate function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTBuw2H1g2K5"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, dataloader, device, ) -> tuple:\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    data_len = 0\n",
        "    iou_scores = 0.0\n",
        "    ious_per_class = torch.zeros(num_classes)\n",
        "\n",
        "    for (inputs, masks, boundaries) in dataloader:\n",
        "\n",
        "        data_len += inputs.size(0)\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        masks = masks.to(device)\n",
        "        boundaries = boundaries.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        loss, outputs, acc, loss_list = model(inputs, masks, boundaries)\n",
        "        running_loss += loss.item()*inputs.size(0)\n",
        "\n",
        "        # Calculate mIoU\n",
        "        iou, iou_per_class = calculate_iou(outputs[1], masks, num_classes)\n",
        "        iou_scores += iou*inputs.size(0)\n",
        "        ious_per_class+=iou_per_class.cpu()*inputs.size(0)\n",
        "\n",
        "    mIoU = iou_scores/data_len\n",
        "    loss = running_loss/data_len\n",
        "    ious_per_class/=data_len\n",
        "\n",
        "\n",
        "    return loss, mIoU, ious_per_class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sayvzvTNg2K5"
      },
      "source": [
        "#### Training/evaluation loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8H5Ptzeg2K5",
        "outputId": "8900677c-cad6-49e3-ce25-6084aa0ad708"
      },
      "outputs": [],
      "source": [
        "val_losses, val_accuracies = [], []\n",
        "train_losses, train_accuracies = [], []\n",
        "miou_scores = []\n",
        "miou_per_category = dict()\n",
        "best_mIoU = -1\n",
        "best_num_epochs = None\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    current_step = 0\n",
        "    train_loss = 0.0\n",
        "    model.train()\n",
        "    for (inputs, masks, boundaries) in train_loader:\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        boundaries = boundaries.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss, outputs, pixel_acc, [loss_s, loss_b, loss_sb] = model(inputs, masks, boundaries)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if current_step % log_frequency == 0:\n",
        "            print(f\"Epoch {epoch+1}, Iteration {current_step}, Loss: {loss.item():.3f} Loss_s: {loss_s.item():.3f} Loss_b: {loss_b.item():.3f} Loss_sb: {loss_sb.item():.3f}\")\n",
        "\n",
        "        current_step += 1\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    print(f\"End of Epoch {epoch+1}\")\n",
        "    print(f\"Training loss: {train_loss:.5f}\")\n",
        "\n",
        "\n",
        "    val_loss, val_mean_iou, ious_per_class = evaluate(model, val_loader, device)\n",
        "\n",
        "\n",
        "    print(f\"Validation mIoU: {val_mean_iou*100:.3f}%, Validation loss: {val_loss:.5f}\")\n",
        "\n",
        "    for i, cat in enumerate(categories.keys()):\n",
        "        if cat in miou_per_category:\n",
        "            miou_per_category[cat] += [ious_per_class[i].item()]\n",
        "        else:\n",
        "            miou_per_category[cat] = [ious_per_class[i].item()]\n",
        "        print(f\"{cat} mIoU: {ious_per_class[i]*100:.3f}%\")\n",
        "\n",
        "    val_losses.append(val_loss)\n",
        "    train_losses.append(train_loss)\n",
        "    miou_scores.append(val_mean_iou)\n",
        "\n",
        "\n",
        "    print()\n",
        "    # Scheduler is None if learning rate is constant\n",
        "    if scheduler is not None:\n",
        "        scheduler.step()\n",
        "\n",
        "miou_scores = list(map(lambda x: x.item(), miou_scores))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8iauVEpE4L_"
      },
      "source": [
        "# Step 4a: Adversarial Domain Adaptation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdtc4sem8CWj"
      },
      "source": [
        "### FC discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpuJzsnTJLGs"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RUVAzcqGn8k"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGe5tURwGn8k"
      },
      "outputs": [],
      "source": [
        "resize = 512\n",
        "BATCH_SIZE = 6\n",
        "num_epochs = 20\n",
        "\n",
        "LR = 1e-3\n",
        "MOMEUNTUM = 0.9\n",
        "WEIGHT_DECAY = 1e-2\n",
        "STEP_SIZE = 10\n",
        "GAMMA = 0.1\n",
        "\n",
        "LAMBDA = 1e-3\n",
        "\n",
        "log_frequency = 50\n",
        "\n",
        "LR_D = 1e-5\n",
        "WEIGHT_DECAY_D = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6LfLAKqGn8k"
      },
      "source": [
        "### Dataset preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkJg7vN6Gn8k"
      },
      "source": [
        "#### Normalization metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WotCMM15Gn8l"
      },
      "outputs": [],
      "source": [
        "num_workers = 2 if device.type == 'cuda' else 0\n",
        "\n",
        "# Poiché il modello è pretrainato su ImageNet, si usano media e varianza di ImageNet\n",
        "avg = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXngZ-RUGn8l"
      },
      "source": [
        "#### Training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMnHJZszGn8l"
      },
      "outputs": [],
      "source": [
        "train_transform = A.Compose([\n",
        "    A.Normalize(mean=avg, std=std, p=1, always_apply=True, max_pixel_value=255),\n",
        "    A.RandomCrop(height=resize, width=resize, p=0.5),\n",
        "    A.Resize(resize, resize, p=1, always_apply=True)\n",
        "])\n",
        "\n",
        "train_dataset = LoveDA(TRAIN_DIR, IMG_PATH, MASK_PATH, directories=URBAN_PATH, bd=True, transforms=train_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=num_workers, worker_init_fn=seed_worker, generator=g)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQD_HWhvU1kA"
      },
      "outputs": [],
      "source": [
        "train_transform_target = A.Compose([\n",
        "    A.Normalize(mean=avg, std=std, p=1, always_apply=True, max_pixel_value=255),\n",
        "    #A.RandomCrop(height=resize, width=resize, p=0.5),\n",
        "    A.Resize(resize, resize, p=1, always_apply=True)\n",
        "])\n",
        "\n",
        "train_dataset_target = LoveDA(TRAIN_DIR, IMG_PATH, MASK_PATH, directories=RURAL_PATH, bd=True, transforms=train_transform_target)\n",
        "train_loader_target = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=num_workers, worker_init_fn=seed_worker, generator=g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUIuMk6YGn8l"
      },
      "source": [
        "#### Validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLnJqJN5Gn8l"
      },
      "outputs": [],
      "source": [
        "val_transform = A.Compose([\n",
        "    A.Normalize(mean=avg, std=std, p=1, always_apply=True, max_pixel_value=255)\n",
        "])\n",
        "\n",
        "val_dataset = LoveDA(VAL_DIR, IMG_PATH, MASK_PATH, directories=RURAL_PATH, bd=True, transforms=val_transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=num_workers, worker_init_fn=seed_worker, generator=g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQN-FpydGn8l"
      },
      "source": [
        "### Training process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQNdlT0xGn8l"
      },
      "source": [
        "#### Model engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrbpPpt0Gn8l",
        "outputId": "af7302de-0139-4506-f4ac-44221ac13c28"
      },
      "outputs": [],
      "source": [
        "pidnet = get_seg_model(\"pidnet_s\", num_classes, PIDNET_S_WEIGHTS,imgnet_pretrained=True)\n",
        "model = FullModel(pidnet, sem_loss=CrossEntropy(ignore_label=IGNORE_INDEX), bd_loss=BondaryLoss())\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = LR, momentum = MOMEUNTUM, weight_decay = WEIGHT_DECAY)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = STEP_SIZE, gamma = GAMMA)\n",
        "\n",
        "domain_criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model_domain = FCDiscriminator(num_classes=7)\n",
        "model_domain = model_domain.to(device)\n",
        "domain_optimizer = torch.optim.Adam(model_domain.parameters(), lr = LR_D, weight_decay = WEIGHT_DECAY_D)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSsNi2hXGn8m"
      },
      "source": [
        "#### Evaluate function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Wg2CMaCGn8m"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, dataloader, device, ) -> tuple:\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    data_len = 0\n",
        "    iou_scores = 0.0\n",
        "    ious_per_class = torch.zeros(num_classes)\n",
        "\n",
        "    for (inputs, masks, boundaries) in dataloader:\n",
        "\n",
        "        data_len += inputs.size(0)\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        masks = masks.to(device)\n",
        "        boundaries = boundaries.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        loss, outputs, _, _ = model(inputs, masks, boundaries)\n",
        "        running_loss += loss.item()*inputs.size(0)\n",
        "\n",
        "        # Calculate mIoU\n",
        "        iou, iou_per_class = calculate_iou(outputs[1], masks, num_classes)\n",
        "        iou_scores += iou*inputs.size(0)\n",
        "        ious_per_class+=iou_per_class.cpu()*inputs.size(0)\n",
        "\n",
        "    mIoU = iou_scores/data_len\n",
        "    loss = running_loss/data_len\n",
        "    ious_per_class/=data_len\n",
        "\n",
        "\n",
        "    return loss, mIoU, ious_per_class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQNZDlwQMdI0"
      },
      "source": [
        "#### Training/evaluation loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WdtMH-UgSESx",
        "outputId": "02173f8c-5e46-42d4-91a1-1f8280647a25"
      },
      "outputs": [],
      "source": [
        "val_losses, val_accuracies = [], []\n",
        "train_losses, train_accuracies = [], []\n",
        "miou_scores = []\n",
        "best_mIoU = -1\n",
        "best_num_epochs = None\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    current_step = 0\n",
        "    running_source_loss_seg = 0.0\n",
        "\n",
        "    loss_G, loss_D = 0, 0\n",
        "\n",
        "    model.train()\n",
        "    model_domain.train()\n",
        "\n",
        "\n",
        "    for (inputs, masks, boundaries), (target_inputs, _, _) in zip(train_loader, train_loader_target):\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        masks = masks.to(device)\n",
        "        boundaries = boundaries.to(device)\n",
        "        target_inputs = target_inputs.to(device)\n",
        "\n",
        "        # Train G\n",
        "        for param in model_domain.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        domain_optimizer.zero_grad()\n",
        "\n",
        "        ## train with source\n",
        "        source_loss, [_, source_PIDNET_output, _], _, _ = model(inputs, masks, boundaries)\n",
        "        source_loss.backward()\n",
        "        running_source_loss_seg += source_loss.item()\n",
        "\n",
        "        ## train with target\n",
        "        _, [_, target_PIDNET_output, _], _, _ = model(target_inputs, None, None)\n",
        "        preds = F.softmax(target_PIDNET_output, dim=1)\n",
        "        D_out = model_domain(preds)\n",
        "\n",
        "        domain_loss = LAMBDA * domain_criterion(D_out, torch.zeros_like(D_out))\n",
        "        domain_loss.backward()\n",
        "        loss_G += domain_loss.item()\n",
        "\n",
        "        # Train D\n",
        "\n",
        "        for param in model_domain.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        ## train with source\n",
        "        source_PIDNET_output = source_PIDNET_output.detach()\n",
        "        preds = F.softmax(source_PIDNET_output, dim=1)\n",
        "        D_out = model_domain(preds)\n",
        "\n",
        "        domain_loss = domain_criterion(D_out, torch.zeros_like(D_out))\n",
        "        domain_loss = domain_loss / 2\n",
        "        domain_loss.backward()\n",
        "        loss_D += domain_loss.item()\n",
        "\n",
        "        ## train with target\n",
        "        target_PIDNET_output = target_PIDNET_output.detach()\n",
        "        preds = F.softmax(target_PIDNET_output, dim=1)\n",
        "        D_out = model_domain(preds)\n",
        "\n",
        "        domain_loss = domain_criterion(D_out, torch.ones_like(D_out))\n",
        "        domain_loss = domain_loss / 2\n",
        "        domain_loss.backward()\n",
        "        loss_D += domain_loss.item()\n",
        "\n",
        "        clip_grad.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), max_norm=35, norm_type=2)\n",
        "        clip_grad.clip_grad_norm_(filter(lambda p: p.requires_grad, model_domain.parameters()), max_norm=35, norm_type=2)\n",
        "        optimizer.step()\n",
        "        domain_optimizer.step()\n",
        "\n",
        "\n",
        "        if current_step % log_frequency == 0:\n",
        "            print(f\"Epoch {epoch+1}, Iteration {current_step}, Source loss: {running_source_loss_seg/(current_step+1):5f}, Domain loss: {loss_G/(current_step+1):.5f} ({loss_D/(current_step+1):.5f}\")\n",
        "        current_step += 1\n",
        "\n",
        "    train_loss = running_source_loss_seg/len(train_loader)\n",
        "    train_domain_loss_G = loss_G/len(train_loader)\n",
        "    train_domain_loss_D = loss_D/len(train_loader)\n",
        "\n",
        "    print(f\"End of Epoch {epoch+1}\")\n",
        "    print(f\"Training loss: {train_loss:.5f}\")\n",
        "    print(f\"Domain loss G: {train_domain_loss_G:.5f}\")\n",
        "    print(f\"Domain loss D: {train_domain_loss_D:.5f}\")\n",
        "\n",
        "    val_loss, val_mean_iou, ious_per_class = evaluate(model, val_loader, device)\n",
        "\n",
        "    path=f\"/content/drive/MyDrive/loveDA_dataset/Model training/PIDNet/PIDNet_{num_epochs}_{LR}_{STEP_SIZE}_{GAMMA}_{resize}_{WEIGHT_DECAY}_{MOMEUNTUM}_{GAMMA}_epoch{epoch}_DA.pth\"\n",
        "    torch.save(model.state_dict(), path)\n",
        "    print(f\"Validation mIoU: {val_mean_iou*100:.3f}%, Validation loss: {val_loss:.5f}\")\n",
        "\n",
        "    for i, cat in enumerate(categories.keys()):\n",
        "        print(f\"{cat} mIoU: {ious_per_class[i]*100:.3f}\")\n",
        "\n",
        "    val_losses.append(val_loss)\n",
        "    train_losses.append(train_loss)\n",
        "    miou_scores.append(val_mean_iou)\n",
        "\n",
        "    print()\n",
        "    # Scheduler is None if learning rate is constant\n",
        "    if scheduler is not None:\n",
        "        scheduler.step()\n",
        "\n",
        "miou_scores = list(map(lambda x: x.item()*100, miou_scores))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnGafopawTBl"
      },
      "source": [
        "# Step 4b: Image-to-Image Domain Adaptation\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw2FHVfqwVQ2"
      },
      "source": [
        "## Mix & EMA model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jgunDqvwYVb"
      },
      "outputs": [],
      "source": [
        "def oneMix(mask, data = None, target = None):\n",
        "    #Mix\n",
        "    if not (data is None):\n",
        "        stackedMask0, _ = torch.broadcast_tensors(mask[0], data[0])\n",
        "        data = (stackedMask0*data[0]+(1-stackedMask0)*data[1]).unsqueeze(0)\n",
        "    if not (target is None):\n",
        "        stackedMask0, _ = torch.broadcast_tensors(mask[0], target[0])\n",
        "        target = (stackedMask0*target[0]+(1-stackedMask0)*target[1]).unsqueeze(0)\n",
        "    return data, target\n",
        "\n",
        "\n",
        "def generate_class_mask(pred, classes):\n",
        "    pred, classes = torch.broadcast_tensors(pred.unsqueeze(0), classes.unsqueeze(1).unsqueeze(2))\n",
        "    N = pred.eq(classes).sum(0)\n",
        "    return N\n",
        "\n",
        "\n",
        "def mix(parameters, data=None, target=None):\n",
        "    assert ((data is not None) or (target is not None))\n",
        "    data, target = oneMix(mask = parameters[\"Mix\"], data = data, target = target)\n",
        "    return data, target\n",
        "\n",
        "def update_ema_variables(ema_model, model, alpha_teacher, iteration):\n",
        "    # Use the \"true\" average until the exponential average is more correct\n",
        "    alpha_teacher = min(1 - 1 / (iteration + 1), alpha_teacher)\n",
        "\n",
        "    for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n",
        "        #ema_param.data.mul_(alpha).add_(1 - alpha, param.data)\n",
        "        ema_param.data[:] = alpha_teacher * ema_param[:].data[:] + (1 - alpha_teacher) * param[:].data[:]\n",
        "    return ema_model\n",
        "\n",
        "def create_ema_model(model):\n",
        "    pidnet = get_seg_model(\"pidnet_s\", num_classes, PIDNET_S_WEIGHTS,imgnet_pretrained=True)\n",
        "    ema_model = FullModel(pidnet, sem_loss=CrossEntropy(ignore_label=IGNORE_INDEX), bd_loss=BondaryLoss())\n",
        "    for param in ema_model.parameters():\n",
        "        param.detach_()\n",
        "    mp = list(model.parameters())\n",
        "    mcp = list(ema_model.parameters())\n",
        "    n = len(mp)\n",
        "    for i in range(0, n):\n",
        "        mcp[i].data[:] = mp[i].data[:].clone()\n",
        "    return ema_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f0_nsCA3GTM"
      },
      "source": [
        "## Unlabeled loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7scEnVbDhhjG"
      },
      "outputs": [],
      "source": [
        "def calc_U_loss(outputs, targets_u, sem_loss, bd_loss):\n",
        "    loss_s = sem_loss(outputs[:-1], targets_u)\n",
        "\n",
        "    bd_gt = np.zeros_like(targets_u.cpu().numpy(), dtype=np.float32)\n",
        "    for i, m in enumerate(targets_u):\n",
        "        bd_gt[i] = generate_bd(m.cpu().numpy().astype(np.uint8))\n",
        "\n",
        "    bd_gt = torch.from_numpy(bd_gt).to(device)\n",
        "\n",
        "    loss_b = bd_loss(outputs[-1], bd_gt)\n",
        "\n",
        "    filler = torch.ones_like(targets_u) * IGNORE_INDEX       #from original configs\n",
        "    bd_label = torch.where(F.sigmoid(outputs[-1][:,0,:,:])>0.8, targets_u, filler)\n",
        "\n",
        "    loss_sb = sem_loss([outputs[-2]], bd_label)\n",
        "\n",
        "    return loss_s + loss_b + loss_sb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jwas5Vutfq4I"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtH0vOD5frEy"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gr1UCvANfrEz"
      },
      "outputs": [],
      "source": [
        "LR = 1e-3\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 1e-2\n",
        "num_epochs = 20\n",
        "STEP_SIZE = 10\n",
        "GAMMA = 0.1\n",
        "\n",
        "BATCH_SIZE = 6\n",
        "resize = 512\n",
        "pixel_weight = \"threshold_uniform\"\n",
        "#pixel_weight = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsikOIN6frEz"
      },
      "source": [
        "### Dataset preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B33DP3uKfrEz"
      },
      "source": [
        "#### Normalization metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbrnZErJfrEz"
      },
      "outputs": [],
      "source": [
        "num_workers = 2 if device.type == 'cuda' else 0\n",
        "\n",
        "# Poiché il modello è pretrainato su ImageNet, si usano media e varianza di ImageNet\n",
        "avg = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJrdKqhmfrEz"
      },
      "source": [
        "#### Training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0dTV1T8frEz"
      },
      "outputs": [],
      "source": [
        "train_transform = A.Compose([\n",
        "    A.Normalize(mean=avg, std=std, p=1, always_apply=True, max_pixel_value=255),\n",
        "    A.RandomCrop(height=resize, width=resize, p=0.5),\n",
        "    A.Resize(resize, resize, p=1, always_apply=True)\n",
        "])\n",
        "\n",
        "train_transform_target = A.Compose([\n",
        "    A.Normalize(mean=avg, std=std, p=1, always_apply=True, max_pixel_value=255),\n",
        "    A.RandomCrop(height=resize, width=resize, p=0.5),\n",
        "    A.Resize(resize, resize, p=1, always_apply=True)\n",
        "])\n",
        "\n",
        "train_dataset = LoveDA(TRAIN_DIR, IMG_PATH, MASK_PATH, directories=URBAN_PATH, bd=True, transforms=train_transform)\n",
        "source_trainloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=num_workers, worker_init_fn=seed_worker, generator=g)\n",
        "train_dataset_target = LoveDA(TRAIN_DIR, IMG_PATH, MASK_PATH, directories=RURAL_PATH, bd=True, transforms=train_transform_target)\n",
        "target_trainloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=num_workers, worker_init_fn=seed_worker, generator=g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCw4hGa4frEz"
      },
      "source": [
        "#### Validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-co18kV2frEz"
      },
      "outputs": [],
      "source": [
        "# La resize è bene farla solo sul training set\n",
        "# La normalizzazione invece può essere applicata anche qui\n",
        "\n",
        "val_transform = A.Compose([\n",
        "    A.Normalize(mean=avg, std=std, p=1, always_apply=True, max_pixel_value=255)\n",
        "])\n",
        "\n",
        "val_dataset = LoveDA(VAL_DIR, IMG_PATH, MASK_PATH, directories=RURAL_PATH, bd=True, transforms=val_transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=num_workers, worker_init_fn=seed_worker, generator=g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPAOdqP6g9FR"
      },
      "source": [
        "### Training process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2Mq7J4HMoy5"
      },
      "source": [
        "#### Model engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGWsHfnDhC8i",
        "outputId": "5644d0b1-b006-4d33-e392-ca63e1115c98"
      },
      "outputs": [],
      "source": [
        "pidnet = get_seg_model(\"pidnet_s\", num_classes, PIDNET_S_WEIGHTS,imgnet_pretrained=True)\n",
        "model = FullModel(pidnet, sem_loss=CrossEntropy(ignore_label=IGNORE_INDEX), bd_loss=BondaryLoss())\n",
        "model.to(device)\n",
        "\n",
        "ema_model = create_ema_model(model)\n",
        "ema_model = ema_model.to(device)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = STEP_SIZE, gamma = GAMMA)\n",
        "\n",
        "sem_loss=CrossEntropy(ignore_label=IGNORE_INDEX)\n",
        "bd_loss=BondaryLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqGgoNC8jHm4"
      },
      "source": [
        "#### Evaluate function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNgm0_9-jHDd"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, dataloader, device, ) -> tuple:\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    data_len = 0\n",
        "    iou_scores = 0.0\n",
        "    ious_per_class = torch.zeros(num_classes)\n",
        "\n",
        "    for (inputs, masks, boundaries) in dataloader:\n",
        "\n",
        "        data_len += inputs.size(0)\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        masks = masks.to(device)\n",
        "        boundaries = boundaries.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        loss, outputs, _, _ = model(inputs, masks, boundaries)\n",
        "        running_loss += loss.item()*inputs.size(0)\n",
        "\n",
        "        # Calculate mIoU\n",
        "        iou, iou_per_class = calculate_iou(outputs[1], masks, num_classes)\n",
        "        iou_scores += iou*inputs.size(0)\n",
        "        ious_per_class+=iou_per_class.cpu()*inputs.size(0)\n",
        "\n",
        "    mIoU = iou_scores/data_len\n",
        "    loss = running_loss/data_len\n",
        "    ious_per_class/=data_len\n",
        "\n",
        "\n",
        "    return loss, mIoU, ious_per_class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20LhVR64gtNb"
      },
      "source": [
        "#### Training/evaluation loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3jgWAuIcL_6",
        "outputId": "b4062fa5-640a-473b-d762-7bb5661c9559"
      },
      "outputs": [],
      "source": [
        "ema_model.train()\n",
        "\n",
        "accumulated_loss_l = []\n",
        "accumulated_loss_u = []\n",
        "\n",
        "miou_scores = []\n",
        "training_losses = []\n",
        "validation_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    loss_u_value = 0\n",
        "    loss_l_value = 0\n",
        "\n",
        "    n = 0\n",
        "    for (src_images, src_labels, src_bd), (tgt_images, _, _) in zip(source_trainloader, target_trainloader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        src_images = src_images.to(device)\n",
        "        src_labels = src_labels.to(device)\n",
        "        tgt_images = tgt_images.to(device)\n",
        "        src_bd = src_bd.to(device)\n",
        "\n",
        "\n",
        "        L_l, [_, pred, _], _, _ = model(src_images, src_labels, src_bd)\n",
        "\n",
        "        # _, [_, logits_u_w, _], _, _ = ema_model(tgt_images, None, None)\n",
        "        _, [_, logits_u_w, _], _, _ = model(tgt_images, None, None)\n",
        "\n",
        "\n",
        "        pseudo_label = torch.softmax(logits_u_w.detach(), dim=1)\n",
        "        max_probs, targets_u_w = torch.max(pseudo_label, dim=1)\n",
        "\n",
        "        inputs_u_s = []\n",
        "        targets_u = []\n",
        "        pixel_weights = []\n",
        "\n",
        "\n",
        "        for i in range(len(src_images)):\n",
        "            classes = torch.unique(src_labels[i])\n",
        "            nclasses = classes.shape[0]\n",
        "            classes = (classes[torch.Tensor(np.random.choice(nclasses, int((nclasses+nclasses%2)/2),replace=False)).long()]).to(device)\n",
        "            MixMask_i = generate_class_mask(src_labels[i], classes).unsqueeze(0).to(device)\n",
        "\n",
        "            strong_parameters = {\"Mix\": MixMask_i}\n",
        "\n",
        "            inputs_u_si, _ = mix(strong_parameters, data = torch.cat((src_images[i].unsqueeze(0),tgt_images[i].unsqueeze(0))))\n",
        "            inputs_u_s.append(inputs_u_si)\n",
        "\n",
        "            _, targets_ui = mix(strong_parameters, target = torch.cat((src_labels[i].unsqueeze(0),targets_u_w[i].unsqueeze(0))))\n",
        "            targets_u.append(targets_ui)\n",
        "\n",
        "        inputs_u_s = torch.cat(inputs_u_s)\n",
        "        _, outputs, _, _ = model(inputs_u_s, None, None)\n",
        "        logits_u_s = outputs[1]\n",
        "\n",
        "        targets_u = torch.cat(targets_u).long().to(device)\n",
        "\n",
        "\n",
        "        if pixel_weight == \"threshold_uniform\":\n",
        "            unlabeled_weight = torch.sum(max_probs.ge(0.968).long() == 1).item() / np.size(np.array(targets_u.cpu()))\n",
        "            pixelWiseWeight = unlabeled_weight * torch.ones(max_probs.shape).to(device)\n",
        "        elif pixel_weight == \"threshold\":\n",
        "            pixelWiseWeight = max_probs.ge(0.968).float().to(device)\n",
        "        elif pixel_weight == False:\n",
        "            pixelWiseWeight = torch.ones(max_probs.shape).to(device)\n",
        "\n",
        "\n",
        "        onesWeights = torch.ones((pixelWiseWeight.shape)).to(device)\n",
        "        for i in range(len(src_images)):\n",
        "            _, pixelWiseWeight_i = mix(strong_parameters, target = torch.cat((onesWeights[0].unsqueeze(0),pixelWiseWeight[0].unsqueeze(0))))\n",
        "            pixel_weights.append(pixelWiseWeight_i)\n",
        "\n",
        "\n",
        "        pixel_weights = torch.cat(pixel_weights).to(device)\n",
        "\n",
        "\n",
        "        L_u = calc_U_loss(outputs, targets_u, sem_loss, bd_loss)\n",
        "        L_u *= torch.mean(pixel_weights)\n",
        "\n",
        "        loss = L_l + L_u\n",
        "\n",
        "        loss_l_value += L_l.item()\n",
        "        loss_u_value += L_u.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        if n %25 == 0:\n",
        "          print('\\tProcessed {0:d} batches, loss_l = {1:.3f}, loss_u = {2:.3f} loss = {3:.3f}'.format(n, loss_l_value/(n+1), loss_u_value/(n+1),(loss_l_value+loss_u_value)/(n+1)))\n",
        "\n",
        "        n+=1\n",
        "\n",
        "    loss_l_value /= len(source_trainloader)\n",
        "    loss_u_value /= len(target_trainloader)\n",
        "\n",
        "    accumulated_loss_l.append(loss_l_value)\n",
        "    accumulated_loss_u.append(loss_u_value)\n",
        "    training_losses.append(loss_l_value+loss_u_value)\n",
        "\n",
        "    # Update learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    # update Mean teacher network\n",
        "    alpha_teacher = 0.99\n",
        "    ema_model = update_ema_variables(ema_model = ema_model, model = model, alpha_teacher=alpha_teacher, iteration=epoch)\n",
        "\n",
        "    print('iter = {0:6d}/{1:6d}, loss_l = {2:.3f}, loss_u = {3:.3f} loss = {4:.3f}'.format(epoch+1, num_epochs, loss_l_value, loss_u_value, loss_l_value+loss_u_value))\n",
        "\n",
        "\n",
        "\n",
        "    val_loss, val_mean_iou, ious_per_class = evaluate(model, val_loader, device)\n",
        "    print(f\"Validation mIoU: {val_mean_iou*100:.3f}%, Validation loss: {val_loss:.5f}\")\n",
        "    for i, cat in enumerate(categories.keys()):\n",
        "        print(f\"{cat} mIoU: {ious_per_class[i]*100:.3f}\")\n",
        "    print()\n",
        "\n",
        "    validation_losses.append(val_loss)\n",
        "    miou_scores.append(val_mean_iou)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1y_fKk3sgbD5",
        "outputId": "74fd1a83-1286-4c5f-ff2a-6293d7a0b093"
      },
      "outputs": [],
      "source": [
        "for i in range(BATCH_SIZE):\n",
        "  plt.imshow(src_images[i].permute(1,2,0).cpu()*torch.tensor(std)+torch.tensor(avg))\n",
        "  plt.show()\n",
        "  plt.imshow(tgt_images[i].permute(1,2,0).cpu()*torch.tensor(std)+torch.tensor(avg))\n",
        "  plt.show()\n",
        "  plt.imshow(inputs_u_s[i].permute(1,2,0).cpu()*torch.tensor(std)+torch.tensor(avg))\n",
        "  plt.show()\n",
        "\n",
        "  plot_tensor_mask(src_labels[i].cpu(), categories)\n",
        "  plot_tensor_mask(targets_u_w[i].cpu(), categories)\n",
        "  plot_tensor_mask(targets_u[i].cpu(), categories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjoF0ZtWbuz3"
      },
      "source": [
        "# Step 5: Improving the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wv_yNNWpP1a8"
      },
      "source": [
        "### Calculate class distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tW6knWDu1cRG"
      },
      "outputs": [],
      "source": [
        "urban_dataset = LoveDA(TRAIN_DIR, IMG_PATH, MASK_PATH, directories=URBAN_PATH)\n",
        "urban_loader = DataLoader(urban_dataset, batch_size=64, worker_init_fn=seed_worker, generator=g)\n",
        "\n",
        "rural_dataset = LoveDA(TRAIN_DIR, IMG_PATH, MASK_PATH, directories=RURAL_PATH)\n",
        "rural_loader = DataLoader(rural_dataset, batch_size=64, worker_init_fn=seed_worker, generator=g)\n",
        "\n",
        "urban_classes = dict()\n",
        "rural_classes = dict()\n",
        "\n",
        "for (_, masks) in urban_loader:\n",
        "\n",
        "      masks = masks.to(device)\n",
        "\n",
        "      for i, cat in enumerate(categories.keys()):\n",
        "        if cat in urban_classes:\n",
        "          urban_classes[cat] += torch.count_nonzero(masks == i)\n",
        "        else:\n",
        "          urban_classes[cat] = torch.count_nonzero(masks == i)\n",
        "\n",
        "for (_, masks) in rural_loader:\n",
        "\n",
        "      masks = masks.to(device)\n",
        "\n",
        "      for i, cat in enumerate(categories.keys()):\n",
        "        if cat in rural_classes:\n",
        "          rural_classes[cat] += torch.count_nonzero(masks == i)\n",
        "        else:\n",
        "          rural_classes[cat] = torch.count_nonzero(masks == i)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 992
        },
        "id": "39fA8Wbbi4KF",
        "outputId": "b7fdfe53-398a-4c7e-a15c-f485bed65998"
      },
      "outputs": [],
      "source": [
        "colors= [np.array(color)/255 for _, color in sorted(categories.values())]\n",
        "\n",
        "wedges, texts, autotexts= plt.pie([v.cpu().numpy() for v in urban_classes.values()], labels=urban_classes.keys(), colors=colors, autopct='%1.1f%%', pctdistance=0.85, labeldistance=1.1, startangle=90)\n",
        "for text in texts:\n",
        "    text.set_fontsize(12)\n",
        "for autotext in autotexts:\n",
        "    autotext.set_fontsize(9)\n",
        "plt.title('Urban Dataset', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"urban_percentage = \", [float(autotext.get_text().strip('%')) for autotext in autotexts])\n",
        "\n",
        "wedges, texts, autotexts= plt.pie([v.cpu().numpy() for v in rural_classes.values()], labels=rural_classes.keys(), colors=colors, autopct='%1.1f%%', pctdistance=0.85, labeldistance=1.1, startangle=90)\n",
        "for text in texts:\n",
        "    text.set_fontsize(12)\n",
        "for autotext in autotexts:\n",
        "    autotext.set_fontsize(9)\n",
        "plt.title(\"Rural dataset\", fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"rural_percentage = \", [float(autotext.get_text().strip('%')) for autotext in autotexts])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "918GoC2vP78y"
      },
      "source": [
        "### Calculate class weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_uMQj3LqDOB",
        "outputId": "cc6d66b9-01e6-4df3-c253-5ada139190ed"
      },
      "outputs": [],
      "source": [
        "def calc_weights(percentages):\n",
        "  percentages = np.array(percentages)\n",
        "  proportions = percentages / 100  # Divide by 100 to convert percentages to fractions\n",
        "\n",
        "  # Calculate class weights inversely proportional to proportions\n",
        "  class_weights = 1 / proportions\n",
        "\n",
        "  # Optional: Normalize weights so the mean is 1\n",
        "  normalized_weights = class_weights / np.mean(class_weights)\n",
        "\n",
        "  alpha = 0.5  # Adjust this hyperparameter\n",
        "  softened_weights = 1 / (proportions ** alpha)\n",
        "  softened_weights /= np.mean(softened_weights)\n",
        "\n",
        "  normalized_weights_v2 = class_weights / max(class_weights)\n",
        "\n",
        "\n",
        "  return list(class_weights), list(normalized_weights), list(softened_weights), list(normalized_weights_v2)\n",
        "\n",
        "\n",
        "urban_percentage =  [48.5, 21.2, 9.3, 3.7, 7.6, 7.9, 1.9]\n",
        "rural_percentage =  [42.9, 3.7, 2.6, 11.6, 3.6, 5.0, 30.5]\n",
        "\n",
        "urban_class_weights , urban_normalized_weights, urban_softened_weights, urban_normalized_weights_v2 = calc_weights(urban_percentage)\n",
        "rural_class_weights , rural_normalized_weights, rural_softened_weights, rural_normalized_weights_v2 = calc_weights(rural_percentage)\n",
        "\n",
        "print(f\"urban_class_weights = {urban_class_weights}\")\n",
        "print(f\"urban_normalized_weights = {urban_normalized_weights}\")\n",
        "print(f\"urban_softened_weights = {urban_softened_weights}\")\n",
        "print(f\"urban_normalized_weights_v2 = {urban_normalized_weights_v2}\")\n",
        "\n",
        "print()\n",
        "\n",
        "print(f\"rural_class_weights = {rural_class_weights}\")\n",
        "print(f\"rural_normalized_weights = {rural_normalized_weights}\")\n",
        "print(f\"rural_softened_weights = {rural_softened_weights}\")\n",
        "print(f\"rural_normalized_weights_v2 = {rural_normalized_weights_v2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl7BNfbwwYcB"
      },
      "source": [
        "We then used **urban_softened_weights** passing them to the Cross Entropy and trained the model. Training loop is not reported again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbBVeL8Vw1t7"
      },
      "outputs": [],
      "source": [
        "pidnet = get_seg_model(\"pidnet_s\", num_classes, PIDNET_S_WEIGHTS,imgnet_pretrained=True)\n",
        "model = FullModel(pidnet, sem_loss=CrossEntropy(ignore_label=IGNORE_INDEX, weight=torch.tensor(urban_softened_weights)), bd_loss=BondaryLoss())\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZirP1ZgqlkcW"
      },
      "source": [
        "# Step 5: BiSeNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8up2B3knSFS"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcQ_xEK3nSFS"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCoD2u0rnSFS"
      },
      "outputs": [],
      "source": [
        "num_epochs = 20\n",
        "BATCH_SIZE = 6\n",
        "learning_rate = 1e-3\n",
        "step_size = 10\n",
        "gamma = 0.1\n",
        "resize = 512\n",
        "w_decay = 1e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15-4beKOnSFS"
      },
      "source": [
        "### Dataset preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fNRQCeKnSFT"
      },
      "source": [
        "#### Normalization metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tL8435sKnSFT"
      },
      "outputs": [],
      "source": [
        "num_workers = 2 if device.type == 'cuda' else 0\n",
        "\n",
        "# Poiché il modello è pretrainato su ImageNet, si usano media e varianza di ImageNet\n",
        "avg = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_XBD1iYnSFT"
      },
      "source": [
        "#### Training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wV9BpUrnSFT"
      },
      "outputs": [],
      "source": [
        "train_transform = A.Compose([\n",
        "    A.Normalize(mean=avg, std=std, p=1, always_apply=True, max_pixel_value=255),\n",
        "    A.Resize(resize, resize, p=1, always_apply=True)\n",
        "])\n",
        "train_dataset = LoveDA(TRAIN_DIR, IMG_PATH, MASK_PATH, directories=URBAN_PATH, transforms=train_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=num_workers, worker_init_fn=seed_worker, generator=g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb5mn06QnSFT"
      },
      "source": [
        "#### Validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvthpg62nSFT"
      },
      "outputs": [],
      "source": [
        "val_transform = A.Compose([\n",
        "    A.Normalize(mean=avg, std=std, p=1, always_apply=True, max_pixel_value=255)\n",
        "])\n",
        "#val_dataset = LoveDA(VAL_DIR, IMG_PATH, MASK_PATH, directories=URBAN_PATH, transforms=val_transform)\n",
        "val_dataset = LoveDA(VAL_DIR, IMG_PATH, MASK_PATH, directories=RURAL_PATH, transforms=val_transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=num_workers, worker_init_fn=seed_worker, generator=g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDYwJmndnSFT"
      },
      "source": [
        "### Training process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbk6UOcKnSFT"
      },
      "source": [
        "#### Model engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hM0_629YnSFU",
        "outputId": "374cb8ff-582d-48e5-c492-2a1f9338e9fc"
      },
      "outputs": [],
      "source": [
        "model = BiSeNet(num_classes,'resnet101').to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=w_decay)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKakLuigvTnb"
      },
      "source": [
        "#### Evaluate function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rbtvULIvTnc"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, dataloader, device, ) -> tuple:\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    data_len = 0\n",
        "    iou_scores = 0.0\n",
        "    ious_per_class = torch.zeros(num_classes)\n",
        "\n",
        "    for i, (inputs, masks) in enumerate(dataloader):\n",
        "\n",
        "        data_len += inputs.size(0)\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, masks)\n",
        "\n",
        "        running_loss += loss.item()*inputs.size(0)\n",
        "\n",
        "        # Calculate mIoU\n",
        "        iou, iou_per_class = calculate_iou(outputs, masks, num_classes)\n",
        "        iou_scores += iou*inputs.size(0)\n",
        "        ious_per_class+=iou_per_class.cpu()*inputs.size(0)\n",
        "\n",
        "    mIoU = iou_scores/data_len\n",
        "    loss = running_loss/data_len\n",
        "    ious_per_class/=data_len\n",
        "\n",
        "\n",
        "    return loss, mIoU, ious_per_class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsE0O98MnSFU"
      },
      "source": [
        "#### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQksizQBnSFU",
        "outputId": "5db941af-2f81-4af2-9454-88e318dd1ebe"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "eval_losses = []\n",
        "mious = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(\"### Training mode\")\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (images, masks) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs, outputs16, outputs32 = model(images)\n",
        "        loss1 = criterion(outputs, masks)\n",
        "        loss2 = criterion(outputs16, masks)\n",
        "        loss3 = criterion(outputs32, masks)\n",
        "        loss = loss1 + loss2 + loss3\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if (i + 1) % 25 == 0:\n",
        "            print(f\"Processed {i + 1} batches, loss: {running_loss / (i+1)}\")\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(epoch_loss)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    print(\"### Evaluation mode\")\n",
        "    val_loss, miou, ious_per_class = evaluate(model, val_loader, device)\n",
        "\n",
        "    print(f\"Validation mIoU: {miou*100:.3f}%, Validation loss: {val_loss:.5f}\")\n",
        "    for i, cat in enumerate(categories.keys()):\n",
        "        print(f\"{cat} mIoU: {ious_per_class[i]*100:.3f}\")\n",
        "    print()\n",
        "\n",
        "    eval_losses.append(val_loss)\n",
        "\n",
        "    mious.append(miou)\n",
        "\n",
        "    print(f\"Epoch: [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}, mIoU: {(miou * 100):.2f}%\")\n",
        "\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "474HU-BtnSFV"
      },
      "source": [
        "### Metric calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "BOAczM-9nSFV",
        "outputId": "31eb5214-b277-44d4-c36c-0365f14037ad"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "mean_latency, _, _, _ = calculate_latency_fps(model, device, 1024, 1024, num_epochs, ModelType.BISENET)\n",
        "print(f\"Mean Latency: {mean_latency:.2f} ms\")\n",
        "\n",
        "calculate_flops_params(model, device, 1024, 1024, ModelType.BISENET)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kf7UOPXtvv4u"
      },
      "source": [
        "# Step 5: STDC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxN5kO6OT1jt"
      },
      "source": [
        "### Download pre-trained weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDwG19HPT1jt"
      },
      "outputs": [],
      "source": [
        "weights_dir = Path(PRETRAINED_WEIGHTS_DIR)\n",
        "if not weights_dir.exists():\n",
        "    weights_dir.mkdir(exist_ok=True)\n",
        "\n",
        "stdc1_weights = Path(STDC1_WEIGHTS)\n",
        "if not stdc1_weights.exists():\n",
        "    # Replace with the correct Google Drive file ID\n",
        "    file_id = \"1DFoXcV42zy-apUcMh5P8WhsXMRJofgl8\"\n",
        "    gdown.download(id=file_id, output=str(stdc1_weights), quiet=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7MBNcStzY2o"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp6UOEKtzY2o"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lu9QZNEgzY2o"
      },
      "outputs": [],
      "source": [
        "num_epochs = 20\n",
        "BATCH_SIZE = 6\n",
        "learning_rate = 1e-3\n",
        "step_size = 10\n",
        "gamma = 0.1\n",
        "resize = 512\n",
        "w_decay = 1e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzkSoImkzY2o"
      },
      "source": [
        "### Dataset preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVjIq8vnzY2p"
      },
      "source": [
        "#### Normalization metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uejw9pwGzY2p"
      },
      "outputs": [],
      "source": [
        "num_workers = 2 if device.type == 'cuda' else 0\n",
        "\n",
        "# Poiché il modello è pretrainato su ImageNet, si usano media e varianza di ImageNet\n",
        "avg = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Df8mfmTwzY2p"
      },
      "source": [
        "#### Training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLC0KJcHzY2p"
      },
      "outputs": [],
      "source": [
        "train_transform = A.Compose([\n",
        "    A.Normalize(mean=avg, std=std, p=1, always_apply=True, max_pixel_value=255),\n",
        "    A.Resize(resize, resize, p=1, always_apply=True)\n",
        "])\n",
        "\n",
        "train_dataset = LoveDA(TRAIN_DIR, IMG_PATH, MASK_PATH, directories=URBAN_PATH, transforms=train_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=num_workers, worker_init_fn=seed_worker, generator=g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0VEbqXdzY2p"
      },
      "source": [
        "#### Validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RW7_R1PzY2p"
      },
      "outputs": [],
      "source": [
        "val_transform = A.Compose([\n",
        "    A.Normalize(mean=avg, std=std, p=1, always_apply=True, max_pixel_value=255),\n",
        "])\n",
        "\n",
        "#val_dataset = LoveDA(VAL_DIR, IMG_PATH, MASK_PATH, directories=URBAN_PATH, transforms=val_transform)\n",
        "val_dataset = LoveDA(VAL_DIR, IMG_PATH, MASK_PATH, directories=RURAL_PATH, transforms=val_transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=num_workers, worker_init_fn=seed_worker, generator=g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7PuqcWGzY2p"
      },
      "source": [
        "### Training process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Fe9yGjKzY2q"
      },
      "source": [
        "#### Model engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xX6X-AwnzY2q",
        "outputId": "352dc834-0aff-45f2-9bc7-8caac19abbab"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "model = STDC(n_classes=num_classes,backbone='STDCNet813', pretrain_model=stdc1_weights).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=w_decay)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bu-lFRI2X6-T"
      },
      "source": [
        "#### Evaluate function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQGIMAbXX6-T"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, dataloader, device, ) -> tuple:\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    data_len = 0\n",
        "    iou_scores = 0.0\n",
        "    ious_per_class = torch.zeros(num_classes)\n",
        "\n",
        "    for i, (inputs, masks) in enumerate(dataloader):\n",
        "\n",
        "        data_len += inputs.size(0)\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs, _, _ = model(inputs)\n",
        "        loss = criterion(outputs, masks)\n",
        "\n",
        "        running_loss += loss.item()*inputs.size(0)\n",
        "\n",
        "        # Calculate mIoU\n",
        "        iou, iou_per_class = calculate_iou(outputs, masks, num_classes)\n",
        "        iou_scores += iou*inputs.size(0)\n",
        "        ious_per_class+=iou_per_class.cpu()*inputs.size(0)\n",
        "\n",
        "    mIoU = iou_scores/data_len\n",
        "    loss = running_loss/data_len\n",
        "    ious_per_class/=data_len\n",
        "\n",
        "\n",
        "    return loss, mIoU, ious_per_class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQbqXe6VzY2q"
      },
      "source": [
        "#### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "8dCDNmAGzY2q",
        "outputId": "0d922b75-71ae-44b6-f6f0-42bec688675d"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "eval_losses = []\n",
        "mious = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(\"### Training mode\")\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (images, masks) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs, outputs16, outputs32 = model(images)\n",
        "        loss1 = criterion(outputs, masks)\n",
        "        loss2 = criterion(outputs16, masks)\n",
        "        loss3 = criterion(outputs32, masks)\n",
        "        loss = loss1 + loss2 + loss3\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if (i + 1) % 25 == 0:\n",
        "            print(f\"Processed {i + 1} batches, loss: {running_loss / (i+1)}\")\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(epoch_loss)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    print(\"### Evaluation mode\")\n",
        "    val_loss, miou, ious_per_class = evaluate(model, val_loader, device)\n",
        "\n",
        "    print(f\"Validation mIoU: {miou*100:.3f}%, Validation loss: {val_loss:.5f}\")\n",
        "    for i, cat in enumerate(categories.keys()):\n",
        "        print(f\"{cat} mIoU: {ious_per_class[i]*100:.3f}\")\n",
        "    print()\n",
        "\n",
        "    eval_losses.append(val_loss)\n",
        "    mious.append(miou)\n",
        "\n",
        "    print(f\"Epoch: [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}, mIoU: {(miou * 100):.2f}%\")\n",
        "\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-e0QWMNzY2r"
      },
      "source": [
        "### Metric calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3HdWgUgzY2r"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "mean_latency, _, _, _ = calculate_latency_fps(model, device, 1024, 1024, num_epochs, ModelType.STDC)\n",
        "print(f\"Mean Latency: {mean_latency:.2f} ms\")\n",
        "\n",
        "calculate_flops_params(model, device, 1024, 1024, ModelType.STDC)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "QPliIXlMh2bf",
        "Xs1L8KHZ9GAq",
        "LsP0RjvL9NlC",
        "eORGOwqKY94j",
        "Zim4nyhUgTEE",
        "BUlnYE8RSUYX",
        "jbMkS3H2iS1v",
        "vk_O5SS6BqXV",
        "glRuMNQhsq1p",
        "2y7JhEDjpOk1",
        "XjoF0ZtWbuz3",
        "_8SuvFTlwe9a",
        "YuH9_bUuxI4n",
        "CULoLihOwxHl",
        "OQKuCveRw8Qm",
        "Cp6UOEKtzY2o",
        "wzkSoImkzY2o",
        "GVjIq8vnzY2p",
        "Df8mfmTwzY2p",
        "K0VEbqXdzY2p",
        "H7PuqcWGzY2p",
        "8Fe9yGjKzY2q",
        "IQbqXe6VzY2q",
        "Les-vObfzY2q",
        "2eNNOT-dzY2r",
        "F-e0QWMNzY2r",
        "1ohsbEQnzY2s",
        "KvfJfTJZxrYG"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
