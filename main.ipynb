{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPliIXlMh2bf"
      },
      "source": [
        "# Seed initialization (to make results reproducible)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCA5kpzUh9aG",
        "outputId": "1a8387a0-71b4-408d-cb32-6bd5ab934dd6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "torch.manual_seed(0)\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.use_deterministic_algorithms(True, warn_only=True)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchark = False\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "g = torch.Generator()\n",
        "g.manual_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUccqUJxsmMl"
      },
      "source": [
        "# Environment setup (execute for any Step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-sBVNVCbUsy"
      },
      "source": [
        "## Package install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FuKcgT_Xo_G",
        "outputId": "975c8696-c043-4694-d615-8729c02a2b08"
      },
      "outputs": [],
      "source": [
        "!pip install wget\n",
        "!pip install requests gdown\n",
        "!pip install fvcore\n",
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnTYhfbCbbUN"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeSEfBMAslUP",
        "outputId": "8cc0bdcb-f00b-44b9-c4d7-2c8c3286bfbd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.patches as mpatches\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.colors as mcolors\n",
        "from pathlib import Path\n",
        "import wget\n",
        "import requests\n",
        "import gdown\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import VisionDataset\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from enum import Enum\n",
        "from google.colab import drive\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "from fvcore.nn import FlopCountAnalysis, flop_count_table\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.backends import cudnn\n",
        "from statistics import mean\n",
        "import cv2\n",
        "from torchmetrics.segmentation import MeanIoU\n",
        "from torch.utils.data import ConcatDataset\n",
        "import random\n",
        "from torch.nn.utils import clip_grad\n",
        "import albumentations as A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vnEBr7iwiI8"
      },
      "source": [
        "## Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABxsgPkxwlmT"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = 'loveDA_dataset'\n",
        "TRAIN_ZIP = f'{DATA_DIR}/train.zip'\n",
        "VAL_ZIP = f'{DATA_DIR}/validation.zip'\n",
        "TEST_ZIP = f'{DATA_DIR}/test.zip'\n",
        "TRAIN_DIR = f'{DATA_DIR}/train'\n",
        "VAL_DIR = f'{DATA_DIR}/validation'\n",
        "TEST_DIR = f'{DATA_DIR}/test'\n",
        "RURAL_PATH = \"Rural\"\n",
        "URBAN_PATH = \"Urban\"\n",
        "IMG_PATH = \"images_png\"\n",
        "MASK_PATH = \"masks_png\"\n",
        "PRETRAINED_WEIGHTS_DIR = 'pretrained_weights'\n",
        "DEEPLAB_V2_WEIGHTS = f'{PRETRAINED_WEIGHTS_DIR}/DeepLab_resnet_pretrained_imagenet.pth'\n",
        "STDC1_WEIGHTS = f\"{PRETRAINED_WEIGHTS_DIR}/STDC1_pretrained_weights.pth\"\n",
        "\n",
        "IGNORE_INDEX=-1\n",
        "\n",
        "RGB = 'RGB'\n",
        "grayscale = 'L'\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Domain(Enum):\n",
        "    RURAL = 0\n",
        "    URBAN = 1\n",
        "\n",
        "class ModelType(Enum):\n",
        "    DEEPLAB = 0\n",
        "    PIDNET = 1\n",
        "    BISENET = 2\n",
        "    STDC = 3\n",
        "\n",
        "categories = {\n",
        "    'BARREN': (0.003921568859368563, (159, 129, 183)),       # Lilla\n",
        "    'AGRICULTURE': (0.027450980618596077, (255, 195, 128)),  # Arancione\n",
        "    'BUILDING': (0.007843137718737125, (255, 0, 0)),         # Rosso\n",
        "    'WATER': (0.01568627543747425, (0, 0, 255)),             # Blu\n",
        "    'ROAD': (0.0117647061124444, (255, 255, 0)),             # Giallo\n",
        "    'BG': (0.019607843831181526, (255, 255, 255)),           # Bianco\n",
        "    'FOREST': (0.0235294122248888, (0, 255, 0))              # Verde\n",
        "}\n",
        "\n",
        "categories = dict(sorted(categories.items(), key=lambda item: item[1][0]))\n",
        "\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "num_classes = len(categories.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl_nALVQtCuk"
      },
      "source": [
        "## Dataset: LoveDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpFwzlM2UoL_"
      },
      "source": [
        "### Download dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs1L8KHZ9GAq"
      },
      "source": [
        "#### Without Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyWrp2BomFDS",
        "outputId": "d8401b12-d04f-4805-a646-c2a725d0d594"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "download_directory = Path(DATA_DIR)\n",
        "if not download_directory.exists():\n",
        "    download_directory.mkdir(exist_ok=True)\n",
        "\n",
        "# Zip download\n",
        "\n",
        "train_zip = Path(TRAIN_ZIP)\n",
        "if not train_zip.exists():\n",
        "    !wget -O {TRAIN_ZIP} 'https://zenodo.org/record/5706578/files/Train.zip?download=1'\n",
        "\n",
        "val_zip = Path(VAL_ZIP)\n",
        "if not val_zip.exists():\n",
        "    !wget -O {VAL_ZIP} 'https://zenodo.org/records/5706578/files/Val.zip?download=1'\n",
        "\n",
        "test_zip = Path(TEST_ZIP)\n",
        "if not test_zip.exists():\n",
        "    !wget -O {TEST_ZIP} 'https://zenodo.org/records/5706578/files/Test.zip?download=1'\n",
        "\n",
        "# Zip extraction\n",
        "\n",
        "## I suppose to not cancel the original zip since who knows\n",
        "\n",
        "train_dir = Path(TRAIN_DIR)\n",
        "if not train_dir.exists():\n",
        "    !unzip -q {TRAIN_ZIP} -d {DATA_DIR}\n",
        "    !mv {DATA_DIR}/Train {TRAIN_DIR}\n",
        "\n",
        "val_dir = Path(VAL_DIR)\n",
        "if not val_dir.exists():\n",
        "    !unzip -q {VAL_ZIP} -d {DATA_DIR}\n",
        "    !mv {DATA_DIR}/Val {VAL_DIR}\n",
        "\n",
        "test_dir = Path(TEST_DIR)\n",
        "if not test_dir.exists():\n",
        "    !unzip -q {TEST_ZIP} -d {DATA_DIR}\n",
        "    !mv {DATA_DIR}/Test {TEST_DIR}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsP0RjvL9NlC"
      },
      "source": [
        "#### With Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7t0sVWE9AMl"
      },
      "outputs": [],
      "source": [
        "def download_to_gdrive():\n",
        "    drive_path_dir = '/content/drive'\n",
        "    mydrive_path_dir = f'{drive_path_dir}/MyDrive'\n",
        "    data_path_dir = f'{mydrive_path_dir}/{DATA_DIR}'\n",
        "    train_path_zip = f'{mydrive_path_dir}/{TRAIN_ZIP}'\n",
        "    val_path_zip = f'{mydrive_path_dir}/{VAL_ZIP}'\n",
        "    test_path_zip = f'{mydrive_path_dir}/{TEST_ZIP}'\n",
        "    train_path_dir = f'{mydrive_path_dir}/{TRAIN_DIR}'\n",
        "    val_path_dir = f'{mydrive_path_dir}/{VAL_DIR}'\n",
        "    test_path_dir = f'{mydrive_path_dir}/{TEST_DIR}'\n",
        "\n",
        "    from google.colab import drive\n",
        "    drive.mount(drive_path_dir)\n",
        "\n",
        "    download_directory = Path(data_path_dir)\n",
        "    if not download_directory.exists():\n",
        "        download_directory.mkdir(exist_ok=True)\n",
        "\n",
        "    train_zip = Path(train_path_zip)\n",
        "    if not train_zip.exists():\n",
        "        !wget -O {train_path_zip} 'https://zenodo.org/record/5706578/files/Train.zip?download=1'\n",
        "\n",
        "    val_zip = Path(val_path_zip)\n",
        "    if not val_zip.exists():\n",
        "        !wget -O {val_path_zip} 'https://zenodo.org/records/5706578/files/Val.zip?download=1'\n",
        "\n",
        "    test_zip = Path(test_path_zip)\n",
        "    if not test_zip.exists():\n",
        "        !wget -O {test_path_zip} 'https://zenodo.org/records/5706578/files/Test.zip?download=1'\n",
        "\n",
        "def extract_from_gdrive():\n",
        "\n",
        "    drive_path_dir = '/content/drive'\n",
        "    mydrive_path_dir = f'{drive_path_dir}/MyDrive'\n",
        "    data_path_dir = f'{mydrive_path_dir}/{DATA_DIR}'\n",
        "    train_path_zip = f'{mydrive_path_dir}/{TRAIN_ZIP}'\n",
        "    val_path_zip = f'{mydrive_path_dir}/{VAL_ZIP}'\n",
        "    test_path_zip = f'{mydrive_path_dir}/{TEST_ZIP}'\n",
        "    train_path_dir = f'{mydrive_path_dir}/{TRAIN_DIR}'\n",
        "    val_path_dir = f'{mydrive_path_dir}/{VAL_DIR}'\n",
        "    test_path_dir = f'{mydrive_path_dir}/{TEST_DIR}'\n",
        "\n",
        "    from google.colab import drive\n",
        "    drive.mount(drive_path_dir)\n",
        "\n",
        "    train_dir = Path(TRAIN_DIR)\n",
        "    if not train_dir.exists():\n",
        "        !unzip -q {train_path_zip} -d {DATA_DIR}\n",
        "        !mv {DATA_DIR}/Train {TRAIN_DIR}\n",
        "\n",
        "    val_dir = Path(VAL_DIR)\n",
        "    if not val_dir.exists():\n",
        "        !unzip -q {val_path_zip} -d {DATA_DIR}\n",
        "        !mv {DATA_DIR}/Val {VAL_DIR}\n",
        "\n",
        "    #test_dir = Path(TEST_DIR)\n",
        "    #if not test_dir.exists():\n",
        "    #    !unzip -q {test_path_zip} -d {DATA_DIR}\n",
        "    #    !mv {DATA_DIR}/Test {TEST_DIR}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZF7aCc_87V2"
      },
      "outputs": [],
      "source": [
        "def copy_to_gdrive():\n",
        "    drive_path_dir = '/content/drive'\n",
        "    mydrive_path_dir = f'{drive_path_dir}/MyDrive'\n",
        "    data_path_dir = f'{mydrive_path_dir}/{DATA_DIR}'\n",
        "    train_path_zip = f'{mydrive_path_dir}/{TRAIN_ZIP}'\n",
        "    val_path_zip = f'{mydrive_path_dir}/{VAL_ZIP}'\n",
        "    test_path_zip = f'{mydrive_path_dir}/{TEST_ZIP}'\n",
        "\n",
        "    from google.colab import drive\n",
        "    drive.mount(drive_path_dir)\n",
        "\n",
        "    import shutil  # Import shutil for file operations\n",
        "\n",
        "    # Create the directory if it doesn't exist\n",
        "    os.makedirs(data_path_dir, exist_ok=True)\n",
        "\n",
        "    # Copy the zip files using shutil.copy\n",
        "    if not os.path.exists(train_path_zip):\n",
        "        shutil.copy(TRAIN_ZIP, train_path_zip)\n",
        "        print(f\"Copied {TRAIN_ZIP} to {train_path_zip}\")\n",
        "\n",
        "    if not os.path.exists(val_path_zip):\n",
        "        shutil.copy(VAL_ZIP, val_path_zip)\n",
        "        print(f\"Copied {VAL_ZIP} to {val_path_zip}\")\n",
        "\n",
        "    if not os.path.exists(test_path_zip):\n",
        "        shutil.copy(TEST_ZIP, test_path_zip)\n",
        "        print(f\"Copied {TEST_ZIP} to {test_path_zip}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnzRinjI_HKi",
        "outputId": "b2e08164-1bd0-4f4f-a1ed-15ee9b35ab8e"
      },
      "outputs": [],
      "source": [
        "#download_to_gdrive()\n",
        "#copy_to_gdrive()\n",
        "extract_from_gdrive()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eORGOwqKY94j"
      },
      "source": [
        "### Dataset construction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1JhaLdfZAnt"
      },
      "outputs": [],
      "source": [
        "def pil_loader(path, codify):\n",
        "    with open(path, 'rb') as f:\n",
        "        img = Image.open(f)\n",
        "        return img.convert(codify)\n",
        "\n",
        "def load_images(root_path, directory, img, mask):\n",
        "    directory_path = root_path / directory\n",
        "    img_path = directory_path / img\n",
        "    mask_path = directory_path / mask\n",
        "    if not img_path.is_dir() or not mask_path.is_dir():\n",
        "        raise RuntimeError(\"folder structure different from expected\")\n",
        "\n",
        "    images = [item.name for item in img_path.iterdir()]\n",
        "    masks = [item.name for item in mask_path.iterdir()]\n",
        "\n",
        "    if set(images) != set(masks):\n",
        "        raise RuntimeError(\"images and masks do not match\")\n",
        "\n",
        "    return images\n",
        "\n",
        "def generate_bd(mask, edge_pad=False, is_flip=False, edge_size=2):\n",
        "\n",
        "    y_k_size = 6\n",
        "    x_k_size = 6\n",
        "\n",
        "    edge = cv2.Canny(mask, 0, 8)\n",
        "    kernel = np.ones((edge_size, edge_size), np.uint8)\n",
        "\n",
        "    if edge_pad:\n",
        "        edge = edge[y_k_size:-y_k_size, x_k_size:-x_k_size]\n",
        "        edge = np.pad(edge, ((y_k_size,y_k_size),(x_k_size,x_k_size)), mode='constant')\n",
        "    edge = (cv2.dilate(edge, kernel, iterations=1)>50)*1.0\n",
        "\n",
        "    return edge\n",
        "\n",
        "class LoveDA(VisionDataset):\n",
        "    def __init__(self, root, img, mask, directories=None, transforms=None, bd=False):\n",
        "        super(LoveDA, self).__init__(root)\n",
        "\n",
        "        root_path = Path(root)\n",
        "\n",
        "        if not root_path.is_dir():\n",
        "            raise RuntimeError(\"root should be a directory\")\n",
        "\n",
        "        self.root = root\n",
        "        self.img_path = img\n",
        "        self.mask_path = mask\n",
        "        self.transforms = transforms\n",
        "\n",
        "        self.image_names = []\n",
        "\n",
        "        self.bd = bd\n",
        "\n",
        "        if directories is None:\n",
        "            raise RuntimeError(\"at least one directory must be passed\")\n",
        "\n",
        "        directories = [directories] if isinstance(directories, str) else directories\n",
        "\n",
        "        for d in directories:\n",
        "          image_names = load_images(root_path, d, img, mask)\n",
        "          self.image_names.extend([(d, image_name) for image_name in image_names])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        dir, image_name = self.image_names[index]\n",
        "        image_path = f'{self.root}/{dir}/{self.img_path}/{image_name}'\n",
        "        mask_path = f'{self.root}/{dir}/{self.mask_path}/{image_name}'\n",
        "\n",
        "        image = pil_loader(image_path, RGB)\n",
        "        mask = pil_loader(mask_path, grayscale)\n",
        "\n",
        "        image = np.array(image)\n",
        "        mask = np.array(mask)\n",
        "\n",
        "        if self.transforms is not None:\n",
        "          data = self.transforms(image=image, mask=mask)\n",
        "          image = data['image']\n",
        "          mask = data['mask']\n",
        "\n",
        "        image = transforms.ToTensor()(image)\n",
        "        mask = transforms.ToTensor()(mask).squeeze(0)\n",
        "        mask = transforms.ToPILImage()(mask)\n",
        "        mask = transforms.PILToTensor()(mask).squeeze(0).long()\n",
        "\n",
        "        mask = mask - 1\n",
        "\n",
        "        if self.bd:\n",
        "            bd = generate_bd(mask.numpy().astype(np.uint8))\n",
        "\n",
        "            return image, mask, bd\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "    def __len__(self):\n",
        "        length = len(self.image_names)\n",
        "        return length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zim4nyhUgTEE"
      },
      "source": [
        "### Statistics and metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUlnYE8RSUYX"
      },
      "source": [
        "#### Average, Standard deviation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZX5a7cLVrqn"
      },
      "outputs": [],
      "source": [
        "def compute_avg_std(dataset, dataloader, device):\n",
        "    with torch.no_grad():\n",
        "        avg = torch.zeros((1,3)).to(device)\n",
        "        std = torch.zeros((1,3)).to(device)\n",
        "        data_len = 0\n",
        "        tot_pixels = 0\n",
        "\n",
        "        assert len(dataloader) > 0, \"Dataloader must contain some data\"\n",
        "\n",
        "        tot_batches = len(dataloader)\n",
        "\n",
        "        for (step, (inputs, labels)) in enumerate(dataloader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            b, _, h, w = inputs.shape\n",
        "\n",
        "            data_len += b\n",
        "            tot_pixels += b * h * w\n",
        "            avg += torch.sum(inputs, dim=(0,2,3))\n",
        "            std += torch.sum(inputs * inputs, dim=(0,2,3))\n",
        "\n",
        "        avg /= tot_pixels\n",
        "        std = torch.sqrt(std / tot_pixels - avg * avg)\n",
        "\n",
        "        return data_len, avg.flatten().tolist(), std.flatten().tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Us9dt9zBSYSq"
      },
      "source": [
        "#### IoU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UBrHS8zpuCS"
      },
      "outputs": [],
      "source": [
        "def calculate_iou(outputs, masks, num_classes):\n",
        "\n",
        "    # Get predictions from the model output probabilities\n",
        "    _, preds = torch.max(outputs, dim=1) # B x H x W\n",
        "\n",
        "    # IoU for each class\n",
        "    iou_per_class = torch.zeros(num_classes, dtype=torch.float32, device=outputs.device)\n",
        "\n",
        "    for i in range(num_classes):  # Iterate over all classes\n",
        "        pred_mask = preds == i\n",
        "        label_mask = masks == i\n",
        "\n",
        "        intersection = torch.logical_and(pred_mask, label_mask).sum().float()\n",
        "        union = torch.logical_or(pred_mask, label_mask).sum().float()\n",
        "\n",
        "        if union > 0:\n",
        "            iou_per_class[i] = intersection / union\n",
        "\n",
        "    # Calculate mIoU for classes with a non-zero IoU\n",
        "    valid_ious = iou_per_class\n",
        "    miou = valid_ious.mean() if len(valid_ious) > 0 else torch.tensor(0.0, device=outputs.device)\n",
        "\n",
        "    return miou, iou_per_class\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIxonvMiSZsv"
      },
      "source": [
        "#### Latency, FPS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODklmynO9hah"
      },
      "outputs": [],
      "source": [
        "def calculate_latency_fps(model, device, height, width, iterations, model_type: ModelType):\n",
        "    image = torch.randn(1, 3, height, width).to(device)\n",
        "    mask = None\n",
        "    boundary = None\n",
        "\n",
        "    if model_type == ModelType.PIDNET:\n",
        "        mask = torch.randint(0, num_classes, (1, height, width), dtype=torch.int64).to(device)\n",
        "        boundary = torch.randint(0, 2, (1, height, width), dtype=torch.float64).to(device)\n",
        "\n",
        "    latency = []\n",
        "    FPS = []\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        start = time.time()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if model_type == ModelType.DEEPLAB:\n",
        "                _ = model(image)\n",
        "            else:\n",
        "                _ = model(image, mask, boundary)\n",
        "\n",
        "        end = time.time()\n",
        "\n",
        "        latency_i = end - start\n",
        "        latency.append(latency_i)\n",
        "\n",
        "        FPS_i = 1 / latency_i\n",
        "        FPS.append(FPS_i)\n",
        "\n",
        "    meanLatency = np.mean(latency) * 1000 # millis\n",
        "    stdLatency = np.std(latency) * 1000\n",
        "    meanFPS = np.mean(FPS)\n",
        "    stdFPS = np.std(FPS)\n",
        "\n",
        "    return meanLatency, stdLatency, meanFPS, stdFPS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gG_0OQ72SdhY"
      },
      "source": [
        "#### FLOPS, Params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qc4UaQlG_lux"
      },
      "outputs": [],
      "source": [
        "def calculate_flops_params(model, device, height, width, model_type: ModelType):\n",
        "    image = torch.zeros(1, 3, height, width).to(device)\n",
        "    model = model.to(device)\n",
        "    flops = None\n",
        "    if model_type == ModelType.PIDNET:\n",
        "        mask = torch.zeros(1, height, width, dtype=torch.int64).to(device)\n",
        "        boundary = torch.zeros(1, height, width, dtype=torch.float64).to(device)\n",
        "        flops = FlopCountAnalysis(model, (image, mask, boundary))\n",
        "    else:\n",
        "        flops = FlopCountAnalysis(model, image)\n",
        "    print(flop_count_table(flops))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8wad1WFHByx"
      },
      "source": [
        "### Plot losses and mious"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tANlpOZDHGoX"
      },
      "outputs": [],
      "source": [
        "def plot_losses_mious(train_losses, eval_losses, miou_scores, num_epochs):\n",
        "    # Crea una figura con due assi, disposti uno accanto all'altro\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 5))\n",
        "\n",
        "    # Disegna il grafico delle perdite di training e validation sul primo asse\n",
        "    ax1.plot(train_losses, label='Training Loss')\n",
        "    ax1.plot(eval_losses, label='Validation Loss')\n",
        "    ax1.set_xlabel('Epochs')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.set_xticks(range(0, num_epochs), range(1, num_epochs + 1))\n",
        "    ax1.set_title('Training and Validation Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid()\n",
        "\n",
        "    # Disegna il grafico di mIoU sul secondo asse\n",
        "    ax2.plot(miou_scores, label='mIoU')\n",
        "    ax2.set_xlabel('Epochs')\n",
        "    ax2.set_ylabel('mIoU')\n",
        "    ax2.set_xticks(range(0, num_epochs), range(1, num_epochs + 1))\n",
        "    ax2.set_title('mIoU')\n",
        "    ax2.legend()\n",
        "    ax2.grid()\n",
        "\n",
        "    # Mostra la figura\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwOd5iQLitoS"
      },
      "outputs": [],
      "source": [
        "def plot_mious_per_category(miou_scores, num_epochs):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for class_name, miou_values in miou_scores.items():\n",
        "        plt.plot(range(num_epochs), miou_values, label=class_name)\n",
        "\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('mIoU (%)')\n",
        "    plt.xticks(range(0, num_epochs), range(1, num_epochs + 1))\n",
        "    plt.title('mIoU per Class over Epochs')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbMkS3H2iS1v"
      },
      "source": [
        "### Checkpoint resume"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yd2ITZT4iWbZ"
      },
      "outputs": [],
      "source": [
        "def resume_checkpoint(resume_path, model, optimizer=None, scheduler=None):\n",
        "    checkpoint = torch.load(resume_path)\n",
        "    iteration = checkpoint['iteration'] + 1\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "    if optimizer is not None:\n",
        "      optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    if scheduler is not None:\n",
        "      scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "    return iteration, model, optimizer, scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEC_H4S0igkg"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(path, iteration, model, optimizer, scheduler):\n",
        "    checkpoint = {\n",
        "        'iteration': iteration,\n",
        "        'model': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'scheduler': scheduler.state_dict()\n",
        "    }\n",
        "    torch.save(checkpoint, path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "II5pa_fmLVv4"
      },
      "source": [
        "### Image visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Teb1PthuoPR8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.colors as mcolors\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "def plot_tensor_mask(mask_tensor, categories):\n",
        "\n",
        "    categories = dict(sorted(categories.items(), key=lambda item: item[1][0]))\n",
        "\n",
        "    # Convert mask tensor to numpy array\n",
        "    mask_array = mask_tensor.squeeze().numpy()\n",
        "\n",
        "    # Create a colored mask image\n",
        "    colored_mask = np.zeros((mask_array.shape[0], mask_array.shape[1], 3), dtype=np.uint8)\n",
        "    for i, (label, (value, color)) in enumerate(categories.items()):\n",
        "        mask = mask_array == i\n",
        "        colored_mask[mask] = color\n",
        "\n",
        "    # Display the colored mask\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.imshow(colored_mask)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    # Create a legend\n",
        "    legend_patches = [mpatches.Patch(color=np.array(color)/255, label=label) for label, (_, color) in categories.items()]\n",
        "    plt.legend(handles=legend_patches, bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8jpvfnedTbG"
      },
      "source": [
        "# Step 2a: Testing classic semantic segmentation network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk_O5SS6BqXV"
      },
      "source": [
        "### Download pre-trained weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoZlf8G-nabe",
        "outputId": "60fa8f65-483e-4bde-cca7-acf6c70de8df"
      },
      "outputs": [],
      "source": [
        "weights_dir = Path(PRETRAINED_WEIGHTS_DIR)\n",
        "if not weights_dir.exists():\n",
        "    weights_dir.mkdir(exist_ok=True)\n",
        "\n",
        "deeplab_v2_weights = Path(DEEPLAB_V2_WEIGHTS)\n",
        "if not deeplab_v2_weights.exists():\n",
        "    # Replace with the correct Google Drive file ID\n",
        "    file_id = '1ZX0UCXvJwqd2uBGCX7LI2n-DfMg3t74v'\n",
        "    gdown.download(id=file_id, output=str(deeplab_v2_weights), quiet=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glRuMNQhsq1p"
      },
      "source": [
        "## Model: DeepLabv2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEYyS132pASP"
      },
      "source": [
        "### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_C5jyCodq6Jf"
      },
      "outputs": [],
      "source": [
        "affine_par = True\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes, affine=affine_par)\n",
        "        for i in self.bn1.parameters():\n",
        "            i.requires_grad = False\n",
        "        padding = dilation\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1,\n",
        "                               padding=padding, bias=False, dilation=dilation)\n",
        "        self.bn2 = nn.BatchNorm2d(planes, affine=affine_par)\n",
        "        for i in self.bn2.parameters():\n",
        "            i.requires_grad = False\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4, affine=affine_par)\n",
        "        for i in self.bn3.parameters():\n",
        "            i.requires_grad = False\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ClassifierModule(nn.Module):\n",
        "    def __init__(self, inplanes, dilation_series, padding_series, num_classes):\n",
        "        super(ClassifierModule, self).__init__()\n",
        "        self.conv2d_list = nn.ModuleList()\n",
        "        for dilation, padding in zip(dilation_series, padding_series):\n",
        "            self.conv2d_list.append(\n",
        "                nn.Conv2d(inplanes, num_classes, kernel_size=3, stride=1, padding=padding,\n",
        "                          dilation=dilation, bias=True))\n",
        "\n",
        "        for m in self.conv2d_list:\n",
        "            m.weight.data.normal_(0, 0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv2d_list[0](x)\n",
        "        for i in range(len(self.conv2d_list) - 1):\n",
        "            out += self.conv2d_list[i + 1](x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNetMulti(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes):\n",
        "        self.inplanes = 64\n",
        "        super(ResNetMulti, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64, affine=affine_par)\n",
        "\n",
        "        for i in self.bn1.parameters():\n",
        "            i.requires_grad = False\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=True)  # change\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4)\n",
        "        self.layer6 = ClassifierModule(2048, [6, 12, 18, 24], [6, 12, 18, 24], num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n",
        "        downsample = None\n",
        "        if (stride != 1\n",
        "                or self.inplanes != planes * block.expansion\n",
        "                or dilation == 2\n",
        "                or dilation == 4):\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion, affine=affine_par))\n",
        "        for i in downsample._modules['1'].parameters():\n",
        "            i.requires_grad = False\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            block(self.inplanes, planes, stride, dilation=dilation, downsample=downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, dilation=dilation))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, _, H, W = x.size()\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.layer6(x)\n",
        "\n",
        "        x = torch.nn.functional.interpolate(x, size=(H, W), mode='bilinear')\n",
        "\n",
        "        if self.training == True:\n",
        "            return x, None, None\n",
        "\n",
        "        return x\n",
        "\n",
        "    def get_1x_lr_params_no_scale(self):\n",
        "        \"\"\"\n",
        "        This generator returns all the parameters of the net except for\n",
        "        the last classification layer. Note that for each batchnorm layer,\n",
        "        requires_grad is set to False in deeplab_resnet.py, therefore this function does not return\n",
        "        any batchnorm parameter\n",
        "        \"\"\"\n",
        "        b = []\n",
        "\n",
        "        b.append(self.conv1)\n",
        "        b.append(self.bn1)\n",
        "        b.append(self.layer1)\n",
        "        b.append(self.layer2)\n",
        "        b.append(self.layer3)\n",
        "        b.append(self.layer4)\n",
        "\n",
        "        for i in range(len(b)):\n",
        "            for j in b[i].modules():\n",
        "                jj = 0\n",
        "                for k in j.parameters():\n",
        "                    jj += 1\n",
        "                    if k.requires_grad:\n",
        "                        yield k\n",
        "\n",
        "    def get_10x_lr_params(self):\n",
        "        \"\"\"\n",
        "        This generator returns all the parameters for the last layer of the net,\n",
        "        which does the classification of pixel into classes\n",
        "        \"\"\"\n",
        "        b = []\n",
        "        if self.multi_level:\n",
        "            b.append(self.layer5.parameters())\n",
        "        b.append(self.layer6.parameters())\n",
        "\n",
        "        for j in range(len(b)):\n",
        "            for i in b[j]:\n",
        "                yield i\n",
        "\n",
        "    def optim_parameters(self, lr):\n",
        "        return [{'params': self.get_1x_lr_params_no_scale(), 'lr': lr},\n",
        "                {'params': self.get_10x_lr_params(), 'lr': 10 * lr}]\n",
        "\n",
        "\n",
        "def get_deeplab_v2(num_classes=19, pretrain=True, pretrain_model_path='DeepLab_resnet_pretrained_imagenet.pth'):\n",
        "    model = ResNetMulti(Bottleneck, [3, 4, 23, 3], num_classes)\n",
        "\n",
        "    # Pretraining loading\n",
        "    if pretrain:\n",
        "        print('Deeplab pretraining loading...')\n",
        "        saved_state_dict = torch.load(pretrain_model_path, weights_only=True)\n",
        "\n",
        "        new_params = model.state_dict().copy()\n",
        "        for i in saved_state_dict:\n",
        "            i_parts = i.split('.')\n",
        "            new_params['.'.join(i_parts[1:])] = saved_state_dict[i]\n",
        "        model.load_state_dict(new_params, strict=False)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDZFT-myZ1Xd"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTdVkPSJoR4M"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XajKG5FoRhQ"
      },
      "outputs": [],
      "source": [
        "# Change in case of resume training\n",
        "RESUME_TRAINING = False\n",
        "RESUME_PATH = f\"/content/drive/MyDrive/loveDA_dataset/Model training/DeepLab/DeepLabV2_{num_epochs}_{learning_rate}_{step_size}_{gamma}_{resize}_{w_decay}_epoch{epoch}.pth.tar\"\n",
        "\n",
        "num_epochs = 20\n",
        "BATCH_SIZE = 6\n",
        "learning_rate = 1e-3\n",
        "step_size = 10\n",
        "gamma = 0.1\n",
        "resize = 512\n",
        "w_decay = 1e-3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y7JhEDjpOk1"
      },
      "source": [
        "### Dataset preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PhgKOtw9p7X"
      },
      "source": [
        "#### Normalization metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10-2EUtg9uMk"
      },
      "outputs": [],
      "source": [
        "#preprocessing_dataset = LoveDA(TRAIN_DIR, IMG_PATH, MASK_PATH, directories=URBAN_PATH)\n",
        "# No shuffle (waste of time), no drop last (we lose some data)\n",
        "\n",
        "num_workers = 2 if device.type == 'cuda' else 0\n",
        "\n",
        "#preprocessing_dataloader = DataLoader(preprocessing_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=num_workers)\n",
        "#_, avg, std = compute_avg_std(preprocessing_dataset, preprocessing_dataloader, device)\n",
        "\n",
        "# Poiché il modello è pretrainato su ImageNet, si usano media e varianza di ImageNet\n",
        "avg = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ-c2UvI9zLl"
      },
      "source": [
        "#### Training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8a0vk2FFpQ6D"
      },
      "outputs": [],
      "source": [
        "train_transform = A.Compose([\n",
        "    A.Normalize(mean=avg, std=std, p=1, always_apply=True, max_pixel_value=255),\n",
        "    A.Resize(resize, resize, p=1, always_apply=True)\n",
        "])\n",
        "train_dataset = LoveDA(TRAIN_DIR, IMG_PATH, MASK_PATH, directories=URBAN_PATH, transforms=train_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=num_workers, worker_init_fn=seed_worker, generator=g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fF9WvY_927b"
      },
      "source": [
        "#### Validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2zFQmoB76kv"
      },
      "outputs": [],
      "source": [
        "val_transform = A.Compose([\n",
        "    A.Normalize(mean=avg, std=std, p=1, always_apply=True, max_pixel_value=255)\n",
        "])\n",
        "val_dataset = LoveDA(VAL_DIR, IMG_PATH, MASK_PATH, directories=URBAN_PATH, transforms=val_transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=num_workers, worker_init_fn=seed_worker, generator=g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpUmP3uj-byv"
      },
      "source": [
        "### Training process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnRpfdeq_o0u"
      },
      "source": [
        "#### Model engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvUK7Dv_8Qol"
      },
      "outputs": [],
      "source": [
        "model = get_deeplab_v2(num_classes=num_classes, pretrain=True, pretrain_model_path=DEEPLAB_V2_WEIGHTS).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=w_decay)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4RC0eIiHoY2"
      },
      "source": [
        "#### Model handling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VBqKEft_0iB"
      },
      "source": [
        "#### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3g1ubaFc5AD"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "\n",
        "if RESUME_TRAINING:\n",
        "  start_epoch, model, optimizer, scheduler = resume_checkpoint(RESUME_PATH, model, optimizer, scheduler)\n",
        "else:\n",
        "  start_epoch = 0\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    print(\"### Training mode\")\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (images, masks) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs, _, _ = model(images)\n",
        "        loss = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if (i + 1) % 25 == 0:\n",
        "            print(f\"Processed {i + 1} batches, loss: {running_loss / (i+1)}\")\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(epoch_loss)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}\")\n",
        "    path=f\"/content/drive/MyDrive/loveDA_dataset/Model training/DeepLab/DeepLabV2_{num_epochs}_{learning_rate}_{step_size}_{gamma}_{resize}_{w_decay}_epoch{epoch}.pth.tar\"\n",
        "    save_checkpoint(path, epoch, model, optimizer, scheduler)\n",
        "\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPeSXzIwhqdD"
      },
      "source": [
        "#### Evaluation loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VLNQrTBhjDH"
      },
      "outputs": [],
      "source": [
        "# Requires saving the models for each epoch\n",
        "\n",
        "start_epoch_eval = 0\n",
        "\n",
        "eval_losses = []\n",
        "mious = []\n",
        "\n",
        "for epoch in range(start_epoch_eval, num_epochs):\n",
        "\n",
        "    model = get_deeplab_v2(num_classes=num_classes, pretrain=False)  # Assuming get_deeplab_v2 is defined\n",
        "\n",
        "    path = f\"/content/drive/MyDrive/loveDA_dataset/Model training/DeepLab/DeepLabV2_{num_epochs}_{learning_rate}_{step_size}_{gamma}_{resize}_{w_decay}_epoch{epoch}.pth.tar\"\n",
        "    _, model, _, _ = resume_checkpoint(path, model)\n",
        "\n",
        "    model.to(device)\n",
        "    print(\"### Evaluation mode\")\n",
        "    miou = 0.0 # Accumulator for mIoU\n",
        "    val_loss = 0.0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (images, masks) in enumerate(val_loader):\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            # loss\n",
        "            outputs= model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # mIoU\n",
        "            iou, _ = calculate_iou(outputs, masks, num_classes)\n",
        "            miou += iou\n",
        "\n",
        "            if (i + 1) % 25 == 0:\n",
        "                print(f\"Processed {i + 1} batches: loss {val_loss / (i+1)}, mIoU: {miou / (i+1)}\")\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "    eval_losses.append(val_loss)\n",
        "\n",
        "    miou /= len(val_loader)\n",
        "\n",
        "    mious.append(miou)\n",
        "\n",
        "    print(f\"Epoch: [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}, mIoU: {(miou * 100):.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D1oBEq_CwDn"
      },
      "source": [
        "### Metric calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qg_YfiVjCF1j"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "mean_latency, _, _, _ = calculate_latency_fps(model, device, 1024, 1024, num_epochs, ModelType.DEEPLAB)\n",
        "print(f\"Mean Latency: {mean_latency:.2f} ms\")\n",
        "\n",
        "calculate_flops_params(model, device, 1024, 1024, ModelType.DEEPLAB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ww8qQrf8bgye"
      },
      "source": [
        "# PIDNet Implementation (execute for any Step from 2b on)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ie5UdjLXxKEQ"
      },
      "source": [
        "## Model: PIDnet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XvgDYVTpV7C"
      },
      "source": [
        "### Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WlsXvYNrsi3"
      },
      "source": [
        "#### Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sr94iPVMrv5v"
      },
      "outputs": [],
      "source": [
        "BatchNorm2d = nn.BatchNorm2d\n",
        "bn_mom = 0.1\n",
        "algc = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqYxBtrFrx3y"
      },
      "source": [
        "#### BasicBlock"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1-0dhaOrz5L"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, no_relu=False):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn1 = BatchNorm2d(planes, momentum=bn_mom)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = BatchNorm2d(planes, momentum=bn_mom)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.no_relu = no_relu\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "\n",
        "        if self.no_relu:\n",
        "            return out\n",
        "        else:\n",
        "            return self.relu(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHqXmzehr2fm"
      },
      "source": [
        "#### Bottleneck"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBbwYnaqr3oP"
      },
      "outputs": [],
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    expansion = 2\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, no_relu=True):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = BatchNorm2d(planes, momentum=bn_mom)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = BatchNorm2d(planes, momentum=bn_mom)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\n",
        "                               bias=False)\n",
        "        self.bn3 = BatchNorm2d(planes * self.expansion, momentum=bn_mom)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.no_relu = no_relu\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        if self.no_relu:\n",
        "            return out\n",
        "        else:\n",
        "            return self.relu(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUns-0fWr5-P"
      },
      "source": [
        "#### SegmentHead"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQ13vvA8r8ck"
      },
      "outputs": [],
      "source": [
        "class segmenthead(nn.Module):\n",
        "\n",
        "    def __init__(self, inplanes, interplanes, outplanes, scale_factor=None):\n",
        "        super(segmenthead, self).__init__()\n",
        "        self.bn1 = BatchNorm2d(inplanes, momentum=bn_mom)\n",
        "        self.conv1 = nn.Conv2d(inplanes, interplanes, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = BatchNorm2d(interplanes, momentum=bn_mom)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(interplanes, outplanes, kernel_size=1, padding=0, bias=True)\n",
        "        self.scale_factor = scale_factor\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(self.relu(self.bn1(x)))\n",
        "        out = self.conv2(self.relu(self.bn2(x)))\n",
        "\n",
        "        if self.scale_factor is not None:\n",
        "            height = x.shape[-2] * self.scale_factor\n",
        "            width = x.shape[-1] * self.scale_factor\n",
        "            out = F.interpolate(out,\n",
        "                        size=[height, width],\n",
        "                        mode='bilinear', align_corners=algc)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg6EOQrxr-Bn"
      },
      "source": [
        "#### DAPPM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-qNKFqAr--T"
      },
      "outputs": [],
      "source": [
        "class DAPPM(nn.Module):\n",
        "    def __init__(self, inplanes, branch_planes, outplanes, BatchNorm=nn.BatchNorm2d):\n",
        "        super(DAPPM, self).__init__()\n",
        "        bn_mom = 0.1\n",
        "        self.scale1 = nn.Sequential(nn.AvgPool2d(kernel_size=5, stride=2, padding=2),\n",
        "                                    BatchNorm(inplanes, momentum=bn_mom),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n",
        "                                    )\n",
        "        self.scale2 = nn.Sequential(nn.AvgPool2d(kernel_size=9, stride=4, padding=4),\n",
        "                                    BatchNorm(inplanes, momentum=bn_mom),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n",
        "                                    )\n",
        "        self.scale3 = nn.Sequential(nn.AvgPool2d(kernel_size=17, stride=8, padding=8),\n",
        "                                    BatchNorm(inplanes, momentum=bn_mom),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n",
        "                                    )\n",
        "        self.scale4 = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n",
        "                                    BatchNorm(inplanes, momentum=bn_mom),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n",
        "                                    )\n",
        "        self.scale0 = nn.Sequential(\n",
        "                                    BatchNorm(inplanes, momentum=bn_mom),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n",
        "                                    )\n",
        "        self.process1 = nn.Sequential(\n",
        "                                    BatchNorm(branch_planes, momentum=bn_mom),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n",
        "                                    )\n",
        "        self.process2 = nn.Sequential(\n",
        "                                    BatchNorm(branch_planes, momentum=bn_mom),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n",
        "                                    )\n",
        "        self.process3 = nn.Sequential(\n",
        "                                    BatchNorm(branch_planes, momentum=bn_mom),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n",
        "                                    )\n",
        "        self.process4 = nn.Sequential(\n",
        "                                    BatchNorm(branch_planes, momentum=bn_mom),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n",
        "                                    )\n",
        "        self.compression = nn.Sequential(\n",
        "                                    BatchNorm(branch_planes * 5, momentum=bn_mom),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(branch_planes * 5, outplanes, kernel_size=1, bias=False),\n",
        "                                    )\n",
        "        self.shortcut = nn.Sequential(\n",
        "                                    BatchNorm(inplanes, momentum=bn_mom),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(inplanes, outplanes, kernel_size=1, bias=False),\n",
        "                                    )\n",
        "\n",
        "    def forward(self, x):\n",
        "        width = x.shape[-1]\n",
        "        height = x.shape[-2]\n",
        "        x_list = []\n",
        "\n",
        "        x_list.append(self.scale0(x))\n",
        "        x_list.append(self.process1((F.interpolate(self.scale1(x),\n",
        "                        size=[height, width],\n",
        "                        mode='bilinear', align_corners=algc)+x_list[0])))\n",
        "        x_list.append((self.process2((F.interpolate(self.scale2(x),\n",
        "                        size=[height, width],\n",
        "                        mode='bilinear', align_corners=algc)+x_list[1]))))\n",
        "        x_list.append(self.process3((F.interpolate(self.scale3(x),\n",
        "                        size=[height, width],\n",
        "                        mode='bilinear', align_corners=algc)+x_list[2])))\n",
        "        x_list.append(self.process4((F.interpolate(self.scale4(x),\n",
        "                        size=[height, width],\n",
        "                        mode='bilinear', align_corners=algc)+x_list[3])))\n",
        "\n",
        "        out = self.compression(torch.cat(x_list, 1)) + self.shortcut(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFs_hru3sCIY"
      },
      "source": [
        "#### PAPPM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8viyWDssEgN"
      },
      "outputs": [],
      "source": [
        "class PAPPM(nn.Module):\n",
        "    def __init__(self, inplanes, branch_planes, outplanes, BatchNorm=nn.BatchNorm2d):\n",
        "        super(PAPPM, self).__init__()\n",
        "        bn_mom = 0.1\n",
        "        self.scale1 = nn.Sequential(nn.AvgPool2d(kernel_size=5, stride=2, padding=2),\n",
        "                                    BatchNorm(inplanes, momentum=bn_mom),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n",
        "                                    )\n",
        "        self.scale2 = nn.Sequential(nn.AvgPool2d(kernel_size=9, stride=4, padding=4),\n",
        "                                    BatchNorm(inplanes, momentum=bn_mom),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n",
        "                                    )\n",
        "        self.scale3 = nn.Sequential(nn.AvgPool2d(kernel_size=17, stride=8, padding=8),\n",
        "                                    BatchNorm(inplanes, momentum=bn_mom),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n",
        "                                    )\n",
        "        self.scale4 = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n",
        "                                    BatchNorm(inplanes, momentum=bn_mom),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n",
        "                                    )\n",
        "\n",
        "        self.scale0 = nn.Sequential(\n",
        "                                    BatchNorm(inplanes, momentum=bn_mom),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n",
        "                                    )\n",
        "\n",
        "        self.scale_process = nn.Sequential(\n",
        "                                    BatchNorm(branch_planes*4, momentum=bn_mom),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(branch_planes*4, branch_planes*4, kernel_size=3, padding=1, groups=4, bias=False),\n",
        "                                    )\n",
        "\n",
        "\n",
        "        self.compression = nn.Sequential(\n",
        "                                    BatchNorm(branch_planes * 5, momentum=bn_mom),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(branch_planes * 5, outplanes, kernel_size=1, bias=False),\n",
        "                                    )\n",
        "\n",
        "        self.shortcut = nn.Sequential(\n",
        "                                    BatchNorm(inplanes, momentum=bn_mom),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(inplanes, outplanes, kernel_size=1, bias=False),\n",
        "                                    )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        width = x.shape[-1]\n",
        "        height = x.shape[-2]\n",
        "        scale_list = []\n",
        "\n",
        "        x_ = self.scale0(x)\n",
        "        scale_list.append(F.interpolate(self.scale1(x), size=[height, width],\n",
        "                        mode='bilinear', align_corners=algc)+x_)\n",
        "        scale_list.append(F.interpolate(self.scale2(x), size=[height, width],\n",
        "                        mode='bilinear', align_corners=algc)+x_)\n",
        "        scale_list.append(F.interpolate(self.scale3(x), size=[height, width],\n",
        "                        mode='bilinear', align_corners=algc)+x_)\n",
        "        scale_list.append(F.interpolate(self.scale4(x), size=[height, width],\n",
        "                        mode='bilinear', align_corners=algc)+x_)\n",
        "\n",
        "        scale_out = self.scale_process(torch.cat(scale_list, 1))\n",
        "\n",
        "        out = self.compression(torch.cat([x_,scale_out], 1)) + self.shortcut(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL6hAM6nsF2o"
      },
      "source": [
        "#### PagFM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lV2NpzorsIBe"
      },
      "outputs": [],
      "source": [
        "class PagFM(nn.Module):\n",
        "    def __init__(self, in_channels, mid_channels, after_relu=False, with_channel=False, BatchNorm=nn.BatchNorm2d):\n",
        "        super(PagFM, self).__init__()\n",
        "        self.with_channel = with_channel\n",
        "        self.after_relu = after_relu\n",
        "        self.f_x = nn.Sequential(\n",
        "                                nn.Conv2d(in_channels, mid_channels,\n",
        "                                          kernel_size=1, bias=False),\n",
        "                                BatchNorm(mid_channels)\n",
        "                                )\n",
        "        self.f_y = nn.Sequential(\n",
        "                                nn.Conv2d(in_channels, mid_channels,\n",
        "                                          kernel_size=1, bias=False),\n",
        "                                BatchNorm(mid_channels)\n",
        "                                )\n",
        "        if with_channel:\n",
        "            self.up = nn.Sequential(\n",
        "                                    nn.Conv2d(mid_channels, in_channels,\n",
        "                                              kernel_size=1, bias=False),\n",
        "                                    BatchNorm(in_channels)\n",
        "                                   )\n",
        "        if after_relu:\n",
        "            self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        input_size = x.size()\n",
        "        if self.after_relu:\n",
        "            y = self.relu(y)\n",
        "            x = self.relu(x)\n",
        "\n",
        "        y_q = self.f_y(y)\n",
        "        y_q = F.interpolate(y_q, size=[input_size[2], input_size[3]],\n",
        "                            mode='bilinear', align_corners=False)\n",
        "        x_k = self.f_x(x)\n",
        "\n",
        "        if self.with_channel:\n",
        "            sim_map = torch.sigmoid(self.up(x_k * y_q))\n",
        "        else:\n",
        "            sim_map = torch.sigmoid(torch.sum(x_k * y_q, dim=1).unsqueeze(1))\n",
        "\n",
        "        y = F.interpolate(y, size=[input_size[2], input_size[3]],\n",
        "                            mode='bilinear', align_corners=False)\n",
        "        x = (1-sim_map)*x + sim_map*y\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIceFMp-sI81"
      },
      "source": [
        "#### LightBag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jM0QYdEIsLDb"
      },
      "outputs": [],
      "source": [
        "class Light_Bag(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, BatchNorm=nn.BatchNorm2d):\n",
        "        super(Light_Bag, self).__init__()\n",
        "        self.conv_p = nn.Sequential(\n",
        "                                nn.Conv2d(in_channels, out_channels,\n",
        "                                          kernel_size=1, bias=False),\n",
        "                                BatchNorm(out_channels)\n",
        "                                )\n",
        "        self.conv_i = nn.Sequential(\n",
        "                                nn.Conv2d(in_channels, out_channels,\n",
        "                                          kernel_size=1, bias=False),\n",
        "                                BatchNorm(out_channels)\n",
        "                                )\n",
        "\n",
        "    def forward(self, p, i, d):\n",
        "        edge_att = torch.sigmoid(d)\n",
        "\n",
        "        p_add = self.conv_p((1-edge_att)*i + p)\n",
        "        i_add = self.conv_i(i + edge_att*p)\n",
        "\n",
        "        return p_add + i_add"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiTFUwt9sRVC"
      },
      "source": [
        "#### DDFMv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9wVwobwsTua"
      },
      "outputs": [],
      "source": [
        "class DDFMv2(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, BatchNorm=nn.BatchNorm2d):\n",
        "        super(DDFMv2, self).__init__()\n",
        "        self.conv_p = nn.Sequential(\n",
        "                                BatchNorm(in_channels),\n",
        "                                nn.ReLU(inplace=True),\n",
        "                                nn.Conv2d(in_channels, out_channels,\n",
        "                                          kernel_size=1, bias=False),\n",
        "                                BatchNorm(out_channels)\n",
        "                                )\n",
        "        self.conv_i = nn.Sequential(\n",
        "                                BatchNorm(in_channels),\n",
        "                                nn.ReLU(inplace=True),\n",
        "                                nn.Conv2d(in_channels, out_channels,\n",
        "                                          kernel_size=1, bias=False),\n",
        "                                BatchNorm(out_channels)\n",
        "                                )\n",
        "\n",
        "    def forward(self, p, i, d):\n",
        "        edge_att = torch.sigmoid(d)\n",
        "\n",
        "        p_add = self.conv_p((1-edge_att)*i + p)\n",
        "        i_add = self.conv_i(i + edge_att*p)\n",
        "\n",
        "        return p_add + i_add"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wx7B4zXsVYJ"
      },
      "source": [
        "#### Bag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jp5N8biH0_ua"
      },
      "outputs": [],
      "source": [
        "class Bag(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, BatchNorm=nn.BatchNorm2d):\n",
        "        super(Bag, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "                                BatchNorm(in_channels),\n",
        "                                nn.ReLU(inplace=True),\n",
        "                                nn.Conv2d(in_channels, out_channels,\n",
        "                                          kernel_size=3, padding=1, bias=False)\n",
        "                                )\n",
        "\n",
        "\n",
        "    def forward(self, p, i, d):\n",
        "        edge_att = torch.sigmoid(d)\n",
        "        return self.conv(edge_att*p + (1-edge_att)*i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6xk3xspsfVQ"
      },
      "source": [
        "#### PIDNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qb9XeuCrxRSJ"
      },
      "outputs": [],
      "source": [
        "class PIDNet(nn.Module):\n",
        "\n",
        "    def __init__(self, m=2, n=3, num_classes=19, planes=64, ppm_planes=96, head_planes=128, augment=True):\n",
        "        super(PIDNet, self).__init__()\n",
        "        self.augment = augment\n",
        "\n",
        "        # I Branch\n",
        "        self.conv1 =  nn.Sequential(\n",
        "                          nn.Conv2d(3,planes,kernel_size=3, stride=2, padding=1),\n",
        "                          BatchNorm2d(planes, momentum=bn_mom),\n",
        "                          nn.ReLU(inplace=True),\n",
        "                          nn.Conv2d(planes,planes,kernel_size=3, stride=2, padding=1),\n",
        "                          BatchNorm2d(planes, momentum=bn_mom),\n",
        "                          nn.ReLU(inplace=True),\n",
        "                      )\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self._make_layer(BasicBlock, planes, planes, m)\n",
        "        self.layer2 = self._make_layer(BasicBlock, planes, planes * 2, m, stride=2)\n",
        "        self.layer3 = self._make_layer(BasicBlock, planes * 2, planes * 4, n, stride=2)\n",
        "        self.layer4 = self._make_layer(BasicBlock, planes * 4, planes * 8, n, stride=2)\n",
        "        self.layer5 =  self._make_layer(Bottleneck, planes * 8, planes * 8, 2, stride=2)\n",
        "\n",
        "        # P Branch\n",
        "        self.compression3 = nn.Sequential(\n",
        "                                          nn.Conv2d(planes * 4, planes * 2, kernel_size=1, bias=False),\n",
        "                                          BatchNorm2d(planes * 2, momentum=bn_mom),\n",
        "                                          )\n",
        "\n",
        "        self.compression4 = nn.Sequential(\n",
        "                                          nn.Conv2d(planes * 8, planes * 2, kernel_size=1, bias=False),\n",
        "                                          BatchNorm2d(planes * 2, momentum=bn_mom),\n",
        "                                          )\n",
        "        self.pag3 = PagFM(planes * 2, planes)\n",
        "        self.pag4 = PagFM(planes * 2, planes)\n",
        "\n",
        "        self.layer3_ = self._make_layer(BasicBlock, planes * 2, planes * 2, m)\n",
        "        self.layer4_ = self._make_layer(BasicBlock, planes * 2, planes * 2, m)\n",
        "        self.layer5_ = self._make_layer(Bottleneck, planes * 2, planes * 2, 1)\n",
        "\n",
        "        # D Branch\n",
        "        if m == 2:\n",
        "            self.layer3_d = self._make_single_layer(BasicBlock, planes * 2, planes)\n",
        "            self.layer4_d = self._make_layer(Bottleneck, planes, planes, 1)\n",
        "            self.diff3 = nn.Sequential(\n",
        "                                        nn.Conv2d(planes * 4, planes, kernel_size=3, padding=1, bias=False),\n",
        "                                        BatchNorm2d(planes, momentum=bn_mom),\n",
        "                                        )\n",
        "            self.diff4 = nn.Sequential(\n",
        "                                     nn.Conv2d(planes * 8, planes * 2, kernel_size=3, padding=1, bias=False),\n",
        "                                     BatchNorm2d(planes * 2, momentum=bn_mom),\n",
        "                                     )\n",
        "            self.spp = PAPPM(planes * 16, ppm_planes, planes * 4)\n",
        "            self.dfm = Light_Bag(planes * 4, planes * 4)\n",
        "        else:\n",
        "            self.layer3_d = self._make_single_layer(BasicBlock, planes * 2, planes * 2)\n",
        "            self.layer4_d = self._make_single_layer(BasicBlock, planes * 2, planes * 2)\n",
        "            self.diff3 = nn.Sequential(\n",
        "                                        nn.Conv2d(planes * 4, planes * 2, kernel_size=3, padding=1, bias=False),\n",
        "                                        BatchNorm2d(planes * 2, momentum=bn_mom),\n",
        "                                        )\n",
        "            self.diff4 = nn.Sequential(\n",
        "                                     nn.Conv2d(planes * 8, planes * 2, kernel_size=3, padding=1, bias=False),\n",
        "                                     BatchNorm2d(planes * 2, momentum=bn_mom),\n",
        "                                     )\n",
        "            self.spp = DAPPM(planes * 16, ppm_planes, planes * 4)\n",
        "            self.dfm = Bag(planes * 4, planes * 4)\n",
        "\n",
        "        self.layer5_d = self._make_layer(Bottleneck, planes * 2, planes * 2, 1)\n",
        "\n",
        "        # Prediction Head\n",
        "        if self.augment:\n",
        "            self.seghead_p = segmenthead(planes * 2, head_planes, num_classes)\n",
        "            self.seghead_d = segmenthead(planes * 2, planes, 1)\n",
        "\n",
        "        self.final_layer = segmenthead(planes * 4, head_planes, num_classes)\n",
        "\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion, momentum=bn_mom),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(inplanes, planes, stride, downsample))\n",
        "        inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            if i == (blocks-1):\n",
        "                layers.append(block(inplanes, planes, stride=1, no_relu=True))\n",
        "            else:\n",
        "                layers.append(block(inplanes, planes, stride=1, no_relu=False))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _make_single_layer(self, block, inplanes, planes, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion, momentum=bn_mom),\n",
        "            )\n",
        "\n",
        "        layer = block(inplanes, planes, stride, downsample, no_relu=True)\n",
        "\n",
        "        return layer\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        width_output = x.shape[-1] // 8\n",
        "        height_output = x.shape[-2] // 8\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu(self.layer2(self.relu(x)))\n",
        "        x_ = self.layer3_(x)\n",
        "        x_d = self.layer3_d(x)\n",
        "\n",
        "        x = self.relu(self.layer3(x))\n",
        "        x_ = self.pag3(x_, self.compression3(x))\n",
        "        x_d = x_d + F.interpolate(\n",
        "                        self.diff3(x),\n",
        "                        size=[height_output, width_output],\n",
        "                        mode='bilinear', align_corners=algc)\n",
        "        if self.augment:\n",
        "            temp_p = x_\n",
        "\n",
        "        x = self.relu(self.layer4(x))\n",
        "        x_ = self.layer4_(self.relu(x_))\n",
        "        x_d = self.layer4_d(self.relu(x_d))\n",
        "\n",
        "        x_ = self.pag4(x_, self.compression4(x))\n",
        "        x_d = x_d + F.interpolate(\n",
        "                        self.diff4(x),\n",
        "                        size=[height_output, width_output],\n",
        "                        mode='bilinear', align_corners=algc)\n",
        "        if self.augment:\n",
        "            temp_d = x_d\n",
        "\n",
        "        x_ = self.layer5_(self.relu(x_))\n",
        "        x_d = self.layer5_d(self.relu(x_d))\n",
        "        x = F.interpolate(\n",
        "                        self.spp(self.layer5(x)),\n",
        "                        size=[height_output, width_output],\n",
        "                        mode='bilinear', align_corners=algc)\n",
        "\n",
        "        x_ = self.final_layer(self.dfm(x_, x, x_d))\n",
        "\n",
        "        if self.augment:\n",
        "            x_extra_p = self.seghead_p(temp_p)\n",
        "            x_extra_d = self.seghead_d(temp_d)\n",
        "            return [x_extra_p, x_, x_extra_d]\n",
        "        else:\n",
        "            return x_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQf0GNbGsmF_"
      },
      "source": [
        "#### SemanticLoss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6wZsa7zWWWY"
      },
      "source": [
        "##### CrossEntropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kb3toAV5IMeb"
      },
      "outputs": [],
      "source": [
        "class CrossEntropy(nn.Module):\n",
        "    def __init__(self, ignore_label=-1, weight=None):\n",
        "        super(CrossEntropy, self).__init__()\n",
        "        self.ignore_label = ignore_label\n",
        "        self.criterion = nn.CrossEntropyLoss(\n",
        "            weight=weight,\n",
        "            ignore_index=ignore_label\n",
        "        )\n",
        "\n",
        "    def _forward(self, score, target):\n",
        "\n",
        "        loss = self.criterion(score, target)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def forward(self, score, target):\n",
        "\n",
        "        # From original configs\n",
        "        balance_weights = [0.4, 1.0]\n",
        "        sb_weights = 1.0\n",
        "\n",
        "        if len(balance_weights) == len(score):\n",
        "            return sum([w * self._forward(x, target) for (w, x) in zip(balance_weights, score)])\n",
        "        elif len(score) == 1:\n",
        "            return sb_weights * self._forward(score[0], target)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"lengths of prediction and target are not identical!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lvrZlx9WYB6"
      },
      "source": [
        "##### OHEM Cross Entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWnHp6r1ZcWa"
      },
      "outputs": [],
      "source": [
        "class OhemCrossEntropy(nn.Module):\n",
        "    def __init__(self, ignore_label=-1, thres=0.7,\n",
        "                 min_kept=100000, weight=None):\n",
        "        super(OhemCrossEntropy, self).__init__()\n",
        "        self.thresh = thres\n",
        "        self.min_kept = max(1, min_kept)\n",
        "        self.ignore_label = ignore_label\n",
        "        self.criterion = nn.CrossEntropyLoss(\n",
        "            weight=weight,\n",
        "            ignore_index=ignore_label,\n",
        "            reduction='none'\n",
        "        )\n",
        "\n",
        "    def _ce_forward(self, score, target):\n",
        "\n",
        "        loss = self.criterion(score, target)\n",
        "\n",
        "        return loss.mean()\n",
        "\n",
        "    def _ohem_forward(self, score, target, **kwargs):\n",
        "\n",
        "        pred = F.softmax(score, dim=1)\n",
        "        pixel_losses = self.criterion(score, target).contiguous().view(-1)\n",
        "        mask = target.contiguous().view(-1) != self.ignore_label\n",
        "\n",
        "        tmp_target = target.clone()\n",
        "        tmp_target[tmp_target == self.ignore_label] = 0\n",
        "        pred = pred.gather(1, tmp_target.unsqueeze(1))\n",
        "        pred, ind = pred.contiguous().view(-1,)[mask].contiguous().sort()\n",
        "\n",
        "        min_value = pred[min(self.min_kept, pred.numel() - 1)]\n",
        "\n",
        "        threshold = max(min_value, self.thresh)\n",
        "\n",
        "        pixel_losses = pixel_losses[mask][ind]\n",
        "        pixel_losses = pixel_losses[pred < threshold]\n",
        "        return pixel_losses.mean()\n",
        "\n",
        "    def forward(self, score, target):\n",
        "\n",
        "        if not (isinstance(score, list) or isinstance(score, tuple)):\n",
        "            score = [score]\n",
        "\n",
        "        balance_weights = [0.4, 1.0]\n",
        "        sb_weights = 1.0\n",
        "\n",
        "        if len(balance_weights) == len(score):\n",
        "            functions = [self._ce_forward] * (len(balance_weights) - 1) + [self._ohem_forward]\n",
        "            return sum([w * func(x, target) for (w, x, func) in zip(balance_weights, score, functions)])\n",
        "\n",
        "        elif len(score) == 1:\n",
        "            return sb_weights * self._ohem_forward(score[0], target)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"lengths of prediction and target are not identical!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTbG6XMB5qyZ"
      },
      "source": [
        "##### Focal Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJ3MytsV6KRw"
      },
      "outputs": [],
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=0, alpha=None, ignore_label=-1, weight=None):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.ignore_label = ignore_label\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.criterion = nn.CrossEntropyLoss(\n",
        "            weight=weight,\n",
        "            ignore_index=ignore_label\n",
        "        )\n",
        "\n",
        "    def _forward(self, score, target):\n",
        "\n",
        "        ce_loss = self.criterion(score, target)\n",
        "\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = torch.pow(1 - pt, self.gamma)\n",
        "\n",
        "        if self.alpha is not None:\n",
        "            return self.alpha * focal_loss\n",
        "        return focal_loss\n",
        "\n",
        "    def forward(self, score, target):\n",
        "\n",
        "        if not (isinstance(score, list) or isinstance(score, tuple)):\n",
        "            score = [score]\n",
        "\n",
        "        # From original configs\n",
        "        balance_weights = [0.4, 1.0]\n",
        "        sb_weights = 1.0\n",
        "\n",
        "        if len(balance_weights) == len(score):\n",
        "            return sum([w * self._forward(x, target) for (w, x) in zip(balance_weights, score)])\n",
        "        elif len(score) == 1:\n",
        "            return sb_weights * self._forward(score[0], target)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"lengths of prediction and target are not identical!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIC-vs6us1fn"
      },
      "source": [
        "#### BoundaryLoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBfjIOCoH_Xq"
      },
      "outputs": [],
      "source": [
        "def weighted_bce(bd_pre, target):\n",
        "    n, c, h, w = bd_pre.size()\n",
        "    log_p = bd_pre.permute(0,2,3,1).contiguous().view(1, -1)\n",
        "    target_t = target.view(1, -1)\n",
        "\n",
        "    pos_index = (target_t == 1)\n",
        "    neg_index = (target_t == 0)\n",
        "\n",
        "    weight = torch.zeros_like(log_p)\n",
        "    pos_num = pos_index.sum()\n",
        "    neg_num = neg_index.sum()\n",
        "    sum_num = pos_num + neg_num\n",
        "    weight[pos_index] = neg_num * 1.0 / sum_num\n",
        "    weight[neg_index] = pos_num * 1.0 / sum_num\n",
        "\n",
        "    loss = F.binary_cross_entropy_with_logits(log_p, target_t, weight, reduction='mean')\n",
        "\n",
        "    return loss\n",
        "\n",
        "class BondaryLoss(nn.Module):\n",
        "    def __init__(self, coeff_bce = 20.0):\n",
        "        super(BondaryLoss, self).__init__()\n",
        "        self.coeff_bce = coeff_bce\n",
        "\n",
        "    def forward(self, bd_pre, bd_gt):\n",
        "        bce_loss = self.coeff_bce * weighted_bce(bd_pre, bd_gt)\n",
        "        loss = bce_loss\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHFohgLtsep5"
      },
      "source": [
        "#### FullModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zz72vdhb9cL2"
      },
      "outputs": [],
      "source": [
        "class FullModel(nn.Module):\n",
        "\n",
        "    def __init__(self, model, sem_loss, bd_loss):\n",
        "        super(FullModel, self).__init__()\n",
        "        self.model = model\n",
        "        self.sem_loss = sem_loss\n",
        "        self.bd_loss = bd_loss\n",
        "\n",
        "    def pixel_acc(self, pred, label):\n",
        "        _, preds = torch.max(pred, dim=1)\n",
        "        valid = (label != IGNORE_INDEX).long()\n",
        "        acc_sum = torch.sum(valid * (preds == label).long())\n",
        "        pixel_sum = torch.sum(valid)\n",
        "        acc = acc_sum.float() / (pixel_sum.float() + 1e-10)\n",
        "        return acc\n",
        "\n",
        "    def forward(self, inputs, labels, bd_gt, *args, **kwargs):\n",
        "        outputs = self.model(inputs, *args, **kwargs)\n",
        "\n",
        "        if labels is None:\n",
        "          h, w = inputs.size(2), inputs.size(3)\n",
        "        else:\n",
        "          h, w = labels.size(1), labels.size(2)\n",
        "\n",
        "        ph, pw = outputs[0].size(2), outputs[0].size(3)\n",
        "        if ph != h or pw != w:\n",
        "            for i in range(len(outputs)):\n",
        "                outputs[i] = F.interpolate(outputs[i], size=(\n",
        "                    h, w), mode='bilinear', align_corners=True)     #from original configs\n",
        "\n",
        "        if bd_gt is  None:\n",
        "            return None, outputs, None, None\n",
        "\n",
        "        acc  = self.pixel_acc(outputs[-2], labels)\n",
        "        loss_s = self.sem_loss(outputs[:-1], labels)\n",
        "\n",
        "        loss_b = self.bd_loss(outputs[-1], bd_gt)\n",
        "\n",
        "        filler = torch.ones_like(labels) * IGNORE_INDEX       #from original configs\n",
        "        bd_label = torch.where(F.sigmoid(outputs[-1][:,0,:,:])>0.8, labels, filler)\n",
        "\n",
        "        loss_sb = self.sem_loss([outputs[-2]], bd_label)\n",
        "\n",
        "        loss = loss_s + loss_b + loss_sb\n",
        "\n",
        "\n",
        "        return torch.unsqueeze(loss,0), outputs, acc, [loss_s, loss_b, loss_sb]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waOaWaJzszW_"
      },
      "source": [
        "#### Other functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKdWPZZHs4kU"
      },
      "outputs": [],
      "source": [
        "def get_seg_model(model_name, num_classes, pretrained_weights, imgnet_pretrained):\n",
        "\n",
        "    if 's' in model_name:\n",
        "        model = PIDNet(m=2, n=3, num_classes=num_classes, planes=32, ppm_planes=96, head_planes=128, augment=True)\n",
        "    elif 'm' in model_name:\n",
        "        model = PIDNet(m=2, n=3, num_classes=num_classes, planes=64, ppm_planes=96, head_planes=128, augment=True)\n",
        "    else:\n",
        "        model = PIDNet(m=3, n=4, num_classes=num_classes, planes=64, ppm_planes=112, head_planes=256, augment=True)\n",
        "\n",
        "    if imgnet_pretrained:\n",
        "        pretrained_state = torch.load(pretrained_weights, map_location='cpu')['state_dict']\n",
        "        model_dict = model.state_dict()\n",
        "        pretrained_state = {k: v for k, v in pretrained_state.items() if (k in model_dict and v.shape == model_dict[k].shape)}\n",
        "        model_dict.update(pretrained_state)\n",
        "        msg = 'Loaded {} parameters!'.format(len(pretrained_state))\n",
        "        print('Attention!!!')\n",
        "        print(msg)\n",
        "        print('Over!!!')\n",
        "        model.load_state_dict(model_dict, strict = False)\n",
        "    else:\n",
        "        pretrained_dict = torch.load(pretrained_weights, map_location='cpu')\n",
        "        if 'state_dict' in pretrained_dict:\n",
        "            pretrained_dict = pretrained_dict['state_dict']\n",
        "        model_dict = model.state_dict()\n",
        "        pretrained_dict = {k[6:]: v for k, v in pretrained_dict.items() if (k[6:] in model_dict and v.shape == model_dict[k[6:]].shape)}\n",
        "        msg = 'Loaded {} parameters!'.format(len(pretrained_dict))\n",
        "        print('Attention!!!')\n",
        "        print(msg)\n",
        "        print('Over!!!')\n",
        "        model_dict.update(pretrained_dict)\n",
        "        model.load_state_dict(model_dict, strict = False)\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_pred_model(name, num_classes):\n",
        "\n",
        "    if 's' in name:\n",
        "        model = PIDNet(m=2, n=3, num_classes=num_classes, planes=32, ppm_planes=96, head_planes=128, augment=False)\n",
        "    elif 'm' in name:\n",
        "        model = PIDNet(m=2, n=3, num_classes=num_classes, planes=64, ppm_planes=96, head_planes=128, augment=False)\n",
        "    else:\n",
        "        model = PIDNet(m=3, n=4, num_classes=num_classes, planes=64, ppm_planes=112, head_planes=256, augment=False)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p8kkdlmmd37"
      },
      "source": [
        "## Download pre-trained weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2kM2RnbF44b"
      },
      "outputs": [],
      "source": [
        "weights_dir = Path(PRETRAINED_WEIGHTS_DIR)\n",
        "if not weights_dir.exists():\n",
        "    weights_dir.mkdir(exist_ok=True)\n",
        "\n",
        "PIDNET_S_WEIGHTS = weights_dir / 'pidnet_s_imagenet_pretrained.pth'\n",
        "\n",
        "pidnet_s_weights = Path(PIDNET_S_WEIGHTS)\n",
        "if not pidnet_s_weights.exists():\n",
        "    # Replace with the correct Google Drive file ID\n",
        "\n",
        "    file_id = '1hIBp_8maRr60-B3PF0NVtaA6TYBvO4y-'\n",
        "    gdown.download(id=file_id, output=str(pidnet_s_weights), quiet=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZwedtEfnOOM"
      },
      "source": [
        "# Step 2b: Real-time semantic segmentation network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ofu29vcvCFb"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWfFzjriv59i"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaEZ_xJkv7e8"
      },
      "outputs": [],
      "source": [
        "resize = 512\n",
        "BATCH_SIZE = 6\n",
        "num_epochs = 20\n",
        "\n",
        "LR = 1e-3\n",
        "MOMEUNTUM = 0.9\n",
        "WEIGHT_DECAY = 1e-2\n",
        "STEP_SIZE = 10\n",
        "GAMMA = 0.1\n",
        "\n",
        "log_frequency = 25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCZ8MTPjup_T"
      },
      "source": [
        "### Dataset preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoxLxfHfuwZ9"
      },
      "source": [
        "#### Normalization metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vup_B8CouyYT"
      },
      "outputs": [],
      "source": [
        "num_workers = 2 if device.type == 'cuda' else 0\n",
        "\n",
        "# Poiché il modello è pretrainato su ImageNet, si usano media e varianza di ImageNet\n",
        "avg = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCzdv2TCu10X"
      },
      "source": [
        "#### Training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcRZO3dwucly"
      },
      "outputs": [],
      "source": [
        "train_transform = A.Compose([\n",
        "    A.Normalize(mean=avg, std=std, p=1, always_apply=True, max_pixel_value=255),\n",
        "    A.Resize(resize, resize, p=1, always_apply=True)\n",
        "])\n",
        "train_dataset = LoveDA(TRAIN_DIR, IMG_PATH, MASK_PATH, directories=URBAN_PATH, transforms=train_transform, bd=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=num_workers, worker_init_fn=seed_worker, generator=g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwFbFBRvu4LI"
      },
      "source": [
        "#### Validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_62pcuDKu59K"
      },
      "outputs": [],
      "source": [
        "# La resize è bene farla solo sul training set\n",
        "# La normalizzazione invece può essere applicata anche qui\n",
        "\n",
        "val_transform = A.Compose([\n",
        "    A.Normalize(mean=avg, std=std, p=1, always_apply=True, max_pixel_value=255)\n",
        "])\n",
        "val_dataset = LoveDA(VAL_DIR, IMG_PATH, MASK_PATH, directories=URBAN_PATH, transforms=val_transform, bd=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=num_workers, worker_init_fn=seed_worker, generator=g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpiF-7qPvzUy"
      },
      "source": [
        "### Training process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgQujuEIwUHO"
      },
      "source": [
        "#### Model engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExGpSNtXwaXe",
        "outputId": "c11270cd-688f-4314-9a4a-f961444ebb23"
      },
      "outputs": [],
      "source": [
        "pidnet = get_seg_model(\"pidnet_s\", num_classes, PIDNET_S_WEIGHTS,imgnet_pretrained=True)\n",
        "model = FullModel(pidnet, sem_loss=CrossEntropy(ignore_label=IGNORE_INDEX), bd_loss=BondaryLoss())\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = LR, momentum = MOMEUNTUM, weight_decay = WEIGHT_DECAY)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgdaTcMTxkQI"
      },
      "source": [
        "#### Evaluate function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JG1ocX7Fxmcd"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, dataloader, device, ) -> tuple:\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    data_len = 0\n",
        "    iou_scores = 0.0\n",
        "\n",
        "    for (inputs, masks, boundaries) in dataloader:\n",
        "\n",
        "        data_len += inputs.size(0)\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        masks = masks.to(device)\n",
        "        boundaries = boundaries.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        loss, outputs, acc, loss_list = model(inputs, masks, boundaries)\n",
        "        running_loss += loss.item()*inputs.size(0)\n",
        "\n",
        "        # Calculate mIoU\n",
        "        iou, _ = calculate_iou(outputs[1], masks, num_classes)\n",
        "        iou_scores += iou*inputs.size(0)\n",
        "\n",
        "    mIoU = iou_scores/data_len\n",
        "    loss = running_loss/data_len\n",
        "\n",
        "\n",
        "    return loss, mIoU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9jPEHbfw_uG"
      },
      "source": [
        "#### Training/evaluation loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4NPhxYwxEAD"
      },
      "outputs": [],
      "source": [
        "val_losses, val_accuracies = [], []\n",
        "train_losses, train_accuracies = [], []\n",
        "miou_scores = []\n",
        "best_mIoU = -1\n",
        "best_num_epochs = None\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    current_step = 0\n",
        "    train_loss = 0.0\n",
        "    model.train()\n",
        "    for (inputs, masks, boundaries) in train_loader:\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        boundaries = boundaries.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss, outputs, pixel_acc, [loss_s, loss_b, loss_sb] = model(inputs, masks, boundaries)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if current_step % log_frequency == 0:\n",
        "            print(f\"Epoch {epoch+1}, Iteration {current_step}, Loss: {loss.item():.3f} Loss_s: {loss_s.item():.3f} Loss_b: {loss_b.item():.3f} Loss_sb: {loss_sb.item():.3f}\")\n",
        "\n",
        "        current_step += 1\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    print(f\"End of Epoch {epoch+1}\")\n",
        "    print(f\"Training loss: {train_loss:.5f}\")\n",
        "\n",
        "\n",
        "    val_loss, val_mean_iou = evaluate(model, val_loader, device)\n",
        "    print(f\"Validation mIoU: {val_mean_iou*100:.3f}%, Validation loss: {val_loss:.5f}\")\n",
        "\n",
        "    val_losses.append(val_loss)\n",
        "    miou_scores.append(val_mean_iou)\n",
        "    val_accuracies.append(val_mean_iou.cpu().item())\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    print()\n",
        "    # Scheduler is None if learning rate is constant\n",
        "    if scheduler is not None:\n",
        "        scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj0QJgtO_IiW"
      },
      "source": [
        "### Metric calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cw5AGBi6_IiW"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "mean_latency, _, _, _ = calculate_latency_fps(model, device, 1024, 1024, num_epochs, ModelType.PIDNET)\n",
        "print(f\"Mean Latency: {mean_latency:.2f} ms\")\n",
        "calculate_flops_params(model, device, 1024, 1024, ModelType.PIDNET)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsZrRWKDYLjM"
      },
      "source": [
        "# Step 3a: Evaluating the domain shift problem in Semantic Segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtM5dWX-Yc5K"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LHK7BBwYc5L"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHYrmBZSYc5L"
      },
      "outputs": [],
      "source": [
        "resize = 512\n",
        "BATCH_SIZE = 6\n",
        "num_epochs = 20\n",
        "\n",
        "LR = 1e-3\n",
        "MOMEUNTUM = 0.9\n",
        "WEIGHT_DECAY = 1e-2\n",
        "STEP_SIZE = 10\n",
        "GAMMA = 0.1\n",
        "\n",
        "log_frequency = 25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3myb3foIYc5L"
      },
      "source": [
        "### Dataset preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxLdbQm_Yc5L"
      },
      "source": [
        "#### Normalization metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzyfFAqyYc5L"
      },
      "outputs": [],
      "source": [
        "num_workers = 2 if device.type == 'cuda' else 0\n",
        "\n",
        "# Poiché il modello è pretrainato su ImageNet, si usano media e varianza di ImageNet\n",
        "avg = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QMBfrdVYc5L"
      },
      "source": [
        "#### Training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWjByEsQYc5L"
      },
      "outputs": [],
      "source": [
        "train_transform = A.Compose([\n",
        "    A.Normalize(mean=avg, std=std, p=1, always_apply=True, max_pixel_value=255),\n",
        "    A.Resize(resize, resize, p=1, always_apply=True)\n",
        "])\n",
        "\n",
        "train_dataset = LoveDA(TRAIN_DIR, IMG_PATH, MASK_PATH, directories=URBAN_PATH, bd=True, transforms=train_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=num_workers, worker_init_fn=seed_worker, generator=g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqwfjp-YYc5M"
      },
      "source": [
        "#### Validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bsc_hSQ1Yc5M"
      },
      "outputs": [],
      "source": [
        "val_transform = A.Compose([\n",
        "    A.Normalize(mean=avg, std=std, p=1, always_apply=True, max_pixel_value=255)\n",
        "])\n",
        "\n",
        "val_dataset = LoveDA(VAL_DIR, IMG_PATH, MASK_PATH, directories=RURAL_PATH, bd=True, transforms=val_transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=num_workers, worker_init_fn=seed_worker, generator=g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks8YNvchYc5M"
      },
      "source": [
        "### Training process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htCmAmq0Yc5M"
      },
      "source": [
        "#### Model engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8Jj6eYiYc5M",
        "outputId": "e17b6364-d588-4c9b-9d93-6a2acf9d5ad5"
      },
      "outputs": [],
      "source": [
        "pidnet = get_seg_model(\"pidnet_s\", num_classes, PIDNET_S_WEIGHTS,imgnet_pretrained=True)\n",
        "model = FullModel(pidnet, sem_loss=CrossEntropy(ignore_label=IGNORE_INDEX), bd_loss=BondaryLoss())\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = LR, momentum = MOMEUNTUM, weight_decay = WEIGHT_DECAY)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = STEP_SIZE, gamma = GAMMA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBGrbjXaYc5M"
      },
      "source": [
        "#### Evaluate function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HILKj4bYc5M"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, dataloader, device, ) -> tuple:\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    data_len = 0\n",
        "    iou_scores = 0.0\n",
        "    ious_per_class = torch.zeros(num_classes)\n",
        "\n",
        "    for (inputs, masks, boundaries) in dataloader:\n",
        "\n",
        "        data_len += inputs.size(0)\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        masks = masks.to(device)\n",
        "        boundaries = boundaries.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        loss, outputs, acc, loss_list = model(inputs, masks, boundaries)\n",
        "        running_loss += loss.item()*inputs.size(0)\n",
        "\n",
        "        # Calculate mIoU\n",
        "        iou, iou_per_class = calculate_iou(outputs[1], masks, num_classes)\n",
        "        iou_scores += iou*inputs.size(0)\n",
        "        ious_per_class+=iou_per_class.cpu()*inputs.size(0)\n",
        "\n",
        "    mIoU = iou_scores/data_len\n",
        "    loss = running_loss/data_len\n",
        "    ious_per_class/=data_len\n",
        "\n",
        "\n",
        "    return loss, mIoU, ious_per_class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyZhv1kVYc5N"
      },
      "source": [
        "#### Training/evaluation loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziNXwVeiYc5N",
        "outputId": "dc1e6bda-2fdb-47ac-d2ae-4cc100431df4"
      },
      "outputs": [],
      "source": [
        "val_losses, val_accuracies = [], []\n",
        "train_losses, train_accuracies = [], []\n",
        "miou_scores = []\n",
        "best_mIoU = -1\n",
        "best_num_epochs = None\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    current_step = 0\n",
        "    train_loss = 0.0\n",
        "    model.train()\n",
        "    for (inputs, masks, boundaries) in train_loader:\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        boundaries = boundaries.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss, outputs, pixel_acc, [loss_s, loss_b, loss_sb] = model(inputs, masks, boundaries)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if current_step % log_frequency == 0:\n",
        "            print(f\"Epoch {epoch+1}, Iteration {current_step}, Loss: {loss.item():.3f} Loss_s: {loss_s.item():.3f} Loss_b: {loss_b.item():.3f} Loss_sb: {loss_sb.item():.3f}\")\n",
        "\n",
        "        current_step += 1\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    print(f\"End of Epoch {epoch+1}\")\n",
        "    print(f\"Training loss: {train_loss:.5f}\")\n",
        "\n",
        "\n",
        "    val_loss, val_mean_iou, ious_per_class = evaluate(model, val_loader, device)\n",
        "\n",
        "    print(f\"Validation mIoU: {val_mean_iou*100:.3f}%, Validation loss: {val_loss:.5f}\")\n",
        "\n",
        "    for i, cat in enumerate(categories.keys()):\n",
        "        print(f\"{cat} mIoU: {ious_per_class[i]*100:.3f}\")\n",
        "\n",
        "    val_losses.append(val_loss)\n",
        "    train_losses.append(train_loss)\n",
        "    miou_scores.append(val_mean_iou)\n",
        "\n",
        "    print()\n",
        "    # Scheduler is None if learning rate is constant\n",
        "    if scheduler is not None:\n",
        "        scheduler.step()\n",
        "\n",
        "miou_scores = list(map(lambda x: x.item(), miou_scores))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cx9j7hzvL_M0"
      },
      "source": [
        "### Evaluate using saved models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        },
        "id": "Ykk41_c7-xrp",
        "outputId": "e33346c0-fc4c-45d4-fb3c-252f6c15946a"
      },
      "outputs": [],
      "source": [
        "validation_losses = []\n",
        "miou_scores = []\n",
        "\n",
        "starting_epoch = 0\n",
        "\n",
        "for epoch in range(starting_epoch, num_epochs):\n",
        "    model_path = f\"/content/drive/MyDrive/loveDA_dataset/Model training/PIDNet_{num_epochs}_{LR}_{STEP_SIZE}_{GAMMA}_{resize}_{WEIGHT_DECAY}_{MOMEUNTUM}_{GAMMA}_epoch{epoch}.pth\"\n",
        "\n",
        "    pidnet = get_seg_model(\"pidnet_s\", num_classes, PIDNET_S_WEIGHTS,imgnet_pretrained=True)\n",
        "    model = FullModel(pidnet, sem_loss=CrossEntropy(ignore_label=IGNORE_INDEX), bd_loss=BondaryLoss())\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    val_loss, val_mean_iou, ious_per_class = evaluate(model, val_loader, device)\n",
        "\n",
        "    validation_losses.append(val_loss)\n",
        "    miou_scores.append(val_mean_iou)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}\")\n",
        "    print(f\"Validation mIoU: {val_mean_iou*100:.3f}%, Validation loss: {val_loss:.5f}\")\n",
        "\n",
        "    for i, cat in enumerate(categories.keys()):\n",
        "        print(f\"{cat} mIoU: {ious_per_class[i]*100:.3f}\")\n",
        "\n",
        "    print()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOz--WLUg2K4"
      },
      "source": [
        "# Step 3b: Data augmentations to reduce the domain shift"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UvUsl_sg2K4"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KdKOjqdg2K4"
      },
      "outputs": [],
      "source": [
        "resize = 512\n",
        "BATCH_SIZE = 6\n",
        "num_epochs = 20\n",
        "\n",
        "LR = 1e-3\n",
        "MOMEUNTUM = 0.9\n",
        "WEIGHT_DECAY = 1e-2\n",
        "STEP_SIZE = 10\n",
        "GAMMA = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS8KiIRmg2K4"
      },
      "source": [
        "### Dataset preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOF0Fu6vg2K4"
      },
      "source": [
        "#### Normalization metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKLXZraAg2K4"
      },
      "outputs": [],
      "source": [
        "num_workers = 2 if device.type == 'cuda' else 0\n",
        "\n",
        "# Poiché il modello è pretrainato su ImageNet, si usano media e varianza di ImageNet\n",
        "avg = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyigoPYag2K4"
      },
      "source": [
        "#### Augmentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5YCifjTg2K5"
      },
      "outputs": [],
      "source": [
        "aug_prob = 0.5\n",
        "\n",
        "augmentations = [\n",
        "    A.ShiftScaleRotate(p=1),\n",
        "    A.GridDistortion(p=1),\n",
        "    A.RandomCrop(height=resize, width=resize, p=1),\n",
        "    A.HorizontalFlip(p=1),\n",
        "    A.GaussianBlur(p=1),\n",
        "    A.GridDropout(p=1),\n",
        "    A.ColorJitter(p=1),\n",
        "    A.GaussNoise(var_limit=(0.2, 0.3), p=1),\n",
        "    A.ChannelDropout(p=1),\n",
        "    A.RandomSizedCrop(min_max_height=(resize//8, resize), height=resize, width=resize, p=1),\n",
        "]\n",
        "\n",
        "selected_indices = [2]\n",
        "\n",
        "selected_augmentations = A.Compose([augmentations[i] for i in selected_indices], p=aug_prob)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LY9ONa1Ig2K5"
      },
      "source": [
        "#### Training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARH3hXgpg2K5"
      },
      "outputs": [],
      "source": [
        "train_transform = A.Compose([\n",
        "    A.Normalize(mean=avg, std=std, p=1, always_apply=True, max_pixel_value=255),\n",
        "    selected_augmentations,\n",
        "    A.Resize(resize, resize, p=1, always_apply=True)\n",
        "])\n",
        "\n",
        "train_dataset = LoveDA(TRAIN_DIR, IMG_PATH, MASK_PATH, directories=URBAN_PATH, bd=True, transforms=train_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=num_workers, worker_init_fn=seed_worker, generator=g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXT2zPn2g2K5"
      },
      "source": [
        "#### Validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87g4SX0_g2K5"
      },
      "outputs": [],
      "source": [
        "val_transform = A.Compose([\n",
        "    A.Normalize(mean=avg, std=std, p=1, always_apply=True, max_pixel_value=255)\n",
        "])\n",
        "\n",
        "val_dataset = LoveDA(VAL_DIR, IMG_PATH, MASK_PATH, directories=RURAL_PATH, bd=True, transforms=val_transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=num_workers, worker_init_fn=seed_worker, generator=g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pYGh7_Wg2K5"
      },
      "source": [
        "### Training process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz14zhkQg2K5"
      },
      "source": [
        "#### Model engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZD5mOHBg2K5",
        "outputId": "a49a8553-e2f7-4de6-d02a-cec361836181"
      },
      "outputs": [],
      "source": [
        "pidnet = get_seg_model(\"pidnet_s\", num_classes, PIDNET_S_WEIGHTS,imgnet_pretrained=True)\n",
        "model = FullModel(pidnet, sem_loss=CrossEntropy(ignore_label=IGNORE_INDEX), bd_loss=BondaryLoss())\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = LR, momentum = MOMEUNTUM, weight_decay = WEIGHT_DECAY)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = STEP_SIZE, gamma = GAMMA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcaSQTY9g2K5"
      },
      "source": [
        "#### Evaluate function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTBuw2H1g2K5"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, dataloader, device, ) -> tuple:\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    data_len = 0\n",
        "    iou_scores = 0.0\n",
        "    ious_per_class = torch.zeros(num_classes)\n",
        "\n",
        "    for (inputs, masks, boundaries) in dataloader:\n",
        "\n",
        "        data_len += inputs.size(0)\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        masks = masks.to(device)\n",
        "        boundaries = boundaries.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        loss, outputs, acc, loss_list = model(inputs, masks, boundaries)\n",
        "        running_loss += loss.item()*inputs.size(0)\n",
        "\n",
        "        # Calculate mIoU\n",
        "        iou, iou_per_class = calculate_iou(outputs[1], masks, num_classes)\n",
        "        iou_scores += iou*inputs.size(0)\n",
        "        ious_per_class+=iou_per_class.cpu()*inputs.size(0)\n",
        "\n",
        "    mIoU = iou_scores/data_len\n",
        "    loss = running_loss/data_len\n",
        "    ious_per_class/=data_len\n",
        "\n",
        "\n",
        "    return loss, mIoU, ious_per_class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sayvzvTNg2K5"
      },
      "source": [
        "#### Training/evaluation loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8H5Ptzeg2K5",
        "outputId": "8900677c-cad6-49e3-ce25-6084aa0ad708"
      },
      "outputs": [],
      "source": [
        "val_losses, val_accuracies = [], []\n",
        "train_losses, train_accuracies = [], []\n",
        "miou_scores = []\n",
        "miou_per_category = dict()\n",
        "best_mIoU = -1\n",
        "best_num_epochs = None\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    current_step = 0\n",
        "    train_loss = 0.0\n",
        "    model.train()\n",
        "    for (inputs, masks, boundaries) in train_loader:\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        boundaries = boundaries.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss, outputs, pixel_acc, [loss_s, loss_b, loss_sb] = model(inputs, masks, boundaries)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if current_step % log_frequency == 0:\n",
        "            print(f\"Epoch {epoch+1}, Iteration {current_step}, Loss: {loss.item():.3f} Loss_s: {loss_s.item():.3f} Loss_b: {loss_b.item():.3f} Loss_sb: {loss_sb.item():.3f}\")\n",
        "\n",
        "        current_step += 1\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    print(f\"End of Epoch {epoch+1}\")\n",
        "    print(f\"Training loss: {train_loss:.5f}\")\n",
        "\n",
        "\n",
        "    val_loss, val_mean_iou, ious_per_class = evaluate(model, val_loader, device)\n",
        "\n",
        "\n",
        "    print(f\"Validation mIoU: {val_mean_iou*100:.3f}%, Validation loss: {val_loss:.5f}\")\n",
        "\n",
        "    for i, cat in enumerate(categories.keys()):\n",
        "        if cat in miou_per_category:\n",
        "            miou_per_category[cat] += [ious_per_class[i].item()]\n",
        "        else:\n",
        "            miou_per_category[cat] = [ious_per_class[i].item()]\n",
        "        print(f\"{cat} mIoU: {ious_per_class[i]*100:.3f}%\")\n",
        "\n",
        "    val_losses.append(val_loss)\n",
        "    train_losses.append(train_loss)\n",
        "    miou_scores.append(val_mean_iou)\n",
        "\n",
        "\n",
        "    print()\n",
        "    # Scheduler is None if learning rate is constant\n",
        "    if scheduler is not None:\n",
        "        scheduler.step()\n",
        "\n",
        "miou_scores = list(map(lambda x: x.item(), miou_scores))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8iauVEpE4L_"
      },
      "source": [
        "# Step 4a: Adversarial Domain Adaptation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdtc4sem8CWj"
      },
      "source": [
        "### FC discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LunWao_K8D9K"
      },
      "outputs": [],
      "source": [
        "class FCDiscriminator(nn.Module):\n",
        "\n",
        "\tdef __init__(self, num_classes, ndf = 64):\n",
        "\t\tsuper(FCDiscriminator, self).__init__()\n",
        "\n",
        "\t\tself.conv1 = nn.Conv2d(num_classes, ndf, kernel_size=4, stride=2, padding=1)\n",
        "\t\tself.conv2 = nn.Conv2d(ndf, ndf*2, kernel_size=4, stride=2, padding=1)\n",
        "\t\tself.conv3 = nn.Conv2d(ndf*2, ndf*4, kernel_size=4, stride=2, padding=1)\n",
        "\t\tself.conv4 = nn.Conv2d(ndf*4, ndf*8, kernel_size=4, stride=2, padding=1)\n",
        "\t\tself.classifier = nn.Conv2d(ndf*8, 1, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "\t\tself.leaky_relu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "\t\tself.up_sample = nn.Upsample(scale_factor=32, mode='bilinear')\n",
        "\t\tself.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tx = self.conv1(x)\n",
        "\t\tx = self.leaky_relu(x)\n",
        "\t\tx = self.conv2(x)\n",
        "\t\tx = self.leaky_relu(x)\n",
        "\t\tx = self.conv3(x)\n",
        "\t\tx = self.leaky_relu(x)\n",
        "\t\tx = self.conv4(x)\n",
        "\t\tx = self.leaky_relu(x)\n",
        "\t\tx = self.classifier(x)\n",
        "\t\t#x = self.up_sample(x)\n",
        "\t\t#x = self.sigmoid(x)\n",
        "\n",
        "\t\treturn x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpuJzsnTJLGs"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RUVAzcqGn8k"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGe5tURwGn8k"
      },
      "outputs": [],
      "source": [
        "resize = 512\n",
        "BATCH_SIZE = 6\n",
        "num_epochs = 20\n",
        "\n",
        "LR = 1e-3\n",
        "MOMEUNTUM = 0.9\n",
        "WEIGHT_DECAY = 1e-2\n",
        "STEP_SIZE = 10\n",
        "GAMMA = 0.1\n",
        "\n",
        "LAMBDA = 1e-3\n",
        "\n",
        "log_frequency = 50\n",
        "\n",
        "LR_D = 1e-5\n",
        "WEIGHT_DECAY_D = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6LfLAKqGn8k"
      },
      "source": [
        "### Dataset preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkJg7vN6Gn8k"
      },
      "source": [
        "#### Normalization metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WotCMM15Gn8l"
      },
      "outputs": [],
      "source": [
        "num_workers = 2 if device.type == 'cuda' else 0\n",
        "\n",
        "# Poiché il modello è pretrainato su ImageNet, si usano media e varianza di ImageNet\n",
        "avg = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXngZ-RUGn8l"
      },
      "source": [
        "#### Training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMnHJZszGn8l"
      },
      "outputs": [],
      "source": [
        "train_transform = A.Compose([\n",
        "    A.Normalize(mean=avg, std=std, p=1, always_apply=True, max_pixel_value=255),\n",
        "    A.RandomCrop(height=resize, width=resize, p=0.5),\n",
        "    A.Resize(resize, resize, p=1, always_apply=True)\n",
        "])\n",
        "\n",
        "train_dataset = LoveDA(TRAIN_DIR, IMG_PATH, MASK_PATH, directories=URBAN_PATH, bd=True, transforms=train_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=num_workers, worker_init_fn=seed_worker, generator=g)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQD_HWhvU1kA"
      },
      "outputs": [],
      "source": [
        "train_transform_target = A.Compose([\n",
        "    A.Normalize(mean=avg, std=std, p=1, always_apply=True, max_pixel_value=255),\n",
        "    #A.RandomCrop(height=resize, width=resize, p=0.5),\n",
        "    A.Resize(resize, resize, p=1, always_apply=True)\n",
        "])\n",
        "\n",
        "train_dataset_target = LoveDA(TRAIN_DIR, IMG_PATH, MASK_PATH, directories=RURAL_PATH, bd=True, transforms=train_transform_target)\n",
        "train_loader_target = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=num_workers, worker_init_fn=seed_worker, generator=g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUIuMk6YGn8l"
      },
      "source": [
        "#### Validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLnJqJN5Gn8l"
      },
      "outputs": [],
      "source": [
        "val_transform = A.Compose([\n",
        "    A.Normalize(mean=avg, std=std, p=1, always_apply=True, max_pixel_value=255)\n",
        "])\n",
        "\n",
        "val_dataset = LoveDA(VAL_DIR, IMG_PATH, MASK_PATH, directories=RURAL_PATH, bd=True, transforms=val_transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=num_workers, worker_init_fn=seed_worker, generator=g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQN-FpydGn8l"
      },
      "source": [
        "### Training process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQNdlT0xGn8l"
      },
      "source": [
        "#### Model engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrbpPpt0Gn8l",
        "outputId": "af7302de-0139-4506-f4ac-44221ac13c28"
      },
      "outputs": [],
      "source": [
        "pidnet = get_seg_model(\"pidnet_s\", num_classes, PIDNET_S_WEIGHTS,imgnet_pretrained=True)\n",
        "model = FullModel(pidnet, sem_loss=CrossEntropy(ignore_label=IGNORE_INDEX), bd_loss=BondaryLoss())\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = LR, momentum = MOMEUNTUM, weight_decay = WEIGHT_DECAY)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = STEP_SIZE, gamma = GAMMA)\n",
        "\n",
        "domain_criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model_domain = FCDiscriminator(num_classes=7)\n",
        "model_domain = model_domain.to(device)\n",
        "domain_optimizer = torch.optim.Adam(model_domain.parameters(), lr = LR_D, weight_decay = WEIGHT_DECAY_D)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSsNi2hXGn8m"
      },
      "source": [
        "#### Evaluate function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Wg2CMaCGn8m"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, dataloader, device, ) -> tuple:\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    data_len = 0\n",
        "    iou_scores = 0.0\n",
        "    ious_per_class = torch.zeros(num_classes)\n",
        "\n",
        "    for (inputs, masks, boundaries) in dataloader:\n",
        "\n",
        "        data_len += inputs.size(0)\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        masks = masks.to(device)\n",
        "        boundaries = boundaries.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        loss, outputs, _, _ = model(inputs, masks, boundaries)\n",
        "        running_loss += loss.item()*inputs.size(0)\n",
        "\n",
        "        # Calculate mIoU\n",
        "        iou, iou_per_class = calculate_iou(outputs[1], masks, num_classes)\n",
        "        iou_scores += iou*inputs.size(0)\n",
        "        ious_per_class+=iou_per_class.cpu()*inputs.size(0)\n",
        "\n",
        "    mIoU = iou_scores/data_len\n",
        "    loss = running_loss/data_len\n",
        "    ious_per_class/=data_len\n",
        "\n",
        "\n",
        "    return loss, mIoU, ious_per_class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQNZDlwQMdI0"
      },
      "source": [
        "#### Training/evaluation loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WdtMH-UgSESx",
        "outputId": "02173f8c-5e46-42d4-91a1-1f8280647a25"
      },
      "outputs": [],
      "source": [
        "val_losses, val_accuracies = [], []\n",
        "train_losses, train_accuracies = [], []\n",
        "miou_scores = []\n",
        "best_mIoU = -1\n",
        "best_num_epochs = None\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    current_step = 0\n",
        "    running_source_loss_seg = 0.0\n",
        "\n",
        "    loss_G, loss_D = 0, 0\n",
        "\n",
        "    model.train()\n",
        "    model_domain.train()\n",
        "\n",
        "\n",
        "    for (inputs, masks, boundaries), (target_inputs, _, _) in zip(train_loader, train_loader_target):\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        masks = masks.to(device)\n",
        "        boundaries = boundaries.to(device)\n",
        "        target_inputs = target_inputs.to(device)\n",
        "\n",
        "        # Train G\n",
        "        for param in model_domain.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        domain_optimizer.zero_grad()\n",
        "\n",
        "        ## train with source\n",
        "        source_loss, [_, source_PIDNET_output, _], _, _ = model(inputs, masks, boundaries)\n",
        "        source_loss.backward()\n",
        "        running_source_loss_seg += source_loss.item()\n",
        "\n",
        "        ## train with target\n",
        "        _, [_, target_PIDNET_output, _], _, _ = model(target_inputs, None, None)\n",
        "        preds = F.softmax(target_PIDNET_output, dim=1)\n",
        "        D_out = model_domain(preds)\n",
        "\n",
        "        domain_loss = LAMBDA * domain_criterion(D_out, torch.zeros_like(D_out))\n",
        "        domain_loss.backward()\n",
        "        loss_G += domain_loss.item()\n",
        "\n",
        "        # Train D\n",
        "\n",
        "        for param in model_domain.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        ## train with source\n",
        "        source_PIDNET_output = source_PIDNET_output.detach()\n",
        "        preds = F.softmax(source_PIDNET_output, dim=1)\n",
        "        D_out = model_domain(preds)\n",
        "\n",
        "        domain_loss = domain_criterion(D_out, torch.zeros_like(D_out))\n",
        "        domain_loss = domain_loss / 2\n",
        "        domain_loss.backward()\n",
        "        loss_D += domain_loss.item()\n",
        "\n",
        "        ## train with target\n",
        "        target_PIDNET_output = target_PIDNET_output.detach()\n",
        "        preds = F.softmax(target_PIDNET_output, dim=1)\n",
        "        D_out = model_domain(preds)\n",
        "\n",
        "        domain_loss = domain_criterion(D_out, torch.ones_like(D_out))\n",
        "        domain_loss = domain_loss / 2\n",
        "        domain_loss.backward()\n",
        "        loss_D += domain_loss.item()\n",
        "\n",
        "        clip_grad.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), max_norm=35, norm_type=2)\n",
        "        clip_grad.clip_grad_norm_(filter(lambda p: p.requires_grad, model_domain.parameters()), max_norm=35, norm_type=2)\n",
        "        optimizer.step()\n",
        "        domain_optimizer.step()\n",
        "\n",
        "\n",
        "        if current_step % log_frequency == 0:\n",
        "            print(f\"Epoch {epoch+1}, Iteration {current_step}, Source loss: {running_source_loss_seg/(current_step+1):5f}, Domain loss: {loss_G/(current_step+1):.5f} ({loss_D/(current_step+1):.5f}\")\n",
        "        current_step += 1\n",
        "\n",
        "    train_loss = running_source_loss_seg/len(train_loader)\n",
        "    train_domain_loss_G = loss_G/len(train_loader)\n",
        "    train_domain_loss_D = loss_D/len(train_loader)\n",
        "\n",
        "    print(f\"End of Epoch {epoch+1}\")\n",
        "    print(f\"Training loss: {train_loss:.5f}\")\n",
        "    print(f\"Domain loss G: {train_domain_loss_G:.5f}\")\n",
        "    print(f\"Domain loss D: {train_domain_loss_D:.5f}\")\n",
        "\n",
        "    val_loss, val_mean_iou, ious_per_class = evaluate(model, val_loader, device)\n",
        "\n",
        "    path=f\"/content/drive/MyDrive/loveDA_dataset/Model training/PIDNet/PIDNet_{num_epochs}_{LR}_{STEP_SIZE}_{GAMMA}_{resize}_{WEIGHT_DECAY}_{MOMEUNTUM}_{GAMMA}_epoch{epoch}_DA.pth\"\n",
        "    torch.save(model.state_dict(), path)\n",
        "    print(f\"Validation mIoU: {val_mean_iou*100:.3f}%, Validation loss: {val_loss:.5f}\")\n",
        "\n",
        "    for i, cat in enumerate(categories.keys()):\n",
        "        print(f\"{cat} mIoU: {ious_per_class[i]*100:.3f}\")\n",
        "\n",
        "    val_losses.append(val_loss)\n",
        "    train_losses.append(train_loss)\n",
        "    miou_scores.append(val_mean_iou)\n",
        "\n",
        "    print()\n",
        "    # Scheduler is None if learning rate is constant\n",
        "    if scheduler is not None:\n",
        "        scheduler.step()\n",
        "\n",
        "miou_scores = list(map(lambda x: x.item()*100, miou_scores))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnGafopawTBl"
      },
      "source": [
        "# Step 4b: Image-to-Image Domain Adaptation\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw2FHVfqwVQ2"
      },
      "source": [
        "## Mix & EMA model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jgunDqvwYVb"
      },
      "outputs": [],
      "source": [
        "def oneMix(mask, data = None, target = None):\n",
        "    #Mix\n",
        "    if not (data is None):\n",
        "        stackedMask0, _ = torch.broadcast_tensors(mask[0], data[0])\n",
        "        data = (stackedMask0*data[0]+(1-stackedMask0)*data[1]).unsqueeze(0)\n",
        "    if not (target is None):\n",
        "        stackedMask0, _ = torch.broadcast_tensors(mask[0], target[0])\n",
        "        target = (stackedMask0*target[0]+(1-stackedMask0)*target[1]).unsqueeze(0)\n",
        "    return data, target\n",
        "\n",
        "\n",
        "def generate_class_mask(pred, classes):\n",
        "    pred, classes = torch.broadcast_tensors(pred.unsqueeze(0), classes.unsqueeze(1).unsqueeze(2))\n",
        "    N = pred.eq(classes).sum(0)\n",
        "    return N\n",
        "\n",
        "\n",
        "def mix(parameters, data=None, target=None):\n",
        "    assert ((data is not None) or (target is not None))\n",
        "    data, target = oneMix(mask = parameters[\"Mix\"], data = data, target = target)\n",
        "    return data, target\n",
        "\n",
        "def update_ema_variables(ema_model, model, alpha_teacher, iteration):\n",
        "    # Use the \"true\" average until the exponential average is more correct\n",
        "    alpha_teacher = min(1 - 1 / (iteration + 1), alpha_teacher)\n",
        "\n",
        "    for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n",
        "        #ema_param.data.mul_(alpha).add_(1 - alpha, param.data)\n",
        "        ema_param.data[:] = alpha_teacher * ema_param[:].data[:] + (1 - alpha_teacher) * param[:].data[:]\n",
        "    return ema_model\n",
        "\n",
        "def create_ema_model(model):\n",
        "    pidnet = get_seg_model(\"pidnet_s\", num_classes, PIDNET_S_WEIGHTS,imgnet_pretrained=True)\n",
        "    ema_model = FullModel(pidnet, sem_loss=CrossEntropy(ignore_label=IGNORE_INDEX), bd_loss=BondaryLoss())\n",
        "    for param in ema_model.parameters():\n",
        "        param.detach_()\n",
        "    mp = list(model.parameters())\n",
        "    mcp = list(ema_model.parameters())\n",
        "    n = len(mp)\n",
        "    for i in range(0, n):\n",
        "        mcp[i].data[:] = mp[i].data[:].clone()\n",
        "    return ema_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f0_nsCA3GTM"
      },
      "source": [
        "## Unlabeled loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7scEnVbDhhjG"
      },
      "outputs": [],
      "source": [
        "def calc_U_loss(outputs):\n",
        "  loss_s = sem_loss(outputs[:-1], targets_u)\n",
        "\n",
        "  bd_gt = np.zeros_like(targets_u.cpu().numpy(), dtype=np.float32)\n",
        "  for i, m in enumerate(targets_u):\n",
        "    bd_gt[i] = generate_bd(m.cpu().numpy().astype(np.uint8))\n",
        "\n",
        "  bd_gt = torch.from_numpy(bd_gt).to(device)\n",
        "\n",
        "  loss_b = bd_loss(outputs[-1], bd_gt)\n",
        "\n",
        "  filler = torch.ones_like(targets_u) * IGNORE_INDEX       #from original configs\n",
        "  bd_label = torch.where(F.sigmoid(outputs[-1][:,0,:,:])>0.8, targets_u, filler)\n",
        "\n",
        "  loss_sb = sem_loss([outputs[-2]], bd_label)\n",
        "\n",
        "  return loss_s + loss_b + loss_sb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jwas5Vutfq4I"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtH0vOD5frEy"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gr1UCvANfrEz"
      },
      "outputs": [],
      "source": [
        "LR = 1e-3\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 1e-2\n",
        "num_epochs = 20\n",
        "STEP_SIZE = 10\n",
        "GAMMA = 0.1\n",
        "\n",
        "BATCH_SIZE = 6\n",
        "resize = 512\n",
        "pixel_weight = \"threshold_uniform\"\n",
        "#pixel_weight = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsikOIN6frEz"
      },
      "source": [
        "### Dataset preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B33DP3uKfrEz"
      },
      "source": [
        "#### Normalization metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbrnZErJfrEz"
      },
      "outputs": [],
      "source": [
        "num_workers = 2 if device.type == 'cuda' else 0\n",
        "\n",
        "# Poiché il modello è pretrainato su ImageNet, si usano media e varianza di ImageNet\n",
        "avg = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJrdKqhmfrEz"
      },
      "source": [
        "#### Training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0dTV1T8frEz"
      },
      "outputs": [],
      "source": [
        "train_transform = A.Compose([\n",
        "    A.Normalize(mean=avg, std=std, p=1, always_apply=True, max_pixel_value=255),\n",
        "    A.RandomCrop(height=resize, width=resize, p=0.5),\n",
        "    A.Resize(resize, resize, p=1, always_apply=True)\n",
        "])\n",
        "\n",
        "train_transform_target = A.Compose([\n",
        "    A.Normalize(mean=avg, std=std, p=1, always_apply=True, max_pixel_value=255),\n",
        "    A.RandomCrop(height=resize, width=resize, p=0.5),\n",
        "    A.Resize(resize, resize, p=1, always_apply=True)\n",
        "])\n",
        "\n",
        "train_dataset = LoveDA(TRAIN_DIR, IMG_PATH, MASK_PATH, directories=URBAN_PATH, bd=True, transforms=train_transform)\n",
        "source_trainloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=num_workers, worker_init_fn=seed_worker, generator=g)\n",
        "train_dataset_target = LoveDA(TRAIN_DIR, IMG_PATH, MASK_PATH, directories=RURAL_PATH, bd=True, transforms=train_transform_target)\n",
        "target_trainloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=num_workers, worker_init_fn=seed_worker, generator=g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCw4hGa4frEz"
      },
      "source": [
        "#### Validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-co18kV2frEz"
      },
      "outputs": [],
      "source": [
        "# La resize è bene farla solo sul training set\n",
        "# La normalizzazione invece può essere applicata anche qui\n",
        "\n",
        "val_transform = A.Compose([\n",
        "    A.Normalize(mean=avg, std=std, p=1, always_apply=True, max_pixel_value=255)\n",
        "])\n",
        "\n",
        "val_dataset = LoveDA(VAL_DIR, IMG_PATH, MASK_PATH, directories=RURAL_PATH, bd=True, transforms=val_transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=num_workers, worker_init_fn=seed_worker, generator=g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPAOdqP6g9FR"
      },
      "source": [
        "### Training process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2Mq7J4HMoy5"
      },
      "source": [
        "#### Model engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGWsHfnDhC8i",
        "outputId": "5644d0b1-b006-4d33-e392-ca63e1115c98"
      },
      "outputs": [],
      "source": [
        "pidnet = get_seg_model(\"pidnet_s\", num_classes, PIDNET_S_WEIGHTS,imgnet_pretrained=True)\n",
        "model = FullModel(pidnet, sem_loss=CrossEntropy(ignore_label=IGNORE_INDEX), bd_loss=BondaryLoss())\n",
        "model.to(device)\n",
        "\n",
        "ema_model = create_ema_model(model)\n",
        "ema_model = ema_model.to(device)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = STEP_SIZE, gamma = GAMMA)\n",
        "\n",
        "sem_loss=CrossEntropy(ignore_label=IGNORE_INDEX)\n",
        "bd_loss=BondaryLoss()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqGgoNC8jHm4"
      },
      "source": [
        "#### Evaluate function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNgm0_9-jHDd"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, dataloader, device, ) -> tuple:\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    data_len = 0\n",
        "    iou_scores = 0.0\n",
        "    ious_per_class = torch.zeros(num_classes)\n",
        "\n",
        "    for (inputs, masks, boundaries) in dataloader:\n",
        "\n",
        "        data_len += inputs.size(0)\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        masks = masks.to(device)\n",
        "        boundaries = boundaries.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        loss, outputs, _, _ = model(inputs, masks, boundaries)\n",
        "        running_loss += loss.item()*inputs.size(0)\n",
        "\n",
        "        # Calculate mIoU\n",
        "        iou, iou_per_class = calculate_iou(outputs[1], masks, num_classes)\n",
        "        iou_scores += iou*inputs.size(0)\n",
        "        ious_per_class+=iou_per_class.cpu()*inputs.size(0)\n",
        "\n",
        "    mIoU = iou_scores/data_len\n",
        "    loss = running_loss/data_len\n",
        "    ious_per_class/=data_len\n",
        "\n",
        "\n",
        "    return loss, mIoU, ious_per_class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20LhVR64gtNb"
      },
      "source": [
        "#### Training/evaluation loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3jgWAuIcL_6",
        "outputId": "b4062fa5-640a-473b-d762-7bb5661c9559"
      },
      "outputs": [],
      "source": [
        "ema_model.train()\n",
        "\n",
        "\n",
        "accumulated_loss_l = []\n",
        "accumulated_loss_u = []\n",
        "\n",
        "miou_scores = []\n",
        "training_losses = []\n",
        "validation_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    loss_u_value = 0\n",
        "    loss_l_value = 0\n",
        "\n",
        "    n = 0\n",
        "    for (src_images, src_labels, src_bd), (tgt_images, _, _) in zip(source_trainloader, target_trainloader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        src_images = src_images.to(device)\n",
        "        src_labels = src_labels.to(device)\n",
        "        tgt_images = tgt_images.to(device)\n",
        "        src_bd = src_bd.to(device)\n",
        "\n",
        "\n",
        "        L_l, [_, pred, _], _, _ = model(src_images, src_labels, src_bd)\n",
        "\n",
        "        # _, [_, logits_u_w, _], _, _ = ema_model(tgt_images, None, None)\n",
        "        _, [_, logits_u_w, _], _, _ = model(tgt_images, None, None)\n",
        "\n",
        "\n",
        "        pseudo_label = torch.softmax(logits_u_w.detach(), dim=1)\n",
        "        max_probs, targets_u_w = torch.max(pseudo_label, dim=1)\n",
        "\n",
        "        inputs_u_s = []\n",
        "        targets_u = []\n",
        "        pixel_weights = []\n",
        "\n",
        "\n",
        "        for i in range(len(src_images)):\n",
        "            classes = torch.unique(src_labels[i])\n",
        "            nclasses = classes.shape[0]\n",
        "            classes = (classes[torch.Tensor(np.random.choice(nclasses, int((nclasses+nclasses%2)/2),replace=False)).long()]).to(device)\n",
        "            MixMask_i = generate_class_mask(src_labels[i], classes).unsqueeze(0).to(device)\n",
        "\n",
        "            strong_parameters = {\"Mix\": MixMask_i}\n",
        "\n",
        "            inputs_u_si, _ = mix(strong_parameters, data = torch.cat((src_images[i].unsqueeze(0),tgt_images[i].unsqueeze(0))))\n",
        "            inputs_u_s.append(inputs_u_si)\n",
        "\n",
        "            _, targets_ui = mix(strong_parameters, target = torch.cat((src_labels[i].unsqueeze(0),targets_u_w[i].unsqueeze(0))))\n",
        "            targets_u.append(targets_ui)\n",
        "\n",
        "        inputs_u_s = torch.cat(inputs_u_s)\n",
        "        _, outputs, _, _ = model(inputs_u_s, None, None)\n",
        "        logits_u_s = outputs[1]\n",
        "\n",
        "        targets_u = torch.cat(targets_u).long().to(device)\n",
        "\n",
        "\n",
        "        if pixel_weight == \"threshold_uniform\":\n",
        "            unlabeled_weight = torch.sum(max_probs.ge(0.968).long() == 1).item() / np.size(np.array(targets_u.cpu()))\n",
        "            pixelWiseWeight = unlabeled_weight * torch.ones(max_probs.shape).to(device)\n",
        "        elif pixel_weight == \"threshold\":\n",
        "            pixelWiseWeight = max_probs.ge(0.968).float().to(device)\n",
        "        elif pixel_weight == False:\n",
        "            pixelWiseWeight = torch.ones(max_probs.shape).to(device)\n",
        "\n",
        "\n",
        "        onesWeights = torch.ones((pixelWiseWeight.shape)).to(device)\n",
        "        for i in range(len(src_images)):\n",
        "            _, pixelWiseWeight_i = mix(strong_parameters, target = torch.cat((onesWeights[0].unsqueeze(0),pixelWiseWeight[0].unsqueeze(0))))\n",
        "            pixel_weights.append(pixelWiseWeight_i)\n",
        "\n",
        "\n",
        "        pixel_weights = torch.cat(pixel_weights).to(device)\n",
        "\n",
        "\n",
        "        L_u = calc_U_loss(outputs)\n",
        "        L_u *= torch.mean(pixel_weights)\n",
        "\n",
        "        loss = L_l + L_u\n",
        "\n",
        "        loss_l_value += L_l.item()\n",
        "        loss_u_value += L_u.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        if n %25 == 0:\n",
        "          print('\\tProcessed {0:d} batches, loss_l = {1:.3f}, loss_u = {2:.3f} loss = {3:.3f}'.format(n, loss_l_value/(n+1), loss_u_value/(n+1),(loss_l_value+loss_u_value)/(n+1)))\n",
        "\n",
        "        n+=1\n",
        "\n",
        "    loss_l_value /= len(source_trainloader)\n",
        "    loss_u_value /= len(target_trainloader)\n",
        "\n",
        "    accumulated_loss_l.append(loss_l_value)\n",
        "    accumulated_loss_u.append(loss_u_value)\n",
        "    training_losses.append(loss_l_value+loss_u_value)\n",
        "\n",
        "    # Update learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    # update Mean teacher network\n",
        "    alpha_teacher = 0.99\n",
        "    ema_model = update_ema_variables(ema_model = ema_model, model = model, alpha_teacher=alpha_teacher, iteration=epoch)\n",
        "\n",
        "    print('iter = {0:6d}/{1:6d}, loss_l = {2:.3f}, loss_u = {3:.3f} loss = {4:.3f}'.format(epoch+1, num_epochs, loss_l_value, loss_u_value, loss_l_value+loss_u_value))\n",
        "\n",
        "\n",
        "\n",
        "    val_loss, val_mean_iou, ious_per_class = evaluate(model, val_loader, device)\n",
        "    print(f\"Validation mIoU: {val_mean_iou*100:.3f}%, Validation loss: {val_loss:.5f}\")\n",
        "    for i, cat in enumerate(categories.keys()):\n",
        "        print(f\"{cat} mIoU: {ious_per_class[i]*100:.3f}\")\n",
        "    print()\n",
        "\n",
        "    validation_losses.append(val_loss)\n",
        "    miou_scores.append(val_mean_iou)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1y_fKk3sgbD5",
        "outputId": "74fd1a83-1286-4c5f-ff2a-6293d7a0b093"
      },
      "outputs": [],
      "source": [
        "for i in range(BATCH_SIZE):\n",
        "  plt.imshow(src_images[i].permute(1,2,0).cpu()*torch.tensor(std)+torch.tensor(avg))\n",
        "  plt.show()\n",
        "  plt.imshow(tgt_images[i].permute(1,2,0).cpu()*torch.tensor(std)+torch.tensor(avg))\n",
        "  plt.show()\n",
        "  plt.imshow(inputs_u_s[i].permute(1,2,0).cpu()*torch.tensor(std)+torch.tensor(avg))\n",
        "  plt.show()\n",
        "\n",
        "  plot_tensor_mask(src_labels[i].cpu(), categories)\n",
        "  plot_tensor_mask(targets_u_w[i].cpu(), categories)\n",
        "  plot_tensor_mask(targets_u[i].cpu(), categories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjoF0ZtWbuz3"
      },
      "source": [
        "# Step 5: Improving the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wv_yNNWpP1a8"
      },
      "source": [
        "### Calculate class distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tW6knWDu1cRG"
      },
      "outputs": [],
      "source": [
        "urban_dataset = LoveDA(TRAIN_DIR, IMG_PATH, MASK_PATH, directories=URBAN_PATH)\n",
        "urban_loader = DataLoader(urban_dataset, batch_size=64, worker_init_fn=seed_worker, generator=g)\n",
        "\n",
        "rural_dataset = LoveDA(TRAIN_DIR, IMG_PATH, MASK_PATH, directories=RURAL_PATH)\n",
        "rural_loader = DataLoader(rural_dataset, batch_size=64, worker_init_fn=seed_worker, generator=g)\n",
        "\n",
        "urban_classes = dict()\n",
        "rural_classes = dict()\n",
        "\n",
        "for (_, masks) in urban_loader:\n",
        "\n",
        "      masks = masks.to(device)\n",
        "\n",
        "      for i, cat in enumerate(categories.keys()):\n",
        "        if cat in urban_classes:\n",
        "          urban_classes[cat] += torch.count_nonzero(masks == i)\n",
        "        else:\n",
        "          urban_classes[cat] = torch.count_nonzero(masks == i)\n",
        "\n",
        "for (_, masks) in rural_loader:\n",
        "\n",
        "      masks = masks.to(device)\n",
        "\n",
        "      for i, cat in enumerate(categories.keys()):\n",
        "        if cat in rural_classes:\n",
        "          rural_classes[cat] += torch.count_nonzero(masks == i)\n",
        "        else:\n",
        "          rural_classes[cat] = torch.count_nonzero(masks == i)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 992
        },
        "id": "39fA8Wbbi4KF",
        "outputId": "b7fdfe53-398a-4c7e-a15c-f485bed65998"
      },
      "outputs": [],
      "source": [
        "colors= [np.array(color)/255 for _, color in sorted(categories.values())]\n",
        "\n",
        "wedges, texts, autotexts= plt.pie([v.cpu().numpy() for v in urban_classes.values()], labels=urban_classes.keys(), colors=colors, autopct='%1.1f%%', pctdistance=0.85, labeldistance=1.1, startangle=90)\n",
        "for text in texts:\n",
        "    text.set_fontsize(12)\n",
        "for autotext in autotexts:\n",
        "    autotext.set_fontsize(9)\n",
        "plt.title('Urban Dataset', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"urban_percentage = \", [float(autotext.get_text().strip('%')) for autotext in autotexts])\n",
        "\n",
        "wedges, texts, autotexts= plt.pie([v.cpu().numpy() for v in rural_classes.values()], labels=rural_classes.keys(), colors=colors, autopct='%1.1f%%', pctdistance=0.85, labeldistance=1.1, startangle=90)\n",
        "for text in texts:\n",
        "    text.set_fontsize(12)\n",
        "for autotext in autotexts:\n",
        "    autotext.set_fontsize(9)\n",
        "plt.title(\"Rural dataset\", fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"rural_percentage = \", [float(autotext.get_text().strip('%')) for autotext in autotexts])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "918GoC2vP78y"
      },
      "source": [
        "### Calculate class weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_uMQj3LqDOB",
        "outputId": "cc6d66b9-01e6-4df3-c253-5ada139190ed"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def calc_weights(percentages):\n",
        "  percentages = np.array(percentages)\n",
        "  proportions = percentages / 100  # Divide by 100 to convert percentages to fractions\n",
        "\n",
        "  # Calculate class weights inversely proportional to proportions\n",
        "  class_weights = 1 / proportions\n",
        "\n",
        "  # Optional: Normalize weights so the mean is 1\n",
        "  normalized_weights = class_weights / np.mean(class_weights)\n",
        "\n",
        "  alpha = 0.5  # Adjust this hyperparameter\n",
        "  softened_weights = 1 / (proportions ** alpha)\n",
        "  softened_weights /= np.mean(softened_weights)\n",
        "\n",
        "  normalized_weights_v2 = class_weights / max(class_weights)\n",
        "\n",
        "\n",
        "  return list(class_weights), list(normalized_weights), list(softened_weights), list(normalized_weights_v2)\n",
        "\n",
        "\n",
        "\n",
        "urban_percentage =  [48.5, 21.2, 9.3, 3.7, 7.6, 7.9, 1.9]\n",
        "rural_percentage =  [42.9, 3.7, 2.6, 11.6, 3.6, 5.0, 30.5]\n",
        "\n",
        "urban_class_weights , urban_normalized_weights, urban_softened_weights, urban_normalized_weights_v2 = calc_weights(urban_percentage)\n",
        "rural_class_weights , rural_normalized_weights, rural_softened_weights, rural_normalized_weights_v2 = calc_weights(rural_percentage)\n",
        "\n",
        "print(f\"urban_class_weights = {urban_class_weights}\")\n",
        "print(f\"urban_normalized_weights = {urban_normalized_weights}\")\n",
        "print(f\"urban_softened_weights = {urban_softened_weights}\")\n",
        "print(f\"urban_normalized_weights_v2 = {urban_normalized_weights_v2}\")\n",
        "\n",
        "print()\n",
        "\n",
        "print(f\"rural_class_weights = {rural_class_weights}\")\n",
        "print(f\"rural_normalized_weights = {rural_normalized_weights}\")\n",
        "print(f\"rural_softened_weights = {rural_softened_weights}\")\n",
        "print(f\"rural_normalized_weights_v2 = {rural_normalized_weights_v2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl7BNfbwwYcB"
      },
      "source": [
        "We then used **urban_softened_weights** passing them to the Cross Entropy and trained the model. Training loop is not reported again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbBVeL8Vw1t7"
      },
      "outputs": [],
      "source": [
        "pidnet = get_seg_model(\"pidnet_s\", num_classes, PIDNET_S_WEIGHTS,imgnet_pretrained=True)\n",
        "model = FullModel(pidnet, sem_loss=CrossEntropy(ignore_label=IGNORE_INDEX, weight=torch.tensor(urban_softened_weights)), bd_loss=BondaryLoss())\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHENH8fJxJHx"
      },
      "source": [
        "## OHEM Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61I60pjixamG"
      },
      "outputs": [],
      "source": [
        "class OhemCrossEntropy(nn.Module):\n",
        "    def __init__(self, ignore_label=-1, thres=0.7,\n",
        "                 min_kept=100000, weight=None):\n",
        "        super(OhemCrossEntropy, self).__init__()\n",
        "        self.thresh = thres\n",
        "        self.min_kept = max(1, min_kept)\n",
        "        self.ignore_label = ignore_label\n",
        "        self.criterion = nn.CrossEntropyLoss(\n",
        "            weight=weight,\n",
        "            ignore_index=ignore_label,\n",
        "            reduction='none'\n",
        "        )\n",
        "\n",
        "    def _ce_forward(self, score, target):\n",
        "\n",
        "        loss = self.criterion(score, target)\n",
        "\n",
        "        return loss.mean()\n",
        "\n",
        "    def _ohem_forward(self, score, target, **kwargs):\n",
        "\n",
        "        pred = F.softmax(score, dim=1)\n",
        "        pixel_losses = self.criterion(score, target).contiguous().view(-1)\n",
        "        mask = target.contiguous().view(-1) != self.ignore_label\n",
        "\n",
        "        tmp_target = target.clone()\n",
        "        tmp_target[tmp_target == self.ignore_label] = 0\n",
        "        pred = pred.gather(1, tmp_target.unsqueeze(1))\n",
        "        pred, ind = pred.contiguous().view(-1,)[mask].contiguous().sort()\n",
        "\n",
        "        min_value = pred[min(self.min_kept, pred.numel() - 1)]\n",
        "\n",
        "        threshold = max(min_value, self.thresh)\n",
        "\n",
        "        pixel_losses = pixel_losses[mask][ind]\n",
        "        pixel_losses = pixel_losses[pred < threshold]\n",
        "        return pixel_losses.mean()\n",
        "\n",
        "    def forward(self, score, target):\n",
        "\n",
        "        if not (isinstance(score, list) or isinstance(score, tuple)):\n",
        "            score = [score]\n",
        "\n",
        "        balance_weights = [0.4, 1.0]\n",
        "        sb_weights = 1.0\n",
        "\n",
        "        if len(balance_weights) == len(score):\n",
        "            functions = [self._ce_forward] * (len(balance_weights) - 1) + [self._ohem_forward]\n",
        "            return sum([w * func(x, target) for (w, x, func) in zip(balance_weights, score, functions)])\n",
        "\n",
        "        elif len(score) == 1:\n",
        "            return sb_weights * self._ohem_forward(score[0], target)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"lengths of prediction and target are not identical!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZFFJHEAxNJh"
      },
      "source": [
        "## Focal Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gIvWAtCxamH"
      },
      "outputs": [],
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=0, alpha=None, ignore_label=-1, weight=None):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.ignore_label = ignore_label\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.criterion = nn.CrossEntropyLoss(\n",
        "            weight=weight,\n",
        "            ignore_index=ignore_label\n",
        "        )\n",
        "\n",
        "    def _forward(self, score, target):\n",
        "\n",
        "        ce_loss = self.criterion(score, target)\n",
        "\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = torch.pow(1 - pt, self.gamma)\n",
        "\n",
        "        if self.alpha is not None:\n",
        "            return self.alpha * focal_loss\n",
        "        return focal_loss\n",
        "\n",
        "    def forward(self, score, target):\n",
        "\n",
        "        if not (isinstance(score, list) or isinstance(score, tuple)):\n",
        "            score = [score]\n",
        "\n",
        "        # From original configs\n",
        "        balance_weights = [0.4, 1.0]\n",
        "        sb_weights = 1.0\n",
        "\n",
        "        if len(balance_weights) == len(score):\n",
        "            return sum([w * self._forward(x, target) for (w, x) in zip(balance_weights, score)])\n",
        "        elif len(score) == 1:\n",
        "            return sb_weights * self._forward(score[0], target)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"lengths of prediction and target are not identical!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZirP1ZgqlkcW"
      },
      "source": [
        "# Step 5: BiSeNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avdxWmNNl42M"
      },
      "source": [
        "### Context path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-I1CA12lm7t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import models\n",
        "\n",
        "\n",
        "class resnet18(torch.nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super().__init__()\n",
        "        self.features = models.resnet18(pretrained=pretrained)\n",
        "        self.conv1 = self.features.conv1\n",
        "        self.bn1 = self.features.bn1\n",
        "        self.relu = self.features.relu\n",
        "        self.maxpool1 = self.features.maxpool\n",
        "        self.layer1 = self.features.layer1\n",
        "        self.layer2 = self.features.layer2\n",
        "        self.layer3 = self.features.layer3\n",
        "        self.layer4 = self.features.layer4\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.conv1(input)\n",
        "        x = self.relu(self.bn1(x))\n",
        "        x = self.maxpool1(x)\n",
        "        feature1 = self.layer1(x)  # 1 / 4\n",
        "        feature2 = self.layer2(feature1)  # 1 / 8\n",
        "        feature3 = self.layer3(feature2)  # 1 / 16\n",
        "        feature4 = self.layer4(feature3)  # 1 / 32\n",
        "        # global average pooling to build tail\n",
        "        tail = torch.mean(feature4, 3, keepdim=True)\n",
        "        tail = torch.mean(tail, 2, keepdim=True)\n",
        "        return feature3, feature4, tail\n",
        "\n",
        "\n",
        "class resnet101(torch.nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super().__init__()\n",
        "        self.features = models.resnet101(pretrained=pretrained)\n",
        "        self.conv1 = self.features.conv1\n",
        "        self.bn1 = self.features.bn1\n",
        "        self.relu = self.features.relu\n",
        "        self.maxpool1 = self.features.maxpool\n",
        "        self.layer1 = self.features.layer1\n",
        "        self.layer2 = self.features.layer2\n",
        "        self.layer3 = self.features.layer3\n",
        "        self.layer4 = self.features.layer4\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.conv1(input)\n",
        "        x = self.relu(self.bn1(x))\n",
        "        x = self.maxpool1(x)\n",
        "        feature1 = self.layer1(x)  # 1 / 4\n",
        "        feature2 = self.layer2(feature1)  # 1 / 8\n",
        "        feature3 = self.layer3(feature2)  # 1 / 16\n",
        "        feature4 = self.layer4(feature3)  # 1 / 32\n",
        "        # global average pooling to build tail\n",
        "        tail = torch.mean(feature4, 3, keepdim=True)\n",
        "        tail = torch.mean(tail, 2, keepdim=True)\n",
        "        return feature3, feature4, tail\n",
        "\n",
        "\n",
        "def build_contextpath(name):\n",
        "    model = {\n",
        "        'resnet18': resnet18(pretrained=True),\n",
        "        'resnet101': resnet101(pretrained=True)\n",
        "    }\n",
        "    return model[name]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BY87bcqmD4K"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9D20VkZpl_U_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "\n",
        "class ConvBlock(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n",
        "                               stride=stride, padding=padding, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.conv1(input)\n",
        "        return self.relu(self.bn(x))\n",
        "\n",
        "\n",
        "class Spatial_path(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.convblock1 = ConvBlock(in_channels=3, out_channels=64)\n",
        "        self.convblock2 = ConvBlock(in_channels=64, out_channels=128)\n",
        "        self.convblock3 = ConvBlock(in_channels=128, out_channels=256)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.convblock1(input)\n",
        "        x = self.convblock2(x)\n",
        "        x = self.convblock3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class AttentionRefinementModule(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.in_channels = in_channels\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
        "\n",
        "    def forward(self, input):\n",
        "        # global average pooling\n",
        "        x = self.avgpool(input)\n",
        "        assert self.in_channels == x.size(1), 'in_channels and out_channels should all be {}'.format(x.size(1))\n",
        "        x = self.conv(x)\n",
        "        x = self.sigmoid(self.bn(x))\n",
        "        # x = self.sigmoid(x)\n",
        "        # channels of input and x should be same\n",
        "        x = torch.mul(input, x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FeatureFusionModule(torch.nn.Module):\n",
        "    def __init__(self, num_classes, in_channels):\n",
        "        super().__init__()\n",
        "        # self.in_channels = input_1.channels + input_2.channels\n",
        "        # resnet101 3328 = 256(from spatial path) + 1024(from context path) + 2048(from context path)\n",
        "        # resnet18  1024 = 256(from spatial path) + 256(from context path) + 512(from context path)\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.convblock = ConvBlock(in_channels=self.in_channels, out_channels=num_classes, stride=1)\n",
        "        self.conv1 = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
        "\n",
        "    def forward(self, input_1, input_2):\n",
        "        x = torch.cat((input_1, input_2), dim=1)\n",
        "        assert self.in_channels == x.size(1), 'in_channels of ConvBlock should be {}'.format(x.size(1))\n",
        "        feature = self.convblock(x)\n",
        "        x = self.avgpool(feature)\n",
        "\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.sigmoid(self.conv2(x))\n",
        "        x = torch.mul(feature, x)\n",
        "        x = torch.add(x, feature)\n",
        "        return x\n",
        "\n",
        "\n",
        "class BiSeNet(torch.nn.Module):\n",
        "    def __init__(self, num_classes, context_path):\n",
        "        super().__init__()\n",
        "        # build spatial path\n",
        "        self.saptial_path = Spatial_path()\n",
        "\n",
        "        # build context path\n",
        "        self.context_path = build_contextpath(name=context_path)\n",
        "\n",
        "        # build attention refinement module  for resnet 101\n",
        "        if context_path == 'resnet101':\n",
        "            self.attention_refinement_module1 = AttentionRefinementModule(1024, 1024)\n",
        "            self.attention_refinement_module2 = AttentionRefinementModule(2048, 2048)\n",
        "            # supervision block\n",
        "            self.supervision1 = nn.Conv2d(in_channels=1024, out_channels=num_classes, kernel_size=1)\n",
        "            self.supervision2 = nn.Conv2d(in_channels=2048, out_channels=num_classes, kernel_size=1)\n",
        "            # build feature fusion module\n",
        "            self.feature_fusion_module = FeatureFusionModule(num_classes, 3328)\n",
        "\n",
        "        elif context_path == 'resnet18':\n",
        "            # build attention refinement module  for resnet 18\n",
        "            self.attention_refinement_module1 = AttentionRefinementModule(256, 256)\n",
        "            self.attention_refinement_module2 = AttentionRefinementModule(512, 512)\n",
        "            # supervision block\n",
        "            self.supervision1 = nn.Conv2d(in_channels=256, out_channels=num_classes, kernel_size=1)\n",
        "            self.supervision2 = nn.Conv2d(in_channels=512, out_channels=num_classes, kernel_size=1)\n",
        "            # build feature fusion module\n",
        "            self.feature_fusion_module = FeatureFusionModule(num_classes, 1024)\n",
        "        else:\n",
        "            print('Error: unspport context_path network \\n')\n",
        "\n",
        "        # build final convolution\n",
        "        self.conv = nn.Conv2d(in_channels=num_classes, out_channels=num_classes, kernel_size=1)\n",
        "\n",
        "        self.init_weight()\n",
        "\n",
        "        self.mul_lr = []\n",
        "        self.mul_lr.append(self.saptial_path)\n",
        "        self.mul_lr.append(self.attention_refinement_module1)\n",
        "        self.mul_lr.append(self.attention_refinement_module2)\n",
        "        self.mul_lr.append(self.supervision1)\n",
        "        self.mul_lr.append(self.supervision2)\n",
        "        self.mul_lr.append(self.feature_fusion_module)\n",
        "        self.mul_lr.append(self.conv)\n",
        "\n",
        "    def init_weight(self):\n",
        "        for name, m in self.named_modules():\n",
        "            if 'context_path' not in name:\n",
        "                if isinstance(m, nn.Conv2d):\n",
        "                    nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "                elif isinstance(m, nn.BatchNorm2d):\n",
        "                    m.eps = 1e-5\n",
        "                    m.momentum = 0.1\n",
        "                    nn.init.constant_(m.weight, 1)\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # output of spatial path\n",
        "        sx = self.saptial_path(input)\n",
        "\n",
        "        # output of context path\n",
        "        cx1, cx2, tail = self.context_path(input)\n",
        "        cx1 = self.attention_refinement_module1(cx1)\n",
        "        cx2 = self.attention_refinement_module2(cx2)\n",
        "        cx2 = torch.mul(cx2, tail)\n",
        "        # upsampling\n",
        "        cx1 = torch.nn.functional.interpolate(cx1, size=sx.size()[-2:], mode='bilinear')\n",
        "        cx2 = torch.nn.functional.interpolate(cx2, size=sx.size()[-2:], mode='bilinear')\n",
        "        cx = torch.cat((cx1, cx2), dim=1)\n",
        "\n",
        "        if self.training == True:\n",
        "            cx1_sup = self.supervision1(cx1)\n",
        "            cx2_sup = self.supervision2(cx2)\n",
        "            cx1_sup = torch.nn.functional.interpolate(cx1_sup, size=input.size()[-2:], mode='bilinear')\n",
        "            cx2_sup = torch.nn.functional.interpolate(cx2_sup, size=input.size()[-2:], mode='bilinear')\n",
        "\n",
        "        # output of feature fusion module\n",
        "        result = self.feature_fusion_module(sx, cx)\n",
        "\n",
        "        # upsampling\n",
        "        result = torch.nn.functional.interpolate(result, scale_factor=8, mode='bilinear')\n",
        "        result = self.conv(result)\n",
        "\n",
        "        if self.training == True:\n",
        "            return result, cx1_sup, cx2_sup\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8up2B3knSFS"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcQ_xEK3nSFS"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCoD2u0rnSFS"
      },
      "outputs": [],
      "source": [
        "num_epochs = 20\n",
        "BATCH_SIZE = 6\n",
        "learning_rate = 1e-3\n",
        "step_size = 10\n",
        "gamma = 0.1\n",
        "resize = 512\n",
        "w_decay = 1e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15-4beKOnSFS"
      },
      "source": [
        "### Dataset preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fNRQCeKnSFT"
      },
      "source": [
        "#### Normalization metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tL8435sKnSFT"
      },
      "outputs": [],
      "source": [
        "num_workers = 2 if device.type == 'cuda' else 0\n",
        "\n",
        "# Poiché il modello è pretrainato su ImageNet, si usano media e varianza di ImageNet\n",
        "avg = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_XBD1iYnSFT"
      },
      "source": [
        "#### Training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wV9BpUrnSFT"
      },
      "outputs": [],
      "source": [
        "train_transform = A.Compose([\n",
        "    A.Normalize(mean=avg, std=std, p=1, always_apply=True, max_pixel_value=255),\n",
        "    A.Resize(resize, resize, p=1, always_apply=True)\n",
        "])\n",
        "train_dataset = LoveDA(TRAIN_DIR, IMG_PATH, MASK_PATH, directories=URBAN_PATH, transforms=train_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=num_workers, worker_init_fn=seed_worker, generator=g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb5mn06QnSFT"
      },
      "source": [
        "#### Validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvthpg62nSFT"
      },
      "outputs": [],
      "source": [
        "val_transform = A.Compose([\n",
        "    A.Normalize(mean=avg, std=std, p=1, always_apply=True, max_pixel_value=255)\n",
        "])\n",
        "#val_dataset = LoveDA(VAL_DIR, IMG_PATH, MASK_PATH, directories=URBAN_PATH, transforms=val_transform)\n",
        "val_dataset = LoveDA(VAL_DIR, IMG_PATH, MASK_PATH, directories=RURAL_PATH, transforms=val_transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=num_workers, worker_init_fn=seed_worker, generator=g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDYwJmndnSFT"
      },
      "source": [
        "### Training process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbk6UOcKnSFT"
      },
      "source": [
        "#### Model engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hM0_629YnSFU",
        "outputId": "374cb8ff-582d-48e5-c492-2a1f9338e9fc"
      },
      "outputs": [],
      "source": [
        "model = BiSeNet(num_classes,'resnet101').to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=w_decay)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKakLuigvTnb"
      },
      "source": [
        "#### Evaluate function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rbtvULIvTnc"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, dataloader, device, ) -> tuple:\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    data_len = 0\n",
        "    iou_scores = 0.0\n",
        "    ious_per_class = torch.zeros(num_classes)\n",
        "\n",
        "    for i, (inputs, masks) in enumerate(dataloader):\n",
        "\n",
        "        data_len += inputs.size(0)\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, masks)\n",
        "\n",
        "        running_loss += loss.item()*inputs.size(0)\n",
        "\n",
        "        # Calculate mIoU\n",
        "        iou, iou_per_class = calculate_iou(outputs, masks, num_classes)\n",
        "        iou_scores += iou*inputs.size(0)\n",
        "        ious_per_class+=iou_per_class.cpu()*inputs.size(0)\n",
        "\n",
        "    mIoU = iou_scores/data_len\n",
        "    loss = running_loss/data_len\n",
        "    ious_per_class/=data_len\n",
        "\n",
        "\n",
        "    return loss, mIoU, ious_per_class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsE0O98MnSFU"
      },
      "source": [
        "#### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQksizQBnSFU",
        "outputId": "5db941af-2f81-4af2-9454-88e318dd1ebe"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "eval_losses = []\n",
        "mious = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(\"### Training mode\")\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (images, masks) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs, outputs16, outputs32 = model(images)\n",
        "        loss1 = criterion(outputs, masks)\n",
        "        loss2 = criterion(outputs16, masks)\n",
        "        loss3 = criterion(outputs32, masks)\n",
        "        loss = loss1 + loss2 + loss3\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if (i + 1) % 25 == 0:\n",
        "            print(f\"Processed {i + 1} batches, loss: {running_loss / (i+1)}\")\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(epoch_loss)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    print(\"### Evaluation mode\")\n",
        "    val_loss, miou, ious_per_class = evaluate(model, val_loader, device)\n",
        "\n",
        "    print(f\"Validation mIoU: {miou*100:.3f}%, Validation loss: {val_loss:.5f}\")\n",
        "    for i, cat in enumerate(categories.keys()):\n",
        "        print(f\"{cat} mIoU: {ious_per_class[i]*100:.3f}\")\n",
        "    print()\n",
        "\n",
        "    eval_losses.append(val_loss)\n",
        "\n",
        "    mious.append(miou)\n",
        "\n",
        "    print(f\"Epoch: [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}, mIoU: {(miou * 100):.2f}%\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "474HU-BtnSFV"
      },
      "source": [
        "### Metric calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "BOAczM-9nSFV",
        "outputId": "31eb5214-b277-44d4-c36c-0365f14037ad"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "mean_latency, _, _, _ = calculate_latency_fps(model, device, 1024, 1024, num_epochs, ModelType.BISENET)\n",
        "print(f\"Mean Latency: {mean_latency:.2f} ms\")\n",
        "\n",
        "calculate_flops_params(model, device, 1024, 1024, ModelType.BISENET)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kf7UOPXtvv4u"
      },
      "source": [
        "# Step 5: STDC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxN5kO6OT1jt"
      },
      "source": [
        "### Download pre-trained weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDwG19HPT1jt"
      },
      "outputs": [],
      "source": [
        "weights_dir = Path(PRETRAINED_WEIGHTS_DIR)\n",
        "if not weights_dir.exists():\n",
        "    weights_dir.mkdir(exist_ok=True)\n",
        "\n",
        "stdc1_weights = Path(STDC1_WEIGHTS)\n",
        "if not stdc1_weights.exists():\n",
        "    # Replace with the correct Google Drive file ID\n",
        "    file_id = \"1DFoXcV42zy-apUcMh5P8WhsXMRJofgl8\"\n",
        "    gdown.download(id=file_id, output=str(stdc1_weights), quiet=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8SuvFTlwe9a"
      },
      "source": [
        "### Nets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAu4l_4cweZr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import math\n",
        "\n",
        "\n",
        "\n",
        "class ConvX(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, kernel=3, stride=1):\n",
        "        super(ConvX, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel, stride=stride, padding=kernel//2, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.bn(self.conv(x)))\n",
        "        return out\n",
        "\n",
        "\n",
        "class AddBottleneck(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, block_num=3, stride=1):\n",
        "        super(AddBottleneck, self).__init__()\n",
        "        assert block_num > 1, print(\"block number should be larger than 1.\")\n",
        "        self.conv_list = nn.ModuleList()\n",
        "        self.stride = stride\n",
        "        if stride == 2:\n",
        "            self.avd_layer = nn.Sequential(\n",
        "                nn.Conv2d(out_planes//2, out_planes//2, kernel_size=3, stride=2, padding=1, groups=out_planes//2, bias=False),\n",
        "                nn.BatchNorm2d(out_planes//2),\n",
        "            )\n",
        "            self.skip = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=2, padding=1, groups=in_planes, bias=False),\n",
        "                nn.BatchNorm2d(in_planes),\n",
        "                nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False),\n",
        "                nn.BatchNorm2d(out_planes),\n",
        "            )\n",
        "            stride = 1\n",
        "\n",
        "        for idx in range(block_num):\n",
        "            if idx == 0:\n",
        "                self.conv_list.append(ConvX(in_planes, out_planes//2, kernel=1))\n",
        "            elif idx == 1 and block_num == 2:\n",
        "                self.conv_list.append(ConvX(out_planes//2, out_planes//2, stride=stride))\n",
        "            elif idx == 1 and block_num > 2:\n",
        "                self.conv_list.append(ConvX(out_planes//2, out_planes//4, stride=stride))\n",
        "            elif idx < block_num - 1:\n",
        "                self.conv_list.append(ConvX(out_planes//int(math.pow(2, idx)), out_planes//int(math.pow(2, idx+1))))\n",
        "            else:\n",
        "                self.conv_list.append(ConvX(out_planes//int(math.pow(2, idx)), out_planes//int(math.pow(2, idx))))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_list = []\n",
        "        out = x\n",
        "\n",
        "        for idx, conv in enumerate(self.conv_list):\n",
        "            if idx == 0 and self.stride == 2:\n",
        "                out = self.avd_layer(conv(out))\n",
        "            else:\n",
        "                out = conv(out)\n",
        "            out_list.append(out)\n",
        "\n",
        "        if self.stride == 2:\n",
        "            x = self.skip(x)\n",
        "\n",
        "        return torch.cat(out_list, dim=1) + x\n",
        "\n",
        "\n",
        "\n",
        "class CatBottleneck(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, block_num=3, stride=1):\n",
        "        super(CatBottleneck, self).__init__()\n",
        "        assert block_num > 1, print(\"block number should be larger than 1.\")\n",
        "        self.conv_list = nn.ModuleList()\n",
        "        self.stride = stride\n",
        "        if stride == 2:\n",
        "            self.avd_layer = nn.Sequential(\n",
        "                nn.Conv2d(out_planes//2, out_planes//2, kernel_size=3, stride=2, padding=1, groups=out_planes//2, bias=False),\n",
        "                nn.BatchNorm2d(out_planes//2),\n",
        "            )\n",
        "            self.skip = nn.AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
        "            stride = 1\n",
        "\n",
        "        for idx in range(block_num):\n",
        "            if idx == 0:\n",
        "                self.conv_list.append(ConvX(in_planes, out_planes//2, kernel=1))\n",
        "            elif idx == 1 and block_num == 2:\n",
        "                self.conv_list.append(ConvX(out_planes//2, out_planes//2, stride=stride))\n",
        "            elif idx == 1 and block_num > 2:\n",
        "                self.conv_list.append(ConvX(out_planes//2, out_planes//4, stride=stride))\n",
        "            elif idx < block_num - 1:\n",
        "                self.conv_list.append(ConvX(out_planes//int(math.pow(2, idx)), out_planes//int(math.pow(2, idx+1))))\n",
        "            else:\n",
        "                self.conv_list.append(ConvX(out_planes//int(math.pow(2, idx)), out_planes//int(math.pow(2, idx))))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_list = []\n",
        "        out1 = self.conv_list[0](x)\n",
        "\n",
        "        for idx, conv in enumerate(self.conv_list[1:]):\n",
        "            if idx == 0:\n",
        "                if self.stride == 2:\n",
        "                    out = conv(self.avd_layer(out1))\n",
        "                else:\n",
        "                    out = conv(out1)\n",
        "            else:\n",
        "                out = conv(out)\n",
        "            out_list.append(out)\n",
        "\n",
        "        if self.stride == 2:\n",
        "            out1 = self.skip(out1)\n",
        "        out_list.insert(0, out1)\n",
        "\n",
        "        out = torch.cat(out_list, dim=1)\n",
        "        return out\n",
        "\n",
        "#STDC2Net\n",
        "class STDCNet1446(nn.Module):\n",
        "    def __init__(self, base=64, layers=[4,5,3], block_num=4, type=\"cat\", num_classes=1000, dropout=0.20, pretrain_model='', use_conv_last=False):\n",
        "        super(STDCNet1446, self).__init__()\n",
        "        if type == \"cat\":\n",
        "            block = CatBottleneck\n",
        "        elif type == \"add\":\n",
        "            block = AddBottleneck\n",
        "        self.use_conv_last = use_conv_last\n",
        "        self.features = self._make_layers(base, layers, block_num, block)\n",
        "        self.conv_last = ConvX(base*16, max(1024, base*16), 1, 1)\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(max(1024, base*16), max(1024, base*16), bias=False)\n",
        "        self.bn = nn.BatchNorm1d(max(1024, base*16))\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.linear = nn.Linear(max(1024, base*16), num_classes, bias=False)\n",
        "\n",
        "        self.x2 = nn.Sequential(self.features[:1])\n",
        "        self.x4 = nn.Sequential(self.features[1:2])\n",
        "        self.x8 = nn.Sequential(self.features[2:6])\n",
        "        self.x16 = nn.Sequential(self.features[6:11])\n",
        "        self.x32 = nn.Sequential(self.features[11:])\n",
        "\n",
        "        if pretrain_model:\n",
        "            print('use pretrain model {}'.format(pretrain_model))\n",
        "            self.init_weight(pretrain_model)\n",
        "        else:\n",
        "            self.init_params()\n",
        "\n",
        "    def init_weight(self, pretrain_model):\n",
        "\n",
        "        state_dict = torch.load(pretrain_model)[\"state_dict\"]\n",
        "        self_state_dict = self.state_dict()\n",
        "        for k, v in state_dict.items():\n",
        "            self_state_dict.update({k: v})\n",
        "        self.load_state_dict(self_state_dict)\n",
        "\n",
        "    def init_params(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                init.constant_(m.weight, 1)\n",
        "                init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                init.normal_(m.weight, std=0.001)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layers(self, base, layers, block_num, block):\n",
        "        features = []\n",
        "        features += [ConvX(3, base//2, 3, 2)]\n",
        "        features += [ConvX(base//2, base, 3, 2)]\n",
        "\n",
        "        for i, layer in enumerate(layers):\n",
        "            for j in range(layer):\n",
        "                if i == 0 and j == 0:\n",
        "                    features.append(block(base, base*4, block_num, 2))\n",
        "                elif j == 0:\n",
        "                    features.append(block(base*int(math.pow(2,i+1)), base*int(math.pow(2,i+2)), block_num, 2))\n",
        "                else:\n",
        "                    features.append(block(base*int(math.pow(2,i+2)), base*int(math.pow(2,i+2)), block_num, 1))\n",
        "\n",
        "        return nn.Sequential(*features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat2 = self.x2(x)\n",
        "        feat4 = self.x4(feat2)\n",
        "        feat8 = self.x8(feat4)\n",
        "        feat16 = self.x16(feat8)\n",
        "        feat32 = self.x32(feat16)\n",
        "        if self.use_conv_last:\n",
        "           feat32 = self.conv_last(feat32)\n",
        "\n",
        "        return feat2, feat4, feat8, feat16, feat32\n",
        "\n",
        "    def forward_impl(self, x):\n",
        "        out = self.features(x)\n",
        "        out = self.conv_last(out).pow(2)\n",
        "        out = self.gap(out).flatten(1)\n",
        "        out = self.fc(out)\n",
        "        # out = self.bn(out)\n",
        "        out = self.relu(out)\n",
        "        # out = self.relu(self.bn(self.fc(out)))\n",
        "        out = self.dropout(out)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "# STDC1Net\n",
        "class STDCNet813(nn.Module):\n",
        "    def __init__(self, base=64, layers=[2,2,2], block_num=4, type=\"cat\", num_classes=1000, dropout=0.20, pretrain_model='', use_conv_last=False):\n",
        "        super(STDCNet813, self).__init__()\n",
        "        if type == \"cat\":\n",
        "            block = CatBottleneck\n",
        "        elif type == \"add\":\n",
        "            block = AddBottleneck\n",
        "        self.use_conv_last = use_conv_last\n",
        "        self.features = self._make_layers(base, layers, block_num, block)\n",
        "        self.conv_last = ConvX(base*16, max(1024, base*16), 1, 1)\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(max(1024, base*16), max(1024, base*16), bias=False)\n",
        "        self.bn = nn.BatchNorm1d(max(1024, base*16))\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.linear = nn.Linear(max(1024, base*16), num_classes, bias=False)\n",
        "\n",
        "        self.x2 = nn.Sequential(self.features[:1])\n",
        "        self.x4 = nn.Sequential(self.features[1:2])\n",
        "        self.x8 = nn.Sequential(self.features[2:4])\n",
        "        self.x16 = nn.Sequential(self.features[4:6])\n",
        "        self.x32 = nn.Sequential(self.features[6:])\n",
        "\n",
        "        if pretrain_model:\n",
        "            print('use pretrain model {}'.format(pretrain_model))\n",
        "            self.init_weight(pretrain_model)\n",
        "        else:\n",
        "            self.init_params()\n",
        "\n",
        "    def init_weight(self, pretrain_model):\n",
        "\n",
        "        state_dict = torch.load(pretrain_model)[\"state_dict\"]\n",
        "        self_state_dict = self.state_dict()\n",
        "        for k, v in state_dict.items():\n",
        "            self_state_dict.update({k: v})\n",
        "        self.load_state_dict(self_state_dict)\n",
        "\n",
        "    def init_params(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                init.constant_(m.weight, 1)\n",
        "                init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                init.normal_(m.weight, std=0.001)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layers(self, base, layers, block_num, block):\n",
        "        features = []\n",
        "        features += [ConvX(3, base//2, 3, 2)]\n",
        "        features += [ConvX(base//2, base, 3, 2)]\n",
        "\n",
        "        for i, layer in enumerate(layers):\n",
        "            for j in range(layer):\n",
        "                if i == 0 and j == 0:\n",
        "                    features.append(block(base, base*4, block_num, 2))\n",
        "                elif j == 0:\n",
        "                    features.append(block(base*int(math.pow(2,i+1)), base*int(math.pow(2,i+2)), block_num, 2))\n",
        "                else:\n",
        "                    features.append(block(base*int(math.pow(2,i+2)), base*int(math.pow(2,i+2)), block_num, 1))\n",
        "\n",
        "        return nn.Sequential(*features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat2 = self.x2(x)\n",
        "        feat4 = self.x4(feat2)\n",
        "        feat8 = self.x8(feat4)\n",
        "        feat16 = self.x16(feat8)\n",
        "        feat32 = self.x32(feat16)\n",
        "        if self.use_conv_last:\n",
        "           feat32 = self.conv_last(feat32)\n",
        "\n",
        "        return feat2, feat4, feat8, feat16, feat32\n",
        "\n",
        "    def forward_impl(self, x):\n",
        "        out = self.features(x)\n",
        "        out = self.conv_last(out).pow(2)\n",
        "        out = self.gap(out).flatten(1)\n",
        "        out = self.fc(out)\n",
        "        # out = self.bn(out)\n",
        "        out = self.relu(out)\n",
        "        # out = self.relu(self.bn(self.fc(out)))\n",
        "        out = self.dropout(out)\n",
        "        out = self.linear(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CULoLihOwxHl"
      },
      "source": [
        "### BN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MPnj9t1wwsG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as functional\n",
        "\n",
        "try:\n",
        "    from queue import Queue\n",
        "except ImportError:\n",
        "    from Queue import Queue\n",
        "\n",
        "\n",
        "class ABN(nn.Module):\n",
        "    \"\"\"Activated Batch Normalization\n",
        "\n",
        "    This gathers a `BatchNorm2d` and an activation function in a single module\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, activation=\"leaky_relu\", slope=0.01):\n",
        "        \"\"\"Creates an Activated Batch Normalization module\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        num_features : int\n",
        "            Number of feature channels in the input and output.\n",
        "        eps : float\n",
        "            Small constant to prevent numerical issues.\n",
        "        momentum : float\n",
        "            Momentum factor applied to compute running statistics as.\n",
        "        affine : bool\n",
        "            If `True` apply learned scale and shift transformation after normalization.\n",
        "        activation : str\n",
        "            Name of the activation functions, one of: `leaky_relu`, `elu` or `none`.\n",
        "        slope : float\n",
        "            Negative slope for the `leaky_relu` activation.\n",
        "        \"\"\"\n",
        "        super(ABN, self).__init__()\n",
        "        self.num_features = num_features\n",
        "        self.affine = affine\n",
        "        self.eps = eps\n",
        "        self.momentum = momentum\n",
        "        self.activation = activation\n",
        "        self.slope = slope\n",
        "        if self.affine:\n",
        "            self.weight = nn.Parameter(torch.ones(num_features))\n",
        "            self.bias = nn.Parameter(torch.zeros(num_features))\n",
        "        else:\n",
        "            self.register_parameter('weight', None)\n",
        "            self.register_parameter('bias', None)\n",
        "        self.register_buffer('running_mean', torch.zeros(num_features))\n",
        "        self.register_buffer('running_var', torch.ones(num_features))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.constant_(self.running_mean, 0)\n",
        "        nn.init.constant_(self.running_var, 1)\n",
        "        if self.affine:\n",
        "            nn.init.constant_(self.weight, 1)\n",
        "            nn.init.constant_(self.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = functional.batch_norm(x, self.running_mean, self.running_var, self.weight, self.bias,\n",
        "                                  self.training, self.momentum, self.eps)\n",
        "\n",
        "        if self.activation == ACT_RELU:\n",
        "            return functional.relu(x, inplace=True)\n",
        "        elif self.activation == ACT_LEAKY_RELU:\n",
        "            return functional.leaky_relu(x, negative_slope=self.slope, inplace=True)\n",
        "        elif self.activation == ACT_ELU:\n",
        "            return functional.elu(x, inplace=True)\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "    def __repr__(self):\n",
        "        rep = '{name}({num_features}, eps={eps}, momentum={momentum},' \\\n",
        "              ' affine={affine}, activation={activation}'\n",
        "        if self.activation == \"leaky_relu\":\n",
        "            rep += ', slope={slope})'\n",
        "        else:\n",
        "            rep += ')'\n",
        "        return rep.format(name=self.__class__.__name__, **self.__dict__)\n",
        "\n",
        "\n",
        "class InPlaceABN(ABN):\n",
        "    \"\"\"InPlace Activated Batch Normalization\"\"\"\n",
        "\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, activation=\"leaky_relu\", slope=0.01):\n",
        "        \"\"\"Creates an InPlace Activated Batch Normalization module\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        num_features : int\n",
        "            Number of feature channels in the input and output.\n",
        "        eps : float\n",
        "            Small constant to prevent numerical issues.\n",
        "        momentum : float\n",
        "            Momentum factor applied to compute running statistics as.\n",
        "        affine : bool\n",
        "            If `True` apply learned scale and shift transformation after normalization.\n",
        "        activation : str\n",
        "            Name of the activation functions, one of: `leaky_relu`, `elu` or `none`.\n",
        "        slope : float\n",
        "            Negative slope for the `leaky_relu` activation.\n",
        "        \"\"\"\n",
        "        super(InPlaceABN, self).__init__(num_features, eps, momentum, affine, activation, slope)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return inplace_abn(x, self.weight, self.bias, self.running_mean, self.running_var,\n",
        "                           self.training, self.momentum, self.eps, self.activation, self.slope)\n",
        "\n",
        "\n",
        "class InPlaceABNSync(ABN):\n",
        "    \"\"\"InPlace Activated Batch Normalization with cross-GPU synchronization\n",
        "    This assumes that it will be replicated across GPUs using the same mechanism as in `nn.DistributedDataParallel`.\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        return inplace_abn_sync(x, self.weight, self.bias, self.running_mean, self.running_var,\n",
        "                                   self.training, self.momentum, self.eps, self.activation, self.slope)\n",
        "\n",
        "    def __repr__(self):\n",
        "        rep = '{name}({num_features}, eps={eps}, momentum={momentum},' \\\n",
        "              ' affine={affine}, activation={activation}'\n",
        "        if self.activation == \"leaky_relu\":\n",
        "            rep += ', slope={slope})'\n",
        "        else:\n",
        "            rep += ')'\n",
        "        return rep.format(name=self.__class__.__name__, **self.__dict__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQKuCveRw8Qm"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DDkjMSFvzlA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "\n",
        "#BatchNorm2d = InPlaceABNSync\n",
        "BatchNorm2d = nn.BatchNorm2d\n",
        "\n",
        "class ConvBNReLU(nn.Module):\n",
        "    def __init__(self, in_chan, out_chan, ks=3, stride=1, padding=1, *args, **kwargs):\n",
        "        super(ConvBNReLU, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_chan,\n",
        "                out_chan,\n",
        "                kernel_size = ks,\n",
        "                stride = stride,\n",
        "                padding = padding,\n",
        "                bias = False)\n",
        "        self.bn = BatchNorm2d(out_chan)\n",
        "        #self.bn = BatchNorm2d(out_chan, activation='none')\n",
        "        self.relu = nn.ReLU()\n",
        "        self.init_weight()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "    def init_weight(self):\n",
        "        for ly in self.children():\n",
        "            if isinstance(ly, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(ly.weight, a=1)\n",
        "                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n",
        "\n",
        "\n",
        "class BiSeNetOutput(nn.Module):\n",
        "    def __init__(self, in_chan, mid_chan, n_classes, *args, **kwargs):\n",
        "        super(BiSeNetOutput, self).__init__()\n",
        "        self.conv = ConvBNReLU(in_chan, mid_chan, ks=3, stride=1, padding=1)\n",
        "        self.conv_out = nn.Conv2d(mid_chan, n_classes, kernel_size=1, bias=False)\n",
        "        self.init_weight()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.conv_out(x)\n",
        "        return x\n",
        "\n",
        "    def init_weight(self):\n",
        "        for ly in self.children():\n",
        "            if isinstance(ly, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(ly.weight, a=1)\n",
        "                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n",
        "\n",
        "    def get_params(self):\n",
        "        wd_params, nowd_params = [], []\n",
        "        for name, module in self.named_modules():\n",
        "            if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
        "                wd_params.append(module.weight)\n",
        "                if not module.bias is None:\n",
        "                    nowd_params.append(module.bias)\n",
        "            elif isinstance(module, BatchNorm2d):\n",
        "                nowd_params += list(module.parameters())\n",
        "        return wd_params, nowd_params\n",
        "\n",
        "\n",
        "class AttentionRefinementModule(nn.Module):\n",
        "    def __init__(self, in_chan, out_chan, *args, **kwargs):\n",
        "        super(AttentionRefinementModule, self).__init__()\n",
        "        self.conv = ConvBNReLU(in_chan, out_chan, ks=3, stride=1, padding=1)\n",
        "        self.conv_atten = nn.Conv2d(out_chan, out_chan, kernel_size= 1, bias=False)\n",
        "        self.bn_atten = BatchNorm2d(out_chan)\n",
        "        #self.bn_atten = BatchNorm2d(out_chan, activation='none')\n",
        "\n",
        "        self.sigmoid_atten = nn.Sigmoid()\n",
        "        self.init_weight()\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat = self.conv(x)\n",
        "        atten = F.avg_pool2d(feat, feat.size()[2:])\n",
        "        atten = self.conv_atten(atten)\n",
        "        atten = self.bn_atten(atten)\n",
        "        atten = self.sigmoid_atten(atten)\n",
        "        out = torch.mul(feat, atten)\n",
        "        return out\n",
        "\n",
        "    def init_weight(self):\n",
        "        for ly in self.children():\n",
        "            if isinstance(ly, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(ly.weight, a=1)\n",
        "                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n",
        "\n",
        "\n",
        "class ContextPath(nn.Module):\n",
        "    def __init__(self, backbone='CatNetSmall', pretrain_model='', use_conv_last=False, *args, **kwargs):\n",
        "        super(ContextPath, self).__init__()\n",
        "\n",
        "        self.backbone_name = backbone\n",
        "        if backbone == 'STDCNet1446':\n",
        "            self.backbone = STDCNet1446(pretrain_model=pretrain_model, use_conv_last=use_conv_last)\n",
        "            self.arm16 = AttentionRefinementModule(512, 128)\n",
        "            inplanes = 1024\n",
        "            if use_conv_last:\n",
        "                inplanes = 1024\n",
        "            self.arm32 = AttentionRefinementModule(inplanes, 128)\n",
        "            self.conv_head32 = ConvBNReLU(128, 128, ks=3, stride=1, padding=1)\n",
        "            self.conv_head16 = ConvBNReLU(128, 128, ks=3, stride=1, padding=1)\n",
        "            self.conv_avg = ConvBNReLU(inplanes, 128, ks=1, stride=1, padding=0)\n",
        "\n",
        "        elif backbone == 'STDCNet813':\n",
        "            self.backbone = STDCNet813(pretrain_model=pretrain_model, use_conv_last=use_conv_last)\n",
        "            self.arm16 = AttentionRefinementModule(512, 128)\n",
        "            inplanes = 1024\n",
        "            if use_conv_last:\n",
        "                inplanes = 1024\n",
        "            self.arm32 = AttentionRefinementModule(inplanes, 128)\n",
        "            self.conv_head32 = ConvBNReLU(128, 128, ks=3, stride=1, padding=1)\n",
        "            self.conv_head16 = ConvBNReLU(128, 128, ks=3, stride=1, padding=1)\n",
        "            self.conv_avg = ConvBNReLU(inplanes, 128, ks=1, stride=1, padding=0)\n",
        "        else:\n",
        "            print(\"backbone is not in backbone lists\")\n",
        "            exit(0)\n",
        "\n",
        "        self.init_weight()\n",
        "\n",
        "    def forward(self, x):\n",
        "        H0, W0 = x.size()[2:]\n",
        "\n",
        "        feat2, feat4, feat8, feat16, feat32 = self.backbone(x)\n",
        "        H8, W8 = feat8.size()[2:]\n",
        "        H16, W16 = feat16.size()[2:]\n",
        "        H32, W32 = feat32.size()[2:]\n",
        "\n",
        "        avg = F.avg_pool2d(feat32, feat32.size()[2:])\n",
        "\n",
        "        avg = self.conv_avg(avg)\n",
        "        avg_up = F.interpolate(avg, (H32, W32), mode='nearest')\n",
        "\n",
        "        feat32_arm = self.arm32(feat32)\n",
        "        feat32_sum = feat32_arm + avg_up\n",
        "        feat32_up = F.interpolate(feat32_sum, (H16, W16), mode='nearest')\n",
        "        feat32_up = self.conv_head32(feat32_up)\n",
        "\n",
        "        feat16_arm = self.arm16(feat16)\n",
        "        feat16_sum = feat16_arm + feat32_up\n",
        "        feat16_up = F.interpolate(feat16_sum, (H8, W8), mode='nearest')\n",
        "        feat16_up = self.conv_head16(feat16_up)\n",
        "\n",
        "        return feat2, feat4, feat8, feat16, feat16_up, feat32_up # x8, x16\n",
        "\n",
        "    def init_weight(self):\n",
        "        for ly in self.children():\n",
        "            if isinstance(ly, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(ly.weight, a=1)\n",
        "                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n",
        "\n",
        "    def get_params(self):\n",
        "        wd_params, nowd_params = [], []\n",
        "        for name, module in self.named_modules():\n",
        "            if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
        "                wd_params.append(module.weight)\n",
        "                if not module.bias is None:\n",
        "                    nowd_params.append(module.bias)\n",
        "            elif isinstance(module, BatchNorm2d):\n",
        "                nowd_params += list(module.parameters())\n",
        "        return wd_params, nowd_params\n",
        "\n",
        "\n",
        "class FeatureFusionModule(nn.Module):\n",
        "    def __init__(self, in_chan, out_chan, *args, **kwargs):\n",
        "        super(FeatureFusionModule, self).__init__()\n",
        "        self.convblk = ConvBNReLU(in_chan, out_chan, ks=1, stride=1, padding=0)\n",
        "        self.conv1 = nn.Conv2d(out_chan,\n",
        "                out_chan//4,\n",
        "                kernel_size = 1,\n",
        "                stride = 1,\n",
        "                padding = 0,\n",
        "                bias = False)\n",
        "        self.conv2 = nn.Conv2d(out_chan//4,\n",
        "                out_chan,\n",
        "                kernel_size = 1,\n",
        "                stride = 1,\n",
        "                padding = 0,\n",
        "                bias = False)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.init_weight()\n",
        "\n",
        "    def forward(self, fsp, fcp):\n",
        "        fcat = torch.cat([fsp, fcp], dim=1)\n",
        "        feat = self.convblk(fcat)\n",
        "        atten = F.avg_pool2d(feat, feat.size()[2:])\n",
        "        atten = self.conv1(atten)\n",
        "        atten = self.relu(atten)\n",
        "        atten = self.conv2(atten)\n",
        "        atten = self.sigmoid(atten)\n",
        "        feat_atten = torch.mul(feat, atten)\n",
        "        feat_out = feat_atten + feat\n",
        "        return feat_out\n",
        "\n",
        "    def init_weight(self):\n",
        "        for ly in self.children():\n",
        "            if isinstance(ly, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(ly.weight, a=1)\n",
        "                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n",
        "\n",
        "    def get_params(self):\n",
        "        wd_params, nowd_params = [], []\n",
        "        for name, module in self.named_modules():\n",
        "            if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
        "                wd_params.append(module.weight)\n",
        "                if not module.bias is None:\n",
        "                    nowd_params.append(module.bias)\n",
        "            elif isinstance(module, BatchNorm2d):\n",
        "                nowd_params += list(module.parameters())\n",
        "        return wd_params, nowd_params\n",
        "\n",
        "\n",
        "class BiSeNet(nn.Module):\n",
        "    def __init__(self, backbone, n_classes, pretrain_model='', use_boundary_2=False, use_boundary_4=False, use_boundary_8=False, use_boundary_16=False, use_conv_last=False, heat_map=False, *args, **kwargs):\n",
        "        super(BiSeNet, self).__init__()\n",
        "\n",
        "        self.use_boundary_2 = use_boundary_2\n",
        "        self.use_boundary_4 = use_boundary_4\n",
        "        self.use_boundary_8 = use_boundary_8\n",
        "        self.use_boundary_16 = use_boundary_16\n",
        "        # self.heat_map = heat_map\n",
        "        self.cp = ContextPath(backbone, pretrain_model, use_conv_last=use_conv_last)\n",
        "\n",
        "\n",
        "\n",
        "        if backbone == 'STDCNet1446':\n",
        "            conv_out_inplanes = 128\n",
        "            sp2_inplanes = 32\n",
        "            sp4_inplanes = 64\n",
        "            sp8_inplanes = 256\n",
        "            sp16_inplanes = 512\n",
        "            inplane = sp8_inplanes + conv_out_inplanes\n",
        "\n",
        "        elif backbone == 'STDCNet813':\n",
        "            conv_out_inplanes = 128\n",
        "            sp2_inplanes = 32\n",
        "            sp4_inplanes = 64\n",
        "            sp8_inplanes = 256\n",
        "            sp16_inplanes = 512\n",
        "            inplane = sp8_inplanes + conv_out_inplanes\n",
        "\n",
        "        else:\n",
        "            print(\"backbone is not in backbone lists\")\n",
        "            exit(0)\n",
        "\n",
        "        self.ffm = FeatureFusionModule(inplane, 256)\n",
        "        self.conv_out = BiSeNetOutput(256, 256, n_classes)\n",
        "        self.conv_out16 = BiSeNetOutput(conv_out_inplanes, 64, n_classes)\n",
        "        self.conv_out32 = BiSeNetOutput(conv_out_inplanes, 64, n_classes)\n",
        "\n",
        "        self.conv_out_sp16 = BiSeNetOutput(sp16_inplanes, 64, 1)\n",
        "\n",
        "        self.conv_out_sp8 = BiSeNetOutput(sp8_inplanes, 64, 1)\n",
        "        self.conv_out_sp4 = BiSeNetOutput(sp4_inplanes, 64, 1)\n",
        "        self.conv_out_sp2 = BiSeNetOutput(sp2_inplanes, 64, 1)\n",
        "        self.init_weight()\n",
        "\n",
        "    def forward(self, x):\n",
        "        H, W = x.size()[2:]\n",
        "\n",
        "        feat_res2, feat_res4, feat_res8, feat_res16, feat_cp8, feat_cp16 = self.cp(x)\n",
        "\n",
        "        feat_out_sp2 = self.conv_out_sp2(feat_res2)\n",
        "\n",
        "        feat_out_sp4 = self.conv_out_sp4(feat_res4)\n",
        "\n",
        "        feat_out_sp8 = self.conv_out_sp8(feat_res8)\n",
        "\n",
        "        feat_out_sp16 = self.conv_out_sp16(feat_res16)\n",
        "\n",
        "        feat_fuse = self.ffm(feat_res8, feat_cp8)\n",
        "\n",
        "        feat_out = self.conv_out(feat_fuse)\n",
        "        feat_out16 = self.conv_out16(feat_cp8)\n",
        "        feat_out32 = self.conv_out32(feat_cp16)\n",
        "\n",
        "        feat_out = F.interpolate(feat_out, (H, W), mode='bilinear', align_corners=True)\n",
        "        feat_out16 = F.interpolate(feat_out16, (H, W), mode='bilinear', align_corners=True)\n",
        "        feat_out32 = F.interpolate(feat_out32, (H, W), mode='bilinear', align_corners=True)\n",
        "\n",
        "\n",
        "        if self.use_boundary_2 and self.use_boundary_4 and self.use_boundary_8:\n",
        "            return feat_out, feat_out16, feat_out32, feat_out_sp2, feat_out_sp4, feat_out_sp8\n",
        "\n",
        "        if (not self.use_boundary_2) and self.use_boundary_4 and self.use_boundary_8:\n",
        "            return feat_out, feat_out16, feat_out32, feat_out_sp4, feat_out_sp8\n",
        "\n",
        "        if (not self.use_boundary_2) and (not self.use_boundary_4) and self.use_boundary_8:\n",
        "            return feat_out, feat_out16, feat_out32, feat_out_sp8\n",
        "\n",
        "        if (not self.use_boundary_2) and (not self.use_boundary_4) and (not self.use_boundary_8):\n",
        "            return feat_out, feat_out16, feat_out32\n",
        "\n",
        "    def init_weight(self):\n",
        "        for ly in self.children():\n",
        "            if isinstance(ly, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(ly.weight, a=1)\n",
        "                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n",
        "\n",
        "    def get_params(self):\n",
        "        wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params = [], [], [], []\n",
        "        for name, child in self.named_children():\n",
        "            child_wd_params, child_nowd_params = child.get_params()\n",
        "            if isinstance(child, (FeatureFusionModule, BiSeNetOutput)):\n",
        "                lr_mul_wd_params += child_wd_params\n",
        "                lr_mul_nowd_params += child_nowd_params\n",
        "            else:\n",
        "                wd_params += child_wd_params\n",
        "                nowd_params += child_nowd_params\n",
        "        return wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7MBNcStzY2o"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp6UOEKtzY2o"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lu9QZNEgzY2o"
      },
      "outputs": [],
      "source": [
        "num_epochs = 20\n",
        "BATCH_SIZE = 6\n",
        "learning_rate = 1e-3\n",
        "step_size = 10\n",
        "gamma = 0.1\n",
        "resize = 512\n",
        "w_decay = 1e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzkSoImkzY2o"
      },
      "source": [
        "### Dataset preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVjIq8vnzY2p"
      },
      "source": [
        "#### Normalization metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uejw9pwGzY2p"
      },
      "outputs": [],
      "source": [
        "num_workers = 2 if device.type == 'cuda' else 0\n",
        "\n",
        "# Poiché il modello è pretrainato su ImageNet, si usano media e varianza di ImageNet\n",
        "avg = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Df8mfmTwzY2p"
      },
      "source": [
        "#### Training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLC0KJcHzY2p"
      },
      "outputs": [],
      "source": [
        "train_transform = A.Compose([\n",
        "    A.Normalize(mean=avg, std=std, p=1, always_apply=True, max_pixel_value=255),\n",
        "    A.Resize(resize, resize, p=1, always_apply=True)\n",
        "])\n",
        "\n",
        "train_dataset = LoveDA(TRAIN_DIR, IMG_PATH, MASK_PATH, directories=URBAN_PATH, transforms=train_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=num_workers, worker_init_fn=seed_worker, generator=g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0VEbqXdzY2p"
      },
      "source": [
        "#### Validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RW7_R1PzY2p"
      },
      "outputs": [],
      "source": [
        "val_transform = A.Compose([\n",
        "    A.Normalize(mean=avg, std=std, p=1, always_apply=True, max_pixel_value=255),\n",
        "])\n",
        "\n",
        "#val_dataset = LoveDA(VAL_DIR, IMG_PATH, MASK_PATH, directories=URBAN_PATH, transforms=val_transform)\n",
        "val_dataset = LoveDA(VAL_DIR, IMG_PATH, MASK_PATH, directories=RURAL_PATH, transforms=val_transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=num_workers, worker_init_fn=seed_worker, generator=g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7PuqcWGzY2p"
      },
      "source": [
        "### Training process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Fe9yGjKzY2q"
      },
      "source": [
        "#### Model engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xX6X-AwnzY2q",
        "outputId": "352dc834-0aff-45f2-9bc7-8caac19abbab"
      },
      "outputs": [],
      "source": [
        "model = BiSeNet(n_classes=num_classes,backbone='STDCNet813', pretrain_model=stdc1_weights).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=w_decay)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bu-lFRI2X6-T"
      },
      "source": [
        "#### Evaluate function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQGIMAbXX6-T"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, dataloader, device, ) -> tuple:\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    data_len = 0\n",
        "    iou_scores = 0.0\n",
        "    ious_per_class = torch.zeros(num_classes)\n",
        "\n",
        "    for i, (inputs, masks) in enumerate(dataloader):\n",
        "\n",
        "        data_len += inputs.size(0)\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs, _, _ = model(inputs)\n",
        "        loss = criterion(outputs, masks)\n",
        "\n",
        "        running_loss += loss.item()*inputs.size(0)\n",
        "\n",
        "        # Calculate mIoU\n",
        "        iou, iou_per_class = calculate_iou(outputs, masks, num_classes)\n",
        "        iou_scores += iou*inputs.size(0)\n",
        "        ious_per_class+=iou_per_class.cpu()*inputs.size(0)\n",
        "\n",
        "    mIoU = iou_scores/data_len\n",
        "    loss = running_loss/data_len\n",
        "    ious_per_class/=data_len\n",
        "\n",
        "\n",
        "    return loss, mIoU, ious_per_class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQbqXe6VzY2q"
      },
      "source": [
        "#### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "8dCDNmAGzY2q",
        "outputId": "0d922b75-71ae-44b6-f6f0-42bec688675d"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "eval_losses = []\n",
        "mious = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(\"### Training mode\")\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (images, masks) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs, outputs16, outputs32 = model(images)\n",
        "        loss1 = criterion(outputs, masks)\n",
        "        loss2 = criterion(outputs16, masks)\n",
        "        loss3 = criterion(outputs32, masks)\n",
        "        loss = loss1 + loss2 + loss3\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if (i + 1) % 25 == 0:\n",
        "            print(f\"Processed {i + 1} batches, loss: {running_loss / (i+1)}\")\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(epoch_loss)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    print(\"### Evaluation mode\")\n",
        "    val_loss, miou, ious_per_class = evaluate(model, val_loader, device)\n",
        "\n",
        "    print(f\"Validation mIoU: {miou*100:.3f}%, Validation loss: {val_loss:.5f}\")\n",
        "    for i, cat in enumerate(categories.keys()):\n",
        "        print(f\"{cat} mIoU: {ious_per_class[i]*100:.3f}\")\n",
        "    print()\n",
        "\n",
        "    eval_losses.append(val_loss)\n",
        "    mious.append(miou)\n",
        "\n",
        "    print(f\"Epoch: [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}, mIoU: {(miou * 100):.2f}%\")\n",
        "\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-e0QWMNzY2r"
      },
      "source": [
        "### Metric calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3HdWgUgzY2r"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "mean_latency, _, _, _ = calculate_latency_fps(model, device, 1024, 1024, num_epochs, ModelType.STDC)\n",
        "print(f\"Mean Latency: {mean_latency:.2f} ms\")\n",
        "\n",
        "calculate_flops_params(model, device, 1024, 1024, ModelType.STDC)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "QPliIXlMh2bf",
        "Xs1L8KHZ9GAq",
        "LsP0RjvL9NlC",
        "eORGOwqKY94j",
        "Zim4nyhUgTEE",
        "BUlnYE8RSUYX",
        "jbMkS3H2iS1v",
        "vk_O5SS6BqXV",
        "glRuMNQhsq1p",
        "2y7JhEDjpOk1",
        "XjoF0ZtWbuz3",
        "_8SuvFTlwe9a",
        "YuH9_bUuxI4n",
        "CULoLihOwxHl",
        "OQKuCveRw8Qm",
        "Cp6UOEKtzY2o",
        "wzkSoImkzY2o",
        "GVjIq8vnzY2p",
        "Df8mfmTwzY2p",
        "K0VEbqXdzY2p",
        "H7PuqcWGzY2p",
        "8Fe9yGjKzY2q",
        "IQbqXe6VzY2q",
        "Les-vObfzY2q",
        "2eNNOT-dzY2r",
        "F-e0QWMNzY2r",
        "1ohsbEQnzY2s",
        "KvfJfTJZxrYG"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
